<!DOCTYPE html>
<html><head>
		<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
		<meta http-equiv="Content-Security-Policy" content="script-src 'none'; media-src 'none'">
		<title>Zotero Report</title>
		<link rel="stylesheet" type="text/css" href="data:text/css;base64,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">
		<link rel="stylesheet" type="text/css" media="screen,projection" href="data:text/css;base64,LyogR2VuZXJpYyBzdHlsZXMgKi8KYm9keSB7Cglmb250OiA2Mi41JSBHZW9yZ2lhLCBUaW1lcywgc2VyaWY7Cgl3aWR0aDogNzgwcHg7CgltYXJnaW46IDAgYXV0bzsKfQoKaDIgewoJZm9udC1zaXplOiAxLjVlbTsKCWxpbmUtaGVpZ2h0OiAxLjVlbTsKCWZvbnQtZmFtaWx5OiBHZW9yZ2lhLCBUaW1lcywgc2VyaWY7Cn0KCnAgewoJbGluZS1oZWlnaHQ6IDEuNWVtOwp9CgphOmxpbmssIGE6dmlzaXRlZCB7Cgljb2xvcjogIzkwMDsKfQoKYTpob3ZlciwgYTphY3RpdmUgewoJY29sb3I6ICM3Nzc7Cn0KCgp1bC5yZXBvcnQgewoJZm9udC1zaXplOiAxLjRlbTsKCXdpZHRoOiA2ODBweDsKCW1hcmdpbjogMCBhdXRvOwoJcGFkZGluZzogMjBweCAyMHB4Owp9CgovKiBNZXRhZGF0YSB0YWJsZSAqLwp0YWJsZSB7Cglib3JkZXI6IDFweCAjY2NjIHNvbGlkOwoJb3ZlcmZsb3c6IGF1dG87Cgl3aWR0aDogMTAwJTsKCW1hcmdpbjogLjFlbSBhdXRvIC43NWVtOwoJcGFkZGluZzogMC41ZW07Cn0K">
		<link rel="stylesheet" type="text/css" media="print" href="data:text/css;base64,Ym9keSB7Cglmb250OiAxMnB0ICJUaW1lcyBOZXcgUm9tYW4iLCBUaW1lcywgR2VvcmdpYSwgc2VyaWY7CgltYXJnaW46IDA7Cgl3aWR0aDogYXV0bzsKCWNvbG9yOiBibGFjazsKfQoKLyogUGFnZSBCcmVha3MgKHBhZ2UtYnJlYWstaW5zaWRlIG9ubHkgcmVjb2duaXplZCBieSBPcGVyYSkgKi8KaDEsIGgyLCBoMywgaDQsIGg1LCBoNiB7CglwYWdlLWJyZWFrLWFmdGVyOiBhdm9pZDsKCXBhZ2UtYnJlYWstaW5zaWRlOiBhdm9pZDsKfQoKdWwsIG9sLCBkbCB7CglwYWdlLWJyZWFrLWluc2lkZTogYXZvaWQ7Cgljb2xvci1hZGp1c3Q6IGV4YWN0Owp9CgpoMiB7Cglmb250LXNpemU6IDEuM2VtOwoJbGluZS1oZWlnaHQ6IDEuM2VtOwp9CgphIHsKCWNvbG9yOiAjMDAwOwoJdGV4dC1kZWNvcmF0aW9uOiBub25lOwp9Cg==">
	</head>
	<body>
		<ul class="report combineChildItems">
			<li id="item_UM3VVUUV" class="item preprint">
			<h2>A Bibliometric Review of Large Language Models Research from 2017 to 2023</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Preprint</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Lizhou Fan</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Lingyao Li</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Zihui Ma</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Sanggyu Lee</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Huizi Yu</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Libby Hemphill</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Large language models (LLMs) are a class of language models 
that have demonstrated outstanding performance across a range of natural
 language processing (NLP) tasks and have become a highly sought-after 
research area, because of their ability to generate human-like language 
and their potential to revolutionize science and technology. In this 
study, we conduct bibliometric and discourse analyses of scholarly 
literature on LLMs. Synthesizing over 5,000 publications, this paper 
serves as a roadmap for researchers, practitioners, and policymakers to 
navigate the current landscape of LLMs research. We present the research
 trends from 2017 to early 2023, identifying patterns in research 
paradigms and collaborations. We start with analyzing the core algorithm
 developments and NLP tasks that are fundamental in LLMs research. We 
then investigate the applications of LLMs in various fields and domains 
including medicine, engineering, social science, and humanities. Our 
review also reveals the dynamic, fast-paced evolution of LLMs research. 
Overall, this paper offers valuable insights into the current state, 
impact, and potential of LLMs research and its applications.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2023-04-03</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>arXiv.org</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://arxiv.org/abs/2304.02020">http://arxiv.org/abs/2304.02020</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>1/7/2025, 5:40:31 PM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>arXiv:2304.02020 [cs]</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.48550/arXiv.2304.02020">10.48550/arXiv.2304.02020</a></td>
					</tr>
					<tr>
					<th>Repository</th>
						<td>arXiv</td>
					</tr>
					<tr>
					<th>Archive ID</th>
						<td>arXiv:2304.02020</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>1/7/2025, 5:40:31 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:39 PM</td>
					</tr>
				</tbody></table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Computer Science - Computation and Language</li>
					<li>Computer Science - Computers and Society</li>
					<li>Computer Science - Digital Libraries</li>
					<li>Computer Science - Social and Information Networks</li>
				</ul>
				<h3 class="notes">Notes:</h3>
				<ul class="notes">
					<li id="item_VH7BX7YV">
<p class="plaintext">Comment: 36 pages, 9 figures, and 4 tables</p>
					</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_28ITMIJF">Preprint PDF					</li>
					<li id="item_KELASNYC">Snapshot					</li>
				</ul>
			</li>


			<li id="item_QBMCRB3X" class="item webpage">
			<h2>A Comparison of Methods in Political Science Text Classification: Transfer Learning Language Models for Politics</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Web Page</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Automated text classification has rapidly become an important 
tool for political analysis. Recent advancements in NLP enabled by 
advances in deep learning now achieve state of the art results in many 
standard tasks for the field. However, these methods require large 
amounts of both computing power and text data to learn the 
characteristics of the language, resources which are not always 
accessible to political scientists. One solution is a transfer learning 
approach, where knowledge learned in one area or source task is 
transferred to another area or a target task. A class of models that 
embody this approach are language models, which demonstrate extremely 
high levels of performance. We investigate the performance of these 
models in the political science by comparing multiple text 
classification methods. We find RoBERTa and XLNet, language models that 
rely on theTransformer, require fewer computing resources and less 
training data to perform on par with – or outperform – several political
 science text classification methods. Moreover, we find that the 
increase in accuracy is especially significant in the case of small 
labeled data, highlighting the potential for reducing the data-labeling 
cost of supervised methods for political scientists via the use of 
pretrained language models.</td>
					</tr>
					<tr>
					<th>Language</th>
						<td>en</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>A Comparison of Methods in Political Science Text Classification</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://csmapnyu.org/research/academic-research/a-comparison-of-methods-in-political-science-text-classification-transfer-learning-language-models-for-politics">https://csmapnyu.org/research/academic-research/a-comparison-of-methods-in-political-science-text-classification-transfer-learning-language-models-for-politics</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>1/8/2025, 2:22:16 PM</td>
					</tr>
					<tr>
					<th>Website Title</th>
						<td>NYU’s Center for Social Media and Politics</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>1/8/2025, 2:22:16 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_GELNSWUT">Snapshot					</li>
				</ul>
			</li>


			<li id="item_UG698S7C" class="item journalArticle">
			<h2>A comprehensive investigation of variational auto-encoders for population synthesis</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Abdoul Razac Sané</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Pierre-Olivier Vandanjon</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Rachid Belaroussi</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Pierre Hankach</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>The use of synthetic populations has grown considerably over 
the recent years, in revolutionizing studies conducted within various 
fields, including social science research, urban planning, public health
 and transportation modeling. These synthetic populations prove to be 
valuable, as substitutes for the often missing or sensitive real data, 
and moreover are capable of preserving both privacy and 
representativeness. They are typically constructed from aggregate and/or
 sample data. Recently, new methods for generating synthetic populations
 based on deep learning, notably Variational Autoencoders (VAEs), have 
been developed. Such methods serve to overcome the limitations of 
traditional methods, such as Iterative Proportional Fitting (IPF), which
 are unable to generate agents with cross-modalities not found in the 
sample data. As such, IPF requires large samples to generate a synthetic
 population closely resembling the actual one. Conversely, the advantage
 of VAE lies in their ability to generate agents not found in the sample
 data, albeit with the risk of creating agents not existing in the 
actual population. However, the practical documentation as well as 
detailed analyses of the architectures and results from implementation 
of these deep learning approaches, in particular VAE, are limited, thus 
making these methods difficult to appropriate for practitioners. This 
paper focuses on generating synthetic populations using VAE. First, an 
in-depth and accessible theoretical explanation of how VAEs function is 
provided. Next, a detailed study of these methods is carried out by 
testing the various architectures, parameters, sample sizes and 
evaluation indicators necessary to guarantee high-quality results. 
Highlighted herein is the ability of VAEs to generate large datasets 
with a small training sample, in addition to VAE performance in 
generating new realistic individuals not present in the learning base. 
Certain limitations are identified, including the difficulties 
encountered by VAEs in managing numerical attributes and the need for 
post-processing to eliminate unrealistic individuals. In conclusion, 
despite a number of limitations, VAE constitutes a very promising 
methodology for generating synthetic populations, in offering 
practitioners numerous advantages. This paper is accompanied by a Python
 notebook to assist interested readers implement this new methodology.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2024-12-11</td>
					</tr>
					<tr>
					<th>Language</th>
						<td>en</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>Springer Link</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://doi.org/10.1007/s42001-024-00332-0">https://doi.org/10.1007/s42001-024-00332-0</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>1/3/2025, 2:06:04 PM</td>
					</tr>
					<tr>
					<th>Volume</th>
						<td>8</td>
					</tr>
					<tr>
					<th>Pages</th>
						<td>13</td>
					</tr>
					<tr>
					<th>Publication</th>
						<td>Journal of Computational Social Science</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.1007/s42001-024-00332-0">10.1007/s42001-024-00332-0</a></td>
					</tr>
					<tr>
					<th>Issue</th>
						<td>1</td>
					</tr>
					<tr>
					<th>Journal Abbr</th>
						<td>J Comput Soc Sc</td>
					</tr>
					<tr>
					<th>ISSN</th>
						<td>2432-2725</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>1/3/2025, 2:06:04 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Artificial Intelligence</li>
					<li>Deep generative model</li>
					<li>Machine learning</li>
					<li>Sampling zeros</li>
					<li>Structural zeros</li>
					<li>Synthetic population</li>
					<li>Variational autoencoders</li>
				</ul>
			</li>


			<li id="item_SLERHJX9" class="item preprint">
			<h2>A Field in Movement: A Bibliometric Analysis of Migration Research in Seven Demographic Journals Using Large Language Models</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Preprint</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Yacine Boujija</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Carolina Pradier</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>This study presents a comprehensive bibliometric analysis of 
migration research within seven prominent English and French demographic
 journals, utilizing Large Language Models and BERT-based topic 
modeling. By leveraging these text processing techniques, we 
systematically classify research papers by migration type (internal vs. 
international) and explore key thematic and geographic trends in 
migration studies. Our results show variations in the importance of 
international and internal migration research over time, but with a 
marked dominance of international migration studies in more recent 
years. We further uncover the geographical bias in migration research, 
noting a stronger emphasis on high-income countries as the main 
destinations studied. Overall, we also note considerable shifts in 
topics over time for both types of migration. The integration of LLMs 
allowed for a more nuanced, quantitative insights into the evolving 
landscape of migration research across time and space, proving to be a 
valuable tool for multilingual bibliometric analysis.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2024-10-11</td>
					</tr>
					<tr>
					<th>Language</th>
						<td>en-us</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>A Field in Movement</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>OSF Preprints</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://osf.io/mwtqy">https://osf.io/mwtqy</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>1/3/2025, 4:21:47 PM</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.31235/osf.io/mwtqy">10.31235/osf.io/mwtqy</a></td>
					</tr>
					<tr>
					<th>Repository</th>
						<td>OSF</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>1/3/2025, 4:21:47 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>BERT</li>
					<li>Bibliometric</li>
					<li>ChatGPT</li>
					<li>demography</li>
					<li>Internal migration</li>
					<li>international migration</li>
					<li>Large Language Models</li>
					<li>migration</li>
					<li>migration studies</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_MCBU2S4P">OSF Preprint					</li>
				</ul>
			</li>


			<li id="item_2LTYGC47" class="item preprint">
			<h2>A Framework For Discussing LLMs as Tools for Qualitative Analysis</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Preprint</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>James Eschrich</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Sarah Sterman</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>We review discourses about the philosophy of science in 
qualitative research and evidence from cognitive linguistics in order to
 ground a framework for discussing the use of Large Language Models 
(LLMs) to support the qualitative analysis process. This framework 
involves asking two key questions: “is the LLM proposing or refuting a 
qualitative model?” and “is the human researcher checking the LLM’s 
decision-making directly?”. We then discuss an implication of this 
framework: that using LLMs to surface counterexamples for human review 
represents a promising space for the adoption of LLMs into the 
qualitative research process. This space is promising because it is a 
site of overlap between researchers working from a variety of 
philosophical assumptions, enabling productive cross-paradigm 
collaboration on tools and practices.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2024-07-15</td>
					</tr>
					<tr>
					<th>Language</th>
						<td>en</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>arXiv.org</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://arxiv.org/abs/2407.11198">http://arxiv.org/abs/2407.11198</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>9/23/2024, 3:45:09 PM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>arXiv:2407.11198 [cs]</td>
					</tr>
					<tr>
					<th>Repository</th>
						<td>arXiv</td>
					</tr>
					<tr>
					<th>Archive ID</th>
						<td>arXiv:2407.11198</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>9/23/2024, 3:45:09 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>9/23/2024, 3:45:09 PM</td>
					</tr>
				</tbody></table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Computer Science - Human-Computer Interaction</li>
				</ul>
				<h3 class="notes">Notes:</h3>
				<ul class="notes">
					<li id="item_EM7F47WE">
<p class="plaintext">Comment: 4 pages, 1 table. Presented at the "LLMs as Research Tools" workshop at CHI 2024 (https://dl.acm.org/doi/10.1145/3613905.3636301)</p>
					</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_MR72HGDN">Eschrich and Sterman - 2024 - A Framework For Discussing LLMs as Tools for Quali.pdf					</li>
				</ul>
			</li>


			<li id="item_RW7DZXWS" class="item preprint">
			<h2>A Large Language Models Digest For Social Scientists</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Preprint</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Daniel Valdenegro</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>I write these lines not as an expert in LLMs, but more as an 
informed user. I did my PhD on the application of early 
transformers-based language models (e.g. BERT, RoBERTa) to classify 
large amounts of text data. I have experience with their ``fine-tuning" 
process and a reasonable amount of knowledge about their training 
process. However, NLP technology has progressed fast and in a couple of 
years we moved from models containing hundreds of millions of 
parameters, to models containing hundreds of billions of parameters. 
Large Language Models keep getting larger and more capable. Companies 
closely related to the work of social scientists are taking note of this
 and they are starting to offer “AI-powered” qualitative software (e.g.,
 see AtlasTi new OpenAI alliance) opening exciting new avenues for 
social scientists.

However, I believe that while the technical capabilities of the models 
have progressed fast, our actual understanding of what they are and what
 they are actually capable of doing has been, maybe purposely, left 
behind. There are several misconceptions about the models' “reasoning” 
and “introspection” capabilities, with claims pointing towards a 
soon-to-be-achieved "Artificial General Intelligence" (AGI), a holy 
grail of the research in artificial intelligence, but without offering 
substantial evidence of such claims. There is also the incredibly thick 
jargon used in the AI and Machine Learning community, ---which I’m more 
than guilty of using---, which, sometimes purposely, contributes to 
gate-keep the knowledge and details of this new to technologies to a 
reduce group of individuals. Finally, there is a fair amount of 
marketing strategy around the communications related to LLMs. Most of 
the newest Large Language Models are the intellectual property and 
product of large companies and corporations, as they are too big and 
costly to be trained by singular individuals. For these corporations, 
being seen by the general public as producing real “Artificial General 
Intelligence” or “Reasoning Machines” is a big selling point, although 
the reality might be much more nuanced than that.

All of this contributes to generate confusion and mystification around 
AI, in general, and Large Language Models in particular. So, in a 
personal effort to clear-up my own thoughts and to hopefully provide 
something useful for others, in this document I will attempt to provide a
 high-level description of what LLMs are, how they work and what they 
are capable of doing, especially for social scientists.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2023-07-27</td>
					</tr>
					<tr>
					<th>Language</th>
						<td>en-us</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>OSF Preprints</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://osf.io/m74vs">https://osf.io/m74vs</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>1/3/2025, 4:19:47 PM</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.31235/osf.io/m74vs">10.31235/osf.io/m74vs</a></td>
					</tr>
					<tr>
					<th>Repository</th>
						<td>OSF</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>1/3/2025, 4:19:47 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>AI</li>
					<li>ChatGPT</li>
					<li>LLMs</li>
					<li>Natural Language Processing</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_KZ9SYUQ3">OSF Preprint					</li>
				</ul>
			</li>


			<li id="item_QW4FEHU6" class="item journalArticle">
			<h2>A scoping review on the use of natural language processing in research on political polarization: trends and research prospects</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Renáta Németh</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>As part of the “text-as-data” movement, Natural Language 
Processing (NLP) provides a computational way to examine political 
polarization. We conducted a methodological scoping review of studies 
published since 2010 (n = 154) to clarify how NLP research has 
conceptualized and measured political polarization, and to characterize 
the degree of integration of the two different research paradigms that 
meet in this research area. We identified biases toward US context 
(59%), Twitter data (43%) and machine learning approach (33%). Research 
covers different layers of the political public sphere (politicians, 
experts, media, or the lay public), however, very few studies involved 
more than one layer. Results indicate that only a few studies made use 
of domain knowledge and a high proportion of the studies were not 
interdisciplinary. Those studies that made efforts to interpret the 
results demonstrated that the characteristics of political texts depend 
not only on the political position of their authors, but also on other 
often-overlooked factors. Ignoring these factors may lead to overly 
optimistic performance measures. Also, spurious results may be obtained 
when causal relations are inferred from textual data. Our paper provides
 arguments for the integration of explanatory and predictive modeling 
paradigms, and for a more interdisciplinary approach to polarization 
research.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2023-04-01</td>
					</tr>
					<tr>
					<th>Language</th>
						<td>en</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>A scoping review on the use of natural language processing in research on political polarization</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>Springer Link</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://doi.org/10.1007/s42001-022-00196-2">https://doi.org/10.1007/s42001-022-00196-2</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>1/3/2025, 3:04:04 PM</td>
					</tr>
					<tr>
					<th>Volume</th>
						<td>6</td>
					</tr>
					<tr>
					<th>Pages</th>
						<td>289-313</td>
					</tr>
					<tr>
					<th>Publication</th>
						<td>Journal of Computational Social Science</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.1007/s42001-022-00196-2">10.1007/s42001-022-00196-2</a></td>
					</tr>
					<tr>
					<th>Issue</th>
						<td>1</td>
					</tr>
					<tr>
					<th>Journal Abbr</th>
						<td>J Comput Soc Sc</td>
					</tr>
					<tr>
					<th>ISSN</th>
						<td>2432-2725</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>1/3/2025, 3:04:04 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Computational text analysis</li>
					<li>Language polarization</li>
					<li>Natural language processing</li>
					<li>Partisan language</li>
					<li>Political polarization</li>
					<li>Text mining</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_MU6LK6JD">Full Text PDF					</li>
				</ul>
			</li>


			<li id="item_PEY5D5W8" class="item journalArticle">
			<h2>A survey of Emotional Artificial Intelligence and crimes: detection, prediction, challenges and future direction</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Tala Talaei Khoei</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Aditi Singh</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Emotional Artificial Intelligence (Emotional AI), with its 
advanced capability to detect, analyze, and interpret human emotions, 
presents a groundbreaking opportunity for enhancing various aspects of 
policing and criminology. This paper delves into the integration of 
Emotional AI in these fields, highlighting its potential to 
revolutionize crime detection, prevention, and the improvement of 
interactions within the criminal justice system. By categorizing the 
applications of Emotional AI, from predictive policing to emotional 
assessments during interrogations, the paper explores how this 
technology can offer novel insights into criminal behavior and support 
mental health initiatives. Additionally, it addresses the ethical 
considerations associated with Emotional AI's deployment, such as 
privacy, bias, and the accuracy of emotion interpretation. The survey 
synthesizes current challenges and proposes future research directions, 
aiming to guide the responsible integration of Emotional AI technologies
 in law enforcement practices. The paper emphasizes the need for a 
balanced approach that respects individual rights while harnessing 
Emotional AI's benefits for justice and public safety.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2024</td>
					</tr>
					<tr>
					<th>Language</th>
						<td>en</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>A survey of Emotional Artificial Intelligence and crimes</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>ideas.repec.org</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://ideas.repec.org//a/spr/jcsosc/v7y2024i3d10.1007_s42001-024-00313-3.html">https://ideas.repec.org//a/spr/jcsosc/v7y2024i3d10.1007_s42001-024-00313-3.html</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>1/3/2025, 2:11:39 PM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>Publisher: Springer</td>
					</tr>
					<tr>
					<th>Volume</th>
						<td>7</td>
					</tr>
					<tr>
					<th>Pages</th>
						<td>2359-2402</td>
					</tr>
					<tr>
					<th>Publication</th>
						<td>Journal of Computational Social Science</td>
					</tr>
					<tr>
					<th>Issue</th>
						<td>3</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>1/3/2025, 2:11:39 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Artificial Intelligence</li>
					<li>Biometrics</li>
					<li>Crime</li>
					<li>Detection</li>
					<li>Emotions</li>
					<li>Facial recognition</li>
					<li>Policing</li>
					<li>Prediction</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_PFRGDFLC">Snapshot					</li>
				</ul>
			</li>


			<li id="item_N2U632YZ" class="item preprint">
			<h2>A Survey on Human-Centric LLMs</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Preprint</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Jing Yi Wang</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Nicholas Sukiennik</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Tong Li</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Weikang Su</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Qianyue Hao</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Jingbo Xu</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Zihan Huang</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Fengli Xu</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Yong Li</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>The rapid evolution of large language models (LLMs) and their 
capacity to simulate human cognition and behavior has given rise to 
LLM-based frameworks and tools that are evaluated and applied based on 
their ability to perform tasks traditionally performed by humans, namely
 those involving cognition, decision-making, and social interaction. 
This survey provides a comprehensive examination of such human-centric 
LLM capabilities, focusing on their performance in both individual tasks
 (where an LLM acts as a stand-in for a single human) and collective 
tasks (where multiple LLMs coordinate to mimic group dynamics). We first
 evaluate LLM competencies across key areas including reasoning, 
perception, and social cognition, comparing their abilities to 
human-like skills. Then, we explore real-world applications of LLMs in 
human-centric domains such as behavioral science, political science, and
 sociology, assessing their effectiveness in replicating human behaviors
 and interactions. Finally, we identify challenges and future research 
directions, such as improving LLM adaptability, emotional intelligence, 
and cultural sensitivity, while addressing inherent biases and enhancing
 frameworks for human-AI collaboration. This survey aims to provide a 
foundational understanding of LLMs from a human-centric perspective, 
offering insights into their current capabilities and potential for 
future development.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2024-12-01</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>arXiv.org</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://arxiv.org/abs/2411.14491">http://arxiv.org/abs/2411.14491</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>1/7/2025, 4:34:30 PM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>arXiv:2411.14491 [cs]</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.48550/arXiv.2411.14491">10.48550/arXiv.2411.14491</a></td>
					</tr>
					<tr>
					<th>Repository</th>
						<td>arXiv</td>
					</tr>
					<tr>
					<th>Archive ID</th>
						<td>arXiv:2411.14491</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>1/7/2025, 4:34:30 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Computer Science - Artificial Intelligence</li>
					<li>Computer Science - Computation and Language</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_GHYNPVCB">Preprint PDF					</li>
					<li id="item_ZWR7Y5GX">Snapshot					</li>
				</ul>
			</li>


			<li id="item_88LDIQQL" class="item journalArticle">
			<h2>A Survey on Large Language Model based Autonomous Agents</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Lei Wang</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Chen Ma</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Xueyang Feng</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Zeyu Zhang</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Hao Yang</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Jingsen Zhang</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Zhiyuan Chen</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Jiakai Tang</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Xu Chen</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Yankai Lin</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Wayne Xin Zhao</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Zhewei Wei</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Ji-Rong Wen</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Autonomous agents have long been a prominent research focus in
 both academic and industry communities. Previous research in this field
 often focuses on training agents with limited knowledge within isolated
 environments, which diverges significantly from human learning 
processes, and thus makes the agents hard to achieve human-like 
decisions. Recently, through the acquisition of vast amounts of web 
knowledge, large language models (LLMs) have demonstrated remarkable 
potential in achieving human-level intelligence. This has sparked an 
upsurge in studies investigating LLM-based autonomous agents. In this 
paper, we present a comprehensive survey of these studies, delivering a 
systematic review of the field of LLM-based autonomous agents from a 
holistic perspective. More specifically, we first discuss the 
construction of LLM-based autonomous agents, for which we propose a 
unified framework that encompasses a majority of the previous work. 
Then, we present a comprehensive overview of the diverse applications of
 LLM-based autonomous agents in the fields of social science, natural 
science, and engineering. Finally, we delve into the evaluation 
strategies commonly used for LLM-based autonomous agents. Based on the 
previous studies, we also present several challenges and future 
directions in this field. To keep track of this field and continuously 
update our survey, we maintain a repository of relevant references at 
https://github.com/Paitesanshi/LLM-Agent-Survey.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>12/2024</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>arXiv.org</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://arxiv.org/abs/2308.11432">http://arxiv.org/abs/2308.11432</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>1/7/2025, 5:38:12 PM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>arXiv:2308.11432 [cs]</td>
					</tr>
					<tr>
					<th>Volume</th>
						<td>18</td>
					</tr>
					<tr>
					<th>Pages</th>
						<td>186345</td>
					</tr>
					<tr>
					<th>Publication</th>
						<td>Frontiers of Computer Science</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.1007/s11704-024-40231-1">10.1007/s11704-024-40231-1</a></td>
					</tr>
					<tr>
					<th>Issue</th>
						<td>6</td>
					</tr>
					<tr>
					<th>Journal Abbr</th>
						<td>Front. Comput. Sci.</td>
					</tr>
					<tr>
					<th>ISSN</th>
						<td>2095-2228, 2095-2236</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>1/7/2025, 5:38:12 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Computer Science - Artificial Intelligence</li>
					<li>Computer Science - Computation and Language</li>
				</ul>
				<h3 class="notes">Notes:</h3>
				<ul class="notes">
					<li id="item_45YUVSN6">
<p class="plaintext">Comment: change several 35 pages, 5 figures, 3 tables</p>
					</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_CXLNJ5KY">Preprint PDF					</li>
					<li id="item_VWF56FGJ">Snapshot					</li>
				</ul>
			</li>


			<li id="item_3DUE7X6G" class="item preprint">
			<h2>A Technological Construction of Society: Comparing GPT-4 and Human Respondents for Occupational Evaluation in the UK</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Preprint</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Paweł Gmyrek</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Christoph Lutz</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Gemma Newlands</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Despite initial research about the biases and perceptions of 
Large Language Models (LLMs), we lack evidence on how LLMs evaluate 
occupations, especially in comparison to human evaluators. In this 
paper, we present a systematic comparison of occupational evaluations by
 GPT-4 with those from an in-depth, high-quality and recent human 
respondents survey in the United Kingdom. Covering the full ISCO-08 
occupational landscape (expanded to 580 occupational groups), and two 
distinct metrics (prestige and social value), our findings indicate that
 GPT-4 and human scores are highly correlated across all ISCO-08 major 
groups. Our analyses show both the potentials and risks of using 
LLM-generated data for sociological and occupational research. 
Potentials include LLMs’ efficiency, cost effectiveness, speed, and 
accuracy in capturing general tendencies. By contrast, there are risks 
of bias, contextual misalignment, and downstream issues, for example 
when problematic and opaque occupational evaluations of LLMs may feed 
back into working life. We also discuss the policy implications of our 
findings for the integration of LLM tools into the world of work.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2024-02-08</td>
					</tr>
					<tr>
					<th>Language</th>
						<td>en-us</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>A Technological Construction of Society</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>OSF Preprints</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://osf.io/8bc4n">https://osf.io/8bc4n</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>1/3/2025, 3:54:14 PM</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.31235/osf.io/8bc4n">10.31235/osf.io/8bc4n</a></td>
					</tr>
					<tr>
					<th>Repository</th>
						<td>OSF</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>1/3/2025, 3:54:14 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>AI</li>
					<li>artificial intelligence</li>
					<li>future of work</li>
					<li>generative AI</li>
					<li>GPT-4</li>
					<li>ICTs</li>
					<li>large language models</li>
					<li>LLMs</li>
					<li>occupational classification</li>
					<li>occupational prestige</li>
					<li>occupational social value</li>
					<li>occupations</li>
					<li>technology</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_K4DTZ9N7">OSF Preprint					</li>
				</ul>
			</li>


			<li id="item_8JT3JA2P" class="item journalArticle">
			<h2>A Two-Step Method for Classifying Political Partisanship Using Deep Learning Models</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Lingshu Hu</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Political partisanship constitutes a pivotal group identity 
that signiﬁcantly inﬂuences individuals’ voting behaviors and shapes 
their ideological and cultural perspectives. While traditional surveys 
and experimental studies can directly capture political identity by 
asking the participants, this task has become intricate when employing 
digital trace data sourced from social media. Previous classiﬁcation 
methods, attempting to infer political identity from users’ networks or 
textual content, suffered from limited efﬁciency or generalizability. In
 response, this study introduces a two-step method that utilizes deep 
learning models to enhance classiﬁcation efﬁciency, generalizability, 
and interpretability. In the ﬁrst step, two deep learning models, 
trained on 2.5 million tweets from 825 Congressional politicians in the 
U.S., achieved accuracy rates of 87.71% and 89.54%, respectively, in 
detecting politicians’ partisanships based on their individual tweets. 
Subsequently, in the second step, by employing a simple machine learning
 model that leverages the aggregated predicted values derived from the 
ﬁrst-step models, accuracy rates of 94.92% and 96.61% were attained for 
identifying non-politician users’ political identities based off their 
50 and 200 tweets, respectively. In addition, an attention mechanism was
 integrated into the deep learning model to assess the contribution of 
each word in the classiﬁcation process.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>08/2024</td>
					</tr>
					<tr>
					<th>Language</th>
						<td>en</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>DOI.org (Crossref)</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://journals.sagepub.com/doi/10.1177/08944393231219685">https://journals.sagepub.com/doi/10.1177/08944393231219685</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>12/24/2024, 6:39:03 PM</td>
					</tr>
					<tr>
					<th>Volume</th>
						<td>42</td>
					</tr>
					<tr>
					<th>Pages</th>
						<td>961-976</td>
					</tr>
					<tr>
					<th>Publication</th>
						<td>Social Science Computer Review</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.1177/08944393231219685">10.1177/08944393231219685</a></td>
					</tr>
					<tr>
					<th>Issue</th>
						<td>4</td>
					</tr>
					<tr>
					<th>Journal Abbr</th>
						<td>Social Science Computer Review</td>
					</tr>
					<tr>
					<th>ISSN</th>
						<td>0894-4393, 1552-8286</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>12/24/2024, 6:39:03 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_66AVSYGB">Hu - 2024 - A Two-Step Method for Classifying Political Partis.pdf					</li>
				</ul>
			</li>


			<li id="item_C8CRJRS7" class="item journalArticle">
			<h2>A view from anthropology: Should anthropologists fear the data  machines?</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Kristoffer Albris</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Eva I Otto</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Sofie L Astrupgaard</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Emilie Munch Gregersen</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Laura Skousgaard Jørgensen</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Olivia Jørgensen</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Clara Rosa Sandbye</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Signe Schønning</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>If you are an anthropologist wanting to use digital methods or
 programming as part of your research, where do you start? In this 
commentary, we discuss three ways in which anthropologists can use 
computational tools to enhance, support, and complement ethnographic 
methods. By presenting our reflections, we hope to contribute to the 
stirring conversations about the potential future role(s) of (social) 
data science vis-a-vis anthropology and ethnography, and to inspire 
other anthropologists to take up the use of digital methods, 
programming, and computational tools in their own research.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2021-07-01</td>
					</tr>
					<tr>
					<th>Language</th>
						<td>en</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>A view from anthropology</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>SAGE Journals</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://doi.org/10.1177/20539517211043655">https://doi.org/10.1177/20539517211043655</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>12/23/2024, 6:04:25 PM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>Publisher: SAGE Publications Ltd</td>
					</tr>
					<tr>
					<th>Volume</th>
						<td>8</td>
					</tr>
					<tr>
					<th>Pages</th>
						<td>20539517211043655</td>
					</tr>
					<tr>
					<th>Publication</th>
						<td>Big Data &amp; Society</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.1177/20539517211043655">10.1177/20539517211043655</a></td>
					</tr>
					<tr>
					<th>Issue</th>
						<td>2</td>
					</tr>
					<tr>
					<th>ISSN</th>
						<td>2053-9517</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>12/23/2024, 6:04:25 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_JPNXRV6Q">SAGE PDF Full Text					</li>
				</ul>
			</li>


			<li id="item_Q52SJ86G" class="item journalArticle">
			<h2>A view from data science</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Anna Sapienza</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Sune Lehmann</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>For better and worse, our world has been transformed by Big 
Data. To understand digital traces generated by individuals, we need to 
design multidisciplinary approaches that combine social and data 
science. Data and social scientists face the challenge of effectively 
building upon each other’s approaches to overcome the limitations 
inherent in each side. Here, we offer a “data science perspective” on 
the challenges that arise when working to establish this 
interdisciplinary environment. We discuss how we perceive the 
differences and commonalities of the questions we ask to understand 
digital behaviors (including how we answer them), and how our methods 
may complement each other. Finally, we describe what a path toward 
common ground between these fields looks like when viewed from data 
science.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2021-07-01</td>
					</tr>
					<tr>
					<th>Language</th>
						<td>en</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>SAGE Journals</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://doi.org/10.1177/20539517211040198">https://doi.org/10.1177/20539517211040198</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>9/10/2024, 5:24:23 PM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>Publisher: SAGE Publications Ltd</td>
					</tr>
					<tr>
					<th>Volume</th>
						<td>8</td>
					</tr>
					<tr>
					<th>Pages</th>
						<td>20539517211040198</td>
					</tr>
					<tr>
					<th>Publication</th>
						<td>Big Data &amp; Society</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.1177/20539517211040198">10.1177/20539517211040198</a></td>
					</tr>
					<tr>
					<th>Issue</th>
						<td>2</td>
					</tr>
					<tr>
					<th>ISSN</th>
						<td>2053-9517</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>9/10/2024, 5:24:23 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_QFENGFTU">SAGE PDF Full Text					</li>
				</ul>
			</li>


			<li id="item_YNFDUT57" class="item preprint">
			<h2>Accelerating science with human-aware artificial intelligence</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Preprint</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Jamshid Sourati</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>James Evans</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Artificial intelligence (AI) models trained on published 
scientific findings have been used to invent valuable materials and 
targeted therapies, but they typically ignore the human scientists who 
continually alter the landscape of discovery. Here we show that 
incorporating the distribution of human expertise by training 
unsupervised models on simulated inferences cognitively accessible to 
experts dramatically improves (up to 400%) AI prediction of future 
discoveries beyond those focused on research content alone, especially 
when relevant literature is sparse. These models succeed by predicting 
human predictions and the scientists who will make them. By tuning 
human-aware AI to avoid the crowd, we can generate scientifically 
promising "alien" hypotheses unlikely to be imagined or pursued without 
intervention until the distant future, which hold promise to punctuate 
scientific advance beyond questions currently pursued. Accelerating 
human discovery or probing its blind spots, human-aware AI enables us to
 move toward and beyond the contemporary scientific frontier.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2023-06-02</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>arXiv.org</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://arxiv.org/abs/2306.01495">http://arxiv.org/abs/2306.01495</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>12/24/2024, 5:10:21 PM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>arXiv:2306.01495 [cs]</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.48550/arXiv.2306.01495">10.48550/arXiv.2306.01495</a></td>
					</tr>
					<tr>
					<th>Repository</th>
						<td>arXiv</td>
					</tr>
					<tr>
					<th>Archive ID</th>
						<td>arXiv:2306.01495</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>12/24/2024, 5:10:21 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Computer Science - Artificial Intelligence</li>
					<li>Computer Science - Social and Information Networks</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_MS6HNLF3">Preprint PDF					</li>
					<li id="item_XNDFEYTF">Snapshot					</li>
				</ul>
			</li>


			<li id="item_WL2AABTD" class="item preprint">
			<h2>Accuracy and Political Bias of News Source Credibility Ratings by Large Language Models</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Preprint</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Kai-Cheng Yang</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Filippo Menczer</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Search engines increasingly leverage large language models 
(LLMs) to generate direct answers, and AI chatbots now access the 
Internet for fresh data. As information curators for billions of users, 
LLMs must assess the accuracy and reliability of different sources. This
 paper audits eight widely used LLMs from three major providers -- 
OpenAI, Google, and Meta -- to evaluate their ability to discern 
credible and high-quality information sources from low-credibility ones.
 We find that while LLMs can rate most tested news outlets, larger 
models more frequently refuse to provide ratings due to insufficient 
information, whereas smaller models are more prone to hallucination in 
their ratings. For sources where ratings are provided, LLMs exhibit a 
high level of agreement among themselves (average Spearman's $\rho = 
0.81$), but their ratings align only moderately with human expert 
evaluations (average $\rho = 0.59$). Analyzing news sources with 
different political leanings in the US, we observe a liberal bias in 
credibility ratings yielded by all LLMs in default configurations. 
Additionally, assigning partisan identities to LLMs consistently results
 in strong politically congruent bias in the ratings. These findings 
have important implications for the use of LLMs in curating news and 
political information.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2024-08-10</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>arXiv.org</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://arxiv.org/abs/2304.00228">http://arxiv.org/abs/2304.00228</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>1/8/2025, 3:12:43 PM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>arXiv:2304.00228 [cs]</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.48550/arXiv.2304.00228">10.48550/arXiv.2304.00228</a></td>
					</tr>
					<tr>
					<th>Repository</th>
						<td>arXiv</td>
					</tr>
					<tr>
					<th>Archive ID</th>
						<td>arXiv:2304.00228</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>1/8/2025, 3:12:43 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Computer Science - Computation and Language</li>
					<li>Computer Science - Computers and Society</li>
					<li>Computer Science - Information Retrieval</li>
				</ul>
				<h3 class="notes">Notes:</h3>
				<ul class="notes">
					<li id="item_YHCE4CER">
<p class="plaintext">Comment: 11 pages, 8 figures</p>
					</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_LIR5ZIG7">Preprint PDF					</li>
					<li id="item_BGMN62U8">Snapshot					</li>
				</ul>
			</li>


			<li id="item_UQ3C2BNU" class="item journalArticle">
			<h2>Adapting computational text analysis to social science (and vice versa)</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Paul DiMaggio</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Social scientists and computer scientist are divided by small 
differences in perspective and not by any significant disciplinary 
divide. In the field of text analysis, several such differences are 
noted: social scientists often use unsupervised models to explore 
corpora, whereas many computer scientists employ supervised models to 
train data; social scientists hold to more conventional causal notions 
than do most computer scientists, and often favor intense exploitation 
of existing algorithms, whereas computer scientists focus more on 
developing new models; and computer scientists tend to trust human 
judgment more than social scientists do. These differences have 
implications that potentially can improve the practice of social 
science.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2015-12-01</td>
					</tr>
					<tr>
					<th>Language</th>
						<td>en</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>SAGE Journals</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://doi.org/10.1177/2053951715602908">https://doi.org/10.1177/2053951715602908</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>3/14/2024, 10:58:47 AM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>Publisher: SAGE Publications Ltd</td>
					</tr>
					<tr>
					<th>Volume</th>
						<td>2</td>
					</tr>
					<tr>
					<th>Pages</th>
						<td>2053951715602908</td>
					</tr>
					<tr>
					<th>Publication</th>
						<td>Big Data &amp; Society</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.1177/2053951715602908">10.1177/2053951715602908</a></td>
					</tr>
					<tr>
					<th>Issue</th>
						<td>2</td>
					</tr>
					<tr>
					<th>ISSN</th>
						<td>2053-9517</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>3/14/2024, 10:58:47 AM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>3/14/2024, 10:58:47 AM</td>
					</tr>
				</tbody></table>
				<h3 class="notes">Notes:</h3>
				<ul class="notes">
					<li id="item_A4U6AU3W">
<div><div data-citation-items="%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F3957624%2Fitems%2FUQ3C2BNU%22%5D%2C%22itemData%22%3A%7B%22id%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F3957624%2Fitems%2FUQ3C2BNU%22%2C%22type%22%3A%22article-journal%22%2C%22abstract%22%3A%22Social%20scientists%20and%20computer%20scientist%20are%20divided%20by%20small%20differences%20in%20perspective%20and%20not%20by%20any%20significant%20disciplinary%20divide.%20In%20the%20field%20of%20text%20analysis%2C%20several%20such%20differences%20are%20noted%3A%20social%20scientists%20often%20use%20unsupervised%20models%20to%20explore%20corpora%2C%20whereas%20many%20computer%20scientists%20employ%20supervised%20models%20to%20train%20data%3B%20social%20scientists%20hold%20to%20more%20conventional%20causal%20notions%20than%20do%20most%20computer%20scientists%2C%20and%20often%20favor%20intense%20exploitation%20of%20existing%20algorithms%2C%20whereas%20computer%20scientists%20focus%20more%20on%20developing%20new%20models%3B%20and%20computer%20scientists%20tend%20to%20trust%20human%20judgment%20more%20than%20social%20scientists%20do.%20These%20differences%20have%20implications%20that%20potentially%20can%20improve%20the%20practice%20of%20social%20science.%22%2C%22container-title%22%3A%22Big%20Data%20%26%20Society%22%2C%22DOI%22%3A%2210.1177%2F2053951715602908%22%2C%22ISSN%22%3A%222053-9517%22%2C%22issue%22%3A%222%22%2C%22language%22%3A%22en%22%2C%22note%22%3A%22publisher%3A%20SAGE%20Publications%20Ltd%22%2C%22page%22%3A%222053951715602908%22%2C%22source%22%3A%22SAGE%20Journals%22%2C%22title%22%3A%22Adapting%20computational%20text%20analysis%20to%20social%20science%20(and%20vice%20versa)%22%2C%22URL%22%3A%22https%3A%2F%2Fdoi.org%2F10.1177%2F2053951715602908%22%2C%22volume%22%3A%222%22%2C%22author%22%3A%5B%7B%22family%22%3A%22DiMaggio%22%2C%22given%22%3A%22Paul%22%7D%5D%2C%22accessed%22%3A%7B%22date-parts%22%3A%5B%5B%222024%22%2C3%2C14%5D%5D%7D%2C%22issued%22%3A%7B%22date-parts%22%3A%5B%5B%222015%22%2C12%2C1%5D%5D%7D%7D%7D%5D" data-schema-version="8"><p>Notes</p>
<p></p>
<p>DiMaggio asserts that it is not the disciplinary divide, but a 
difference in perspective that separates CS and SOC researchers. CS 
preferring supervised and new models, SOC preferring unsupervised and 
causality, and exploring existing methods. (Implies opportunity for SOC 
to follow new methods, non-causal, supervised etc.)</p>
<p>The preference of SOC for unsupervised is surprising, supervised seems preferable to add in human context.</p>
<p>Core here is that DiMaggio says it is SocScis who must do extra work 
to adapt to CS’s techniques. I mostly agree. (does CS need to create 
methods specific to SS?)</p>
<p>DiMaggio notes differences in intent causedby this difference in 
perspective, SS look for (statistical) explanation/causality, CS find 
improvements in stronger models. They “understand ‘getting it right’ in a
 different way.” I do want to preserve, as a key point, that we as SS 
want to keep that explanation a higher priority.</p>
<p>Surprising is DiMaggio’s claim that SS are less trustworthy of human 
judgement than CS are. CS uses human judgement as gold standard, but SS 
understand the subjectivity/unreliability of human coders</p>
<p style="padding-left: 40px" data-indent="1">-indicative of the unavoidable fact that there is no objective baseline in SS matters.</p>
<p>‘<span class="highlight" data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F3957624%2Fitems%2FIEABXGKG%22%2C%22pageLabel%22%3A%224%22%2C%22position%22%3A%7B%22pageIndex%22%3A3%2C%22rects%22%3A%5B%5B61.568%2C283.33%2C284.959%2C292.516%5D%2C%5B49.606%2C271.368%2C284.958%2C280.554%5D%2C%5B49.606%2C259.463%2C284.958%2C268.648%5D%2C%5B49.606%2C247.501%2C282.193%2C256.686%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F3957624%2Fitems%2FUQ3C2BNU%22%5D%2C%22locator%22%3A%224%22%7D%7D">“Engagement
 with computational text analysis entails more than adapting new methods
 to social science research questions. It also requires social 
scientists to relax some of our own disciplinary biases”</span> <span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F3957624%2Fitems%2FUQ3C2BNU%22%5D%2C%22locator%22%3A%224%22%2C%22itemData%22%3A%7B%22id%22%3A39%2C%22type%22%3A%22article-journal%22%2C%22abstract%22%3A%22Social%20scientists%20and%20computer%20scientist%20are%20divided%20by%20small%20differences%20in%20perspective%20and%20not%20by%20any%20significant%20disciplinary%20divide.%20In%20the%20field%20of%20text%20analysis%2C%20several%20such%20differences%20are%20noted%3A%20social%20scientists%20often%20use%20unsupervised%20models%20to%20explore%20corpora%2C%20whereas%20many%20computer%20scientists%20employ%20supervised%20models%20to%20train%20data%3B%20social%20scientists%20hold%20to%20more%20conventional%20causal%20notions%20than%20do%20most%20computer%20scientists%2C%20and%20often%20favor%20intense%20exploitation%20of%20existing%20algorithms%2C%20whereas%20computer%20scientists%20focus%20more%20on%20developing%20new%20models%3B%20and%20computer%20scientists%20tend%20to%20trust%20human%20judgment%20more%20than%20social%20scientists%20do.%20These%20differences%20have%20implications%20that%20potentially%20can%20improve%20the%20practice%20of%20social%20science.%22%2C%22container-title%22%3A%22Big%20Data%20%26%20Society%22%2C%22DOI%22%3A%2210.1177%2F2053951715602908%22%2C%22ISSN%22%3A%222053-9517%22%2C%22issue%22%3A%222%22%2C%22language%22%3A%22en%22%2C%22note%22%3A%22publisher%3A%20SAGE%20Publications%20Ltd%22%2C%22page%22%3A%222053951715602908%22%2C%22source%22%3A%22SAGE%20Journals%22%2C%22title%22%3A%22Adapting%20computational%20text%20analysis%20to%20social%20science%20(and%20vice%20versa)%22%2C%22volume%22%3A%222%22%2C%22author%22%3A%5B%7B%22family%22%3A%22DiMaggio%22%2C%22given%22%3A%22Paul%22%7D%5D%2C%22issued%22%3A%7B%22date-parts%22%3A%5B%5B%222015%22%2C12%2C1%5D%5D%7D%7D%7D%5D%2C%22properties%22%3A%7B%7D%7D">(<span class="citation-item">DiMaggio, 2015, p. 4</span>)</span>”</p>
</div></div>
					</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_IEABXGKG">SAGE PDF Full Text					</li>
				</ul>
			</li>


			<li id="item_9PYX4R82" class="item journalArticle">
			<h2>Advancing Automated Content Analysis for a New Era of Media Effects Research: The Key Role of Transfer Learning</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Anne Kroon</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Kasper Welbers</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Damian Trilling</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Wouter van Atteveldt</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>The availability of individual-level digital trace data offers
 exciting new ways to study media uses and effects based on the actual 
content that people encountered. In this article, we argue that to 
really reap the benefits of this data, we need to update our methodology
 for automated text analysis. We review challenges for the automatic 
identification of theoretically relevant concepts in texts along three 
dimensions: format/style, language, and modality. These dimensions 
unveil a significantly higher level of diversity and complexity in 
individual-level digital trace data, as opposed to the content 
traditionally examined through automated text analysis in our field. 
Consequently, they provide a valuable perspective for exploring the 
limitations of traditional approaches. We argue that recent developments
 within the field of Natural Language Processing, in particular, 
transfer learning using transformer-based models, have the potential to 
aid the development, application, and performance of various 
computational tools. These tools can contribute to the meaningful 
categorization of the content of social (and other) media.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2024-04-02</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>Advancing Automated Content Analysis for a New Era of Media Effects Research</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>Taylor and Francis+NEJM</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://doi.org/10.1080/19312458.2023.2261372">https://doi.org/10.1080/19312458.2023.2261372</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>1/6/2025, 1:43:21 PM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>Publisher: Routledge
_eprint: https://doi.org/10.1080/19312458.2023.2261372</td>
					</tr>
					<tr>
					<th>Volume</th>
						<td>18</td>
					</tr>
					<tr>
					<th>Pages</th>
						<td>142-162</td>
					</tr>
					<tr>
					<th>Publication</th>
						<td>Communication Methods and Measures</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.1080/19312458.2023.2261372">10.1080/19312458.2023.2261372</a></td>
					</tr>
					<tr>
					<th>Issue</th>
						<td>2</td>
					</tr>
					<tr>
					<th>ISSN</th>
						<td>1931-2458</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>1/6/2025, 1:43:22 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_UV22NCEF">Full Text PDF					</li>
				</ul>
			</li>


			<li id="item_B5FVKNTK" class="item preprint">
			<h2>AFSPP: Agent Framework for Shaping Preference and Personality with Large Language Models</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Preprint</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Zihong He</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Changwang Zhang</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>The evolution of Large Language Models (LLMs) has introduced a
 new paradigm for investigating human behavior emulation. Recent 
research has employed LLM-based Agents to create a sociological research
 environment, in which agents exhibit behavior based on the unfiltered 
characteristics of large language models. However, these studies 
overlook the iterative development within a human-like setting - Human 
preferences and personalities are complex, shaped by various factors and
 subject to ongoing change as a result of environmental and subjective 
influences. In light of this observation, we propose Agent Framework for
 Shaping Preference and Personality (AFSPP), exploring the multifaceted 
impact of social networks and subjective consciousness on LLM-based 
Agents' preference and personality formation. With AFSPP, we have, for 
the first time, successfully replicated several key findings from human 
personality experiments. And other AFSPP-based experimental results 
indicate that plan making, sensory perceptions and social networking 
with subjective information, wield the most pronounced influence on 
preference shaping. AFSPP can significantly enhance the efficiency and 
scope of psychological experiments, while yielding valuable insights for
 Trustworthy Artificial Intelligence research for strategies to prevent 
undesirable preference and personality development.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2024-01-05</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>AFSPP</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>arXiv.org</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://arxiv.org/abs/2401.02870">http://arxiv.org/abs/2401.02870</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>1/7/2025, 4:54:21 PM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>arXiv:2401.02870 [cs]</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.48550/arXiv.2401.02870">10.48550/arXiv.2401.02870</a></td>
					</tr>
					<tr>
					<th>Repository</th>
						<td>arXiv</td>
					</tr>
					<tr>
					<th>Archive ID</th>
						<td>arXiv:2401.02870</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>1/7/2025, 4:54:21 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Computer Science - Artificial Intelligence</li>
					<li>Computer Science - Computation and Language</li>
					<li>Computer Science - Multiagent Systems</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_GXGGB6F6">Preprint PDF					</li>
					<li id="item_2AJLI47A">Snapshot					</li>
				</ul>
			</li>


			<li id="item_EBRLAYRT" class="item preprint">
			<h2>Agentic Society: Merging skeleton from real world and texture from Large Language Model</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Preprint</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Yuqi Bai</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Kun Sun</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Huishi Yin</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Recent advancements in large language models (LLMs) and agent 
technologies offer promising solutions to the simulation of social 
science experiments, but the availability of data of real-world 
population required by many of them still poses as a major challenge. 
This paper explores a novel framework that leverages census data and 
LLMs to generate virtual populations, significantly reducing resource 
requirements and bypassing privacy compliance issues associated with 
real-world data, while keeping a statistical truthfulness. Drawing on 
real-world census data, our approach first generates a persona that 
reflects demographic characteristics of the population. We then employ 
LLMs to enrich these personas with intricate details, using techniques 
akin to those in image generative models but applied to textual data. 
Additionally, we propose a framework for the evaluation of the 
feasibility of our method with respect to capability of LLMs based on 
personality trait tests, specifically the Big Five model, which also 
enhances the depth and realism of the generated personas. Through 
preliminary experiments and analysis, we demonstrate that our method 
produces personas with variability essential for simulating diverse 
human behaviors in social science experiments. But the evaluation result
 shows that only weak sign of statistical truthfulness can be produced 
due to limited capability of current LLMs. Insights from our study also 
highlight the tension within LLMs between aligning with human values and
 reflecting real-world complexities. Thorough and rigorous test call for
 further research. Our codes are released at 
https://github.com/baiyuqi/agentic-society.git</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2024-09-02</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>Agentic Society</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>arXiv.org</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://arxiv.org/abs/2409.10550">http://arxiv.org/abs/2409.10550</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>1/7/2025, 5:08:08 PM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>arXiv:2409.10550 [cs]</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.48550/arXiv.2409.10550">10.48550/arXiv.2409.10550</a></td>
					</tr>
					<tr>
					<th>Repository</th>
						<td>arXiv</td>
					</tr>
					<tr>
					<th>Archive ID</th>
						<td>arXiv:2409.10550</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>1/7/2025, 5:08:08 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Computer Science - Computation and Language</li>
					<li>Computer Science - Computers and Society</li>
				</ul>
				<h3 class="notes">Notes:</h3>
				<ul class="notes">
					<li id="item_AF5CCQZL">
<p class="plaintext">Comment: 16 pages, 5 figures and 4 tables</p>
					</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_4WZJQ7XC">Preprint PDF					</li>
					<li id="item_NAM92AZ9">Snapshot					</li>
				</ul>
			</li>


			<li id="item_22BPFEMP" class="item preprint">
			<h2>AgentReview: Exploring Peer Review Dynamics with LLM Agents</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Preprint</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Yiqiao Jin</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Qinlin Zhao</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Yiyang Wang</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Hao Chen</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Kaijie Zhu</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Yijia Xiao</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Jindong Wang</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Peer review is fundamental to the integrity and advancement of
 scientific publication. Traditional methods of peer review analyses 
often rely on exploration and statistics of existing peer review data, 
which do not adequately address the multivariate nature of the process, 
account for the latent variables, and are further constrained by privacy
 concerns due to the sensitive nature of the data. We introduce 
AgentReview, the first large language model (LLM) based peer review 
simulation framework, which effectively disentangles the impacts of 
multiple latent factors and addresses the privacy issue. Our study 
reveals significant insights, including a notable 37.1% variation in 
paper decisions due to reviewers' biases, supported by sociological 
theories such as the social influence theory, altruism fatigue, and 
authority bias. We believe that this study could offer valuable insights
 to improve the design of peer review mechanisms. Our code is available 
at https://github.com/Ahren09/AgentReview.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2024-10-13</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>AgentReview</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>arXiv.org</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://arxiv.org/abs/2406.12708">http://arxiv.org/abs/2406.12708</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>1/7/2025, 4:48:04 PM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>arXiv:2406.12708 [cs]</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.48550/arXiv.2406.12708">10.48550/arXiv.2406.12708</a></td>
					</tr>
					<tr>
					<th>Repository</th>
						<td>arXiv</td>
					</tr>
					<tr>
					<th>Archive ID</th>
						<td>arXiv:2406.12708</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>1/7/2025, 4:48:04 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Computer Science - Computation and Language</li>
				</ul>
				<h3 class="notes">Notes:</h3>
				<ul class="notes">
					<li id="item_YNQTLXFW">
<p class="plaintext">Comment: Accepted at EMNLP 2024. https://agentreview.github.io/</p>
					</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_X3BC2CWY">Preprint PDF					</li>
					<li id="item_S44GPLTA">Snapshot					</li>
				</ul>
			</li>


			<li id="item_WE6QXKUM" class="item journalArticle">
			<h2>Aggregate, Integrate and Align to Embed Everything: A Multi-Modal Framework for Measuring Cultural Dynamics</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Bhargav Srinivasa Desikan</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>James Evans</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>The massive sensing of cultural action served through online 
platforms, and the aggregation of vast samples of cultural artifacts 
(e.g newspapers, images) has provided us with an unprecedented 
opportunity to embed cultural acts and artifacts in high dimensional 
representation space for analysis, comparison, and the efficient 
interactive elicitation of new cultural data, as from ordinal embedding.
 Existing research on culture and deep learning often embeds a single 
modality - text, images, more infrequently, networks and tables, rarely 
smallsample qualitative cultural associations, and never together. In 
this position paper, we propose that cultural data, their creation, 
proliferation, and consumption should be studied together and ongoingly 
elicited in the context of multi-modal neural representations. In short:
 embed everything! Cultural representations that align text, images, 
graphs, tables, elicited qualitative associations, and more can capture 
and compare complex cultural associations and reveal biases within 
dominant representations that neither capture nor serve those less 
present in training data. Within a high-dimensional representation 
space, we can use distance measures to study the full-spectrum influence
 of events, the dynamics of cultural change, and identify clear 
distinctions between representations native to different populations.</td>
					</tr>
					<tr>
					<th>Language</th>
						<td>en</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>Zotero</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>12/24/2024, 5:20:52 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_DREKQ5FL">Desikan and Evans - Aggregate, Integrate and Align to Embed Everything.pdf					</li>
				</ul>
			</li>


			<li id="item_YNKG22QA" class="item preprint">
			<h2>Agree to Disagree? Human and LLM coder bias for constructs of marginalization</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Preprint</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Ahrabhi Kathirgamalingam</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Fabienne Lind</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Jana Bernhard-Harrer</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Hajo G. Boomgaarden</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Human and LLM-based annotation of constructs of 
marginalization such as discrimination or hate speech comes with 
challenges. One such challenge is coder bias, the systematic variation 
in annotations driven by individual characterstics. To systematically 
understand the presence and sources of such biases, we conducted two 
studies and compared both. In Study 1, we surveyed crowdworkers on 
marginalization-related characteristics and analyzed their annotations 
of racism. In Study 2, we assigned personas to LLMs and studied 
variations in the annotations. Moreover, we compared human and LLM coder
 bias from a text-level perspective to account for textual properties 
driving variation. Our findings suggest that being affected by or aware 
of marginalization causes systematic variation in human annotations, 
while persona assignment significantly impacts LLM outputs. Against this
 backdrop, we advocate for ‘inclusive annotation’ which introduces 
variance by intentionally integrating coders with lived experience or 
awareness of the constructs, thereby enriching measurement quality.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2024-10-08</td>
					</tr>
					<tr>
					<th>Language</th>
						<td>en-us</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>Agree to Disagree?</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>OSF Preprints</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://osf.io/agpyr">https://osf.io/agpyr</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>1/6/2025, 11:44:07 AM</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.31235/osf.io/agpyr">10.31235/osf.io/agpyr</a></td>
					</tr>
					<tr>
					<th>Repository</th>
						<td>OSF</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>1/6/2025, 11:44:07 AM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>annotator bias</li>
					<li>coder bias</li>
					<li>content analysis</li>
					<li>intercoder reliability</li>
					<li>LLM</li>
					<li>manual annotation</li>
					<li>marginalization</li>
					<li>text-as-data</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_27Y8YVM9">OSF Preprint					</li>
				</ul>
			</li>


			<li id="item_CC3A2RVV" class="item journalArticle">
			<h2>Agreements ‘in the wild’: Standards and alignment in machine learning benchmark dataset construction</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Isak Engdahl</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>This article presents an ethnographic case study of a 
corporate-academic group constructing a benchmark dataset of daily 
activities for a variety of machine learning and computer vision tasks. 
Using a socio-technical perspective, the article conceptualizes the 
dataset as a knowledge object that is stabilized by both practical 
standards (for daily activities, datafication, annotation and 
benchmarks) and alignment work – that is, efforts including forging 
agreements to make these standards effective in practice. By attending 
to alignment work, the article highlights the informal, communicative 
and supportive efforts that underlie the success of standards and the 
smoothing of tensions between actors and factors. Emphasizing these 
efforts constitutes a contribution in several ways. This article's 
ethnographic mode of analysis challenges and supplements quantitative 
metrics on datasets. It advances the field of dataset analysis by 
offering a detailed empirical examination of the development of a new 
benchmark dataset as a collective accomplishment. By showing the 
importance of alignment efforts and their close ties to standards and 
their limitations, it adds to our understanding of how machine learning 
datasets are built. And, most importantly, it calls into question a key 
characterization of the dataset: that it captures unscripted activities 
occurring naturally ‘in the wild’, as alignment work bleeds into moments
 of data capture.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2024-06-01</td>
					</tr>
					<tr>
					<th>Language</th>
						<td>en</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>Agreements ‘in the wild’</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>SAGE Journals</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://doi.org/10.1177/20539517241242457">https://doi.org/10.1177/20539517241242457</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>12/23/2024, 4:47:05 PM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>Publisher: SAGE Publications Ltd</td>
					</tr>
					<tr>
					<th>Volume</th>
						<td>11</td>
					</tr>
					<tr>
					<th>Pages</th>
						<td>20539517241242457</td>
					</tr>
					<tr>
					<th>Publication</th>
						<td>Big Data &amp; Society</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.1177/20539517241242457">10.1177/20539517241242457</a></td>
					</tr>
					<tr>
					<th>Issue</th>
						<td>2</td>
					</tr>
					<tr>
					<th>ISSN</th>
						<td>2053-9517</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>12/23/2024, 4:47:05 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_3HFVGTL3">SAGE PDF Full Text					</li>
				</ul>
			</li>


			<li id="item_PTGISBI7" class="item journalArticle">
			<h2>AI Empire: Unraveling the interlocking systems of oppression in generative AI's global order</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Jasmina Tacheva</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Srividya Ramasubramanian</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>As artificial intelligence (AI) continues to captivate the 
collective imagination through the latest generation of generative AI 
models such as DALL-E and ChatGPT, the dehumanizing and harmful features
 of the technology industry that have plagued it since its inception 
only seem to deepen and intensify. Far from a “glitch” or unintentional 
error, these endemic issues are a function of the interlocking systems 
of oppression upon which AI is built. Using the analytical framework of 
“Empire,” this paper demonstrates that we live not simply in the “age of
 AI” but in the age of AI Empire. Specifically, we show that this 
networked and distributed global order is rooted in heteropatriarchy, 
racial capitalism, white supremacy, and coloniality and perpetuates its 
influence through the mechanisms of extractivism, automation, 
essentialism, surveillance, and containment. Therefore, we argue that 
any attempt at reforming AI from within the same interlocking oppressive
 systems that created it is doomed to failure and, moreover, risks 
exacerbating existing harm. Instead, to advance justice, we must 
radically transform not just the technology itself, but our ideas about 
it, and develop it from the bottom up, from the perspectives of those 
who stand the most risk of being harmed.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2023-07-01</td>
					</tr>
					<tr>
					<th>Language</th>
						<td>en</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>AI Empire</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>SAGE Journals</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://doi.org/10.1177/20539517231219241">https://doi.org/10.1177/20539517231219241</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>12/23/2024, 5:35:43 PM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>Publisher: SAGE Publications Ltd</td>
					</tr>
					<tr>
					<th>Volume</th>
						<td>10</td>
					</tr>
					<tr>
					<th>Pages</th>
						<td>20539517231219241</td>
					</tr>
					<tr>
					<th>Publication</th>
						<td>Big Data &amp; Society</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.1177/20539517231219241">10.1177/20539517231219241</a></td>
					</tr>
					<tr>
					<th>Issue</th>
						<td>2</td>
					</tr>
					<tr>
					<th>ISSN</th>
						<td>2053-9517</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>12/23/2024, 5:35:43 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_9SVZBFNH">SAGE PDF Full Text					</li>
				</ul>
			</li>


			<li id="item_JNXE5MDE" class="item preprint">
			<h2>AI Expands Scientists' Impact but Contracts Science's Focus</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Preprint</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Qianyue Hao</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Fengli Xu</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Yong Li</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>James Evans</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>The rapid rise of AI in science presents a paradox. Analyzing 
67.9 million research papers across six major fields using a validated 
language model (F1=0.876), we explore AI's impact on science. Scientists
 who adopt AI tools publish 67.37% more papers, receive 3.16 times more 
citations, and become team leaders 4 years earlier than non-adopters. 
This individual success correlates with concerning on collective 
effects: AI-augmented research contracts the diameter of scientific 
topics studied, and diminishes follow-on scientific engagement. Rather 
than catalyzing the exploration of new fields, AI accelerates work in 
established, data-rich domains. This pattern suggests that while AI 
enhances individual scientific productivity, it may simultaneously 
reduce scientific diversity and broad engagement, highlighting a tension
 between personal advancement and collective scientific progress.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2024-12-10</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>arXiv.org</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://arxiv.org/abs/2412.07727">http://arxiv.org/abs/2412.07727</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>12/24/2024, 4:50:29 PM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>arXiv:2412.07727 [cs]</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.48550/arXiv.2412.07727">10.48550/arXiv.2412.07727</a></td>
					</tr>
					<tr>
					<th>Repository</th>
						<td>arXiv</td>
					</tr>
					<tr>
					<th>Archive ID</th>
						<td>arXiv:2412.07727</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>12/24/2024, 4:50:29 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Computer Science - Computers and Society</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_IM9B87HX">Preprint PDF					</li>
					<li id="item_WHFEBUUF">Snapshot					</li>
				</ul>
			</li>


			<li id="item_AGX875MI" class="item preprint">
			<h2>AI for social science and social science of AI: A Survey</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Preprint</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Ruoxi Xu</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Yingfei Sun</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Mengjie Ren</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Shiguang Guo</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Ruotong Pan</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Hongyu Lin</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Le Sun</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Xianpei Han</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Recent advancements in artificial intelligence, particularly 
with the emergence of large language models (LLMs), have sparked a 
rethinking of artificial general intelligence possibilities. The 
increasing human-like capabilities of AI are also attracting attention 
in social science research, leading to various studies exploring the 
combination of these two fields. In this survey, we systematically 
categorize previous explorations in the combination of AI and social 
science into two directions that share common technical approaches but 
differ in their research objectives. The first direction is focused on 
AI for social science, where AI is utilized as a powerful tool to 
enhance various stages of social science research. While the second 
direction is the social science of AI, which examines AI agents as 
social entities with their human-like cognitive and linguistic 
capabilities. By conducting a thorough review, particularly on the 
substantial progress facilitated by recent advancements in large 
language models, this paper introduces a fresh perspective to reassess 
the relationship between AI and social science, provides a cohesive 
framework that allows researchers to understand the distinctions and 
connections between AI for social science and social science of AI, and 
also summarized state-of-art experiment simulation platforms to 
facilitate research in these two directions. We believe that as AI 
technology continues to advance and intelligent agents find increasing 
applications in our daily lives, the significance of the combination of 
AI and social science will become even more prominent.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2024-01-22</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>AI for social science and social science of AI</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>arXiv.org</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://arxiv.org/abs/2401.11839">http://arxiv.org/abs/2401.11839</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>1/7/2025, 5:34:29 PM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>arXiv:2401.11839 [cs]</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.48550/arXiv.2401.11839">10.48550/arXiv.2401.11839</a></td>
					</tr>
					<tr>
					<th>Repository</th>
						<td>arXiv</td>
					</tr>
					<tr>
					<th>Archive ID</th>
						<td>arXiv:2401.11839</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>1/7/2025, 5:34:29 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Computer Science - Computation and Language</li>
					<li>Computer Science - Computers and Society</li>
				</ul>
				<h3 class="notes">Notes:</h3>
				<ul class="notes">
					<li id="item_KDH3EQ2T">
<div>Comment: Accepted by Information Processing and Management (IP&amp;M)</div>
					</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_PZTMM99D">Preprint PDF					</li>
					<li id="item_9JC52TC2">Snapshot					</li>
				</ul>
			</li>


			<li id="item_QYRJ7W36" class="item preprint">
			<h2>AI-Augmented Cultural Sociology: Guidelines for LLM-assisted text analysis and an illustrative example</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Preprint</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Rens Wilderom</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Taylor Price</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Tom Heitland</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>The advent of large language models (LLMs) presents a promising opportunity for how we
analyze text and, by extension, can study the role of culture and symbolic meanings in social
life. Using an illustrative example focused on the concept of “personalized service” within
Michelin-starred restaurants, this research note demonstrates how LLMs can reliably identify
complex, multifaceted concepts similarly to a qualitative data analyst, but in a more scalable
manner. We extend existing validation approaches, offering guidelines on the amount of
manually coded data needed to evaluate LLM-generated outputs, drawing on sampling theory
and a data simulation. We also discuss broader applications of LLMs in cultural sociology,
such as investigations on established concepts (e.g., cultural consecration) and emerging
concepts (e.g., future-oriented deliberation). This discussion underscores that AI-tools can
significantly augment the empirical scope of research projects, building on rather than
replacing traditional qualitative approaches. Our study ultimately advocates for an optimistic
yet cautious engagement with AI-tools in social scientific inquiry, highlighting both their
analytic potential and the need for ongoing reflection on their ethical implications.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2024-12-02</td>
					</tr>
					<tr>
					<th>Language</th>
						<td>en-us</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>AI-Augmented Cultural Sociology</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>OSF Preprints</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://osf.io/tx8jn">https://osf.io/tx8jn</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>1/6/2025, 11:46:28 AM</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.31235/osf.io/tx8jn">10.31235/osf.io/tx8jn</a></td>
					</tr>
					<tr>
					<th>Repository</th>
						<td>OSF</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>1/6/2025, 11:46:28 AM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>cultural consecration</li>
					<li>cultural sociology</li>
					<li>future-oriented deliberation</li>
					<li>large language models</li>
					<li>personalized service</li>
					<li>text analysis</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_K977HH9M">OSF Preprint					</li>
				</ul>
			</li>


			<li id="item_A7PFZP6A" class="item preprint">
			<h2>AI-Augmented Surveys: Leveraging Large Language Models and Surveys for Opinion Prediction</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Preprint</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Junsol Kim</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Byungkyu Lee</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Large language models (LLMs) that produce human-like responses
 have begun to revolutionize research practices in the social sciences. 
We develop a novel methodological framework that fine-tunes LLMs with 
repeated cross-sectional surveys to incorporate the meaning of survey 
questions, individual beliefs, and temporal contexts for opinion 
prediction. We introduce two new emerging applications of the 
AI-augmented survey: retrodiction (i.e., predict year-level missing 
responses) and unasked opinion prediction (i.e., predict entirely 
missing responses). Among 3,110 binarized opinions from 68,846 Americans
 in the General Social Survey from 1972 to 2021, our models based on 
Alpaca-7b excel in retrodiction (AUC = 0.86 for personal opinion 
prediction, $\rho$ = 0.98 for public opinion prediction). These 
remarkable prediction capabilities allow us to fill in missing trends 
with high confidence and pinpoint when public attitudes changed, such as
 the rising support for same-sex marriage. On the other hand, our 
fine-tuned Alpaca-7b models show modest success in unasked opinion 
prediction (AUC = 0.73, $\rho$ = 0.67). We discuss practical constraints
 and ethical concerns regarding individual autonomy and privacy when 
using LLMs for opinion prediction. Our study demonstrates that LLMs and 
surveys can mutually enhance each other's capabilities: LLMs can broaden
 survey potential, while surveys can improve the alignment of LLMs.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2024-04-07</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>AI-Augmented Surveys</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>arXiv.org</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://arxiv.org/abs/2305.09620">http://arxiv.org/abs/2305.09620</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>1/7/2025, 5:39:28 PM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>arXiv:2305.09620 [cs]</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.48550/arXiv.2305.09620">10.48550/arXiv.2305.09620</a></td>
					</tr>
					<tr>
					<th>Repository</th>
						<td>arXiv</td>
					</tr>
					<tr>
					<th>Archive ID</th>
						<td>arXiv:2305.09620</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>1/7/2025, 5:39:28 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Computer Science - Artificial Intelligence</li>
					<li>Computer Science - Computation and Language</li>
					<li>Computer Science - Machine Learning</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_RREPFHEH">Preprint PDF					</li>
					<li id="item_GQ7JIUZ7">Snapshot					</li>
				</ul>
			</li>


			<li id="item_A7L3FMMS" class="item journalArticle">
			<h2>Algorithmic failure as a humanities methodology: Machine learning's mispredictions identify rich cases for qualitative analysis</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Jill Walker Rettberg</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>This commentary tests a methodology proposed by Munk et al. 
(2022) for using failed predictions in machine learning as a method to 
identify ambiguous and rich cases for qualitative analysis. Using a 
dataset describing actions performed by fictional characters interacting
 with machine vision technologies in 500 artworks, movies, novels and 
videogames, I trained a simple machine learning algorithm (using the kNN
 algorithm in R) to predict whether or not an action was active or 
passive using only information about the fictional characters. 
Predictable actions were generally unemotional and unambiguous 
activities where machine vision technologies were treated as simple 
tools. Unpredictable actions, that is, actions that the algorithm could 
not correctly predict, were more ambivalent and emotionally loaded, with
 more complex power relationships between characters and technologies. 
The results thus support Munk et al.'s theory that failed predictions 
can be productively used to identify rich cases for qualitative 
analysis. This test goes beyond simply replicating Munk et al.'s results
 by demonstrating that the method can be applied to a broader humanities
 domain, and that it does not require complex neural networks but can 
also work with a simpler machine learning algorithm. Further research is
 needed to develop an understanding of what kinds of data the method is 
useful for and which kinds of machine learning are most generative. To 
support this, the R code required to produce the results is included so 
the test can be replicated. The code can also be reused or adapted to 
test the method on other datasets.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2022-07-01</td>
					</tr>
					<tr>
					<th>Language</th>
						<td>en</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>Algorithmic failure as a humanities methodology</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>SAGE Journals</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://doi.org/10.1177/20539517221131290">https://doi.org/10.1177/20539517221131290</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>12/23/2024, 5:47:33 PM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>Publisher: SAGE Publications Ltd</td>
					</tr>
					<tr>
					<th>Volume</th>
						<td>9</td>
					</tr>
					<tr>
					<th>Pages</th>
						<td>20539517221131290</td>
					</tr>
					<tr>
					<th>Publication</th>
						<td>Big Data &amp; Society</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.1177/20539517221131290">10.1177/20539517221131290</a></td>
					</tr>
					<tr>
					<th>Issue</th>
						<td>2</td>
					</tr>
					<tr>
					<th>ISSN</th>
						<td>2053-9517</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>12/23/2024, 5:47:33 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_Z37GEHB2">SAGE PDF Full Text					</li>
				</ul>
			</li>


			<li id="item_5AVTGZXH" class="item journalArticle">
			<h2>An AI-based framework for studying visual diversity of urban neighborhoods and its relationship with socio-demographic variables</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Md Amiruzzaman</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Ye Zhao</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Stefanie Amiruzzaman</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Aryn C. Karpinski</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Tsung Heng Wu</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>This study presents a framework to study quantitatively 
geographical visual diversities of urban neighborhood from a large 
collection of street-view images using an Artificial Intelligence 
(AI)-based image segmentation technique. A variety of diversity indices 
are computed from the extracted visual semantics. They are utilized to 
discover the relationships between urban visual appearance and 
socio-demographic variables. This study also validates the reliability 
of the method with human evaluators. The methodology and results 
obtained from this study can potentially be used to study urban 
features, locate houses, establish services, and better operate 
municipalities.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2023-04-01</td>
					</tr>
					<tr>
					<th>Language</th>
						<td>en</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>Springer Link</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://doi.org/10.1007/s42001-022-00197-1">https://doi.org/10.1007/s42001-022-00197-1</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>1/3/2025, 3:04:26 PM</td>
					</tr>
					<tr>
					<th>Volume</th>
						<td>6</td>
					</tr>
					<tr>
					<th>Pages</th>
						<td>315-337</td>
					</tr>
					<tr>
					<th>Publication</th>
						<td>Journal of Computational Social Science</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.1007/s42001-022-00197-1">10.1007/s42001-022-00197-1</a></td>
					</tr>
					<tr>
					<th>Issue</th>
						<td>1</td>
					</tr>
					<tr>
					<th>Journal Abbr</th>
						<td>J Comput Soc Sc</td>
					</tr>
					<tr>
					<th>ISSN</th>
						<td>2432-2725</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>1/3/2025, 3:04:26 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Google street-view</li>
					<li>Interrater reliability</li>
					<li>Semantic segmentation</li>
					<li>Social phenomena</li>
					<li>Urban neighborhood</li>
					<li>Visual diversity</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_62NNNEB7">Full Text PDF					</li>
				</ul>
			</li>


			<li id="item_RUHTXIVQ" class="item journalArticle">
			<h2>An Informed Neural Network for Discovering Historical 
Documentation Assisting the Repatriation of Indigenous Ancestral Human 
Remains</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Md Abul Bashar</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Richi Nayak</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Gareth Knapman</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Paul Turnbull</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Cressida Fforde</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Among the pressing issues facing Australian and other First 
Nations peoples is the repatriation of the bodily remains of their 
ancestors, which are currently held in Western scientific institutions. 
The success of securing the return of these remains to their communities
 for reburial depends largely on locating information within scientific 
and other literature published between 1790 and 1970 documenting their 
theft, donation, sale, or exchange between institutions. This article 
reports on collaborative research by data scientists and social science 
researchers in the Research, Reconcile, Renew Network (RRR) to develop 
and apply text mining techniques to identify this vital information. We 
describe our work to date on developing a machine learning-based 
solution to automate the process of finding and semantically analysing 
relevant texts. Classification models, particularly deep learning-based 
models, are known to have low accuracy when trained with small amounts 
of labelled (i.e. relevant/non-relevant) documents. To improve the 
accuracy of our detection model, we explore the use of an Informed 
Neural Network (INN) model that describes documentary content using 
expert-informed contextual knowledge. Only a few labelled documents are 
used to provide specificity to the model, using conceptually related 
keywords identified by RRR experts in provenance research. The results 
confirm the value of using an INN network model for identifying relevant
 documents related to the investigation of the global commercial trade 
in Indigenous human remains. Empirical analysis suggests that this INN 
model can be generalized for use by other researchers in the social 
sciences and humanities who want to extract relevant information from 
large textual corpora.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2023-12-01</td>
					</tr>
					<tr>
					<th>Language</th>
						<td>en</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>SAGE Journals</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://doi.org/10.1177/08944393231158788">https://doi.org/10.1177/08944393231158788</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>12/24/2024, 7:33:32 PM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>Publisher: SAGE Publications Inc</td>
					</tr>
					<tr>
					<th>Volume</th>
						<td>41</td>
					</tr>
					<tr>
					<th>Pages</th>
						<td>2293-2317</td>
					</tr>
					<tr>
					<th>Publication</th>
						<td>Social Science Computer Review</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.1177/08944393231158788">10.1177/08944393231158788</a></td>
					</tr>
					<tr>
					<th>Issue</th>
						<td>6</td>
					</tr>
					<tr>
					<th>ISSN</th>
						<td>0894-4393</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>12/24/2024, 7:33:32 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_LRUIWXDH">Accepted Version					</li>
				</ul>
			</li>


			<li id="item_ILKCIBVQ" class="item preprint">
			<h2>An Interdisciplinary Outlook on Large Language Models for Scientific Research</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Preprint</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>James Boyko</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Joseph Cohen</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Nathan Fox</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Maria Han Veiga</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Jennifer I.-Hsiu Li</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Jing Liu</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Bernardo Modenesi</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Andreas H. Rauch</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Kenneth N. Reid</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Soumi Tribedi</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Anastasia Visheratina</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Xin Xie</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>In this paper, we describe the capabilities and constraints of
 Large Language Models (LLMs) within disparate academic disciplines, 
aiming to delineate their strengths and limitations with precision. We 
examine how LLMs augment scientific inquiry, offering concrete examples 
such as accelerating literature review by summarizing vast numbers of 
publications, enhancing code development through automated syntax 
correction, and refining the scientific writing process. Simultaneously,
 we articulate the challenges LLMs face, including their reliance on 
extensive and sometimes biased datasets, and the potential ethical 
dilemmas stemming from their use. Our critical discussion extends to the
 varying impacts of LLMs across fields, from the natural sciences, where
 they help model complex biological sequences, to the social sciences, 
where they can parse large-scale qualitative data. We conclude by 
offering a nuanced perspective on how LLMs can be both a boon and a 
boundary to scientific progress.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2023-11-03</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>arXiv.org</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://arxiv.org/abs/2311.04929">http://arxiv.org/abs/2311.04929</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>1/7/2025, 5:42:46 PM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>arXiv:2311.04929 [cs]</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.48550/arXiv.2311.04929">10.48550/arXiv.2311.04929</a></td>
					</tr>
					<tr>
					<th>Repository</th>
						<td>arXiv</td>
					</tr>
					<tr>
					<th>Archive ID</th>
						<td>arXiv:2311.04929</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>1/7/2025, 5:42:46 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Computer Science - Artificial Intelligence</li>
					<li>Computer Science - Computation and Language</li>
					<li>Computer Science - Digital Libraries</li>
					<li>Computer Science - Machine Learning</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_NBI69E75">Preprint PDF					</li>
					<li id="item_PPVSJUTR">Snapshot					</li>
				</ul>
			</li>


			<li id="item_5HRN7BIT" class="item journalArticle">
			<h2>Analysis and classification of privacy-sensitive content in social media posts</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Livio Bioglio</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Ruggero G. Pensa</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>User-generated contents often contain private information, 
even when they are shared publicly on social media and on the web in 
general. Although many filtering and natural language approaches for 
automatically detecting obscenities or hate speech have been proposed, 
determining whether a shared post contains sensitive information is 
still an open issue. The problem has been addressed by assuming, for 
instance, that sensitive contents are published anonymously, on 
anonymous social media platforms or with more restrictive privacy 
settings, but these assumptions are far from being realistic, since the 
authors of posts often underestimate or overlook their actual exposure 
to privacy risks. Hence, in this paper, we address the problem of 
content sensitivity analysis directly, by presenting and characterizing a
 new annotated corpus with around ten thousand posts, each one annotated
 as sensitive or non-sensitive by a pool of experts. We characterize our
 data with respect to the closely-related problem of self-disclosure, 
pointing out the main differences between the two tasks. We also present
 the results of several deep neural network models that outperform 
previous naive attempts of classifying social media posts according to 
their sensitivity, and show that state-of-the-art approaches based on 
anonymity and lexical analysis do not work in realistic application 
scenarios.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2022/12</td>
					</tr>
					<tr>
					<th>Language</th>
						<td>en</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>epjdatascience.springeropen.com</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://epjdatascience.springeropen.com/articles/10.1140/epjds/s13688-022-00324-y">https://epjdatascience.springeropen.com/articles/10.1140/epjds/s13688-022-00324-y</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>1/3/2025, 12:44:52 PM</td>
					</tr>
					<tr>
					<th>Rights</th>
						<td>2022 The Author(s)</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>Number: 1
Publisher: SpringerOpen</td>
					</tr>
					<tr>
					<th>Volume</th>
						<td>11</td>
					</tr>
					<tr>
					<th>Pages</th>
						<td>1-24</td>
					</tr>
					<tr>
					<th>Publication</th>
						<td>EPJ Data Science</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.1140/epjds/s13688-022-00324-y">10.1140/epjds/s13688-022-00324-y</a></td>
					</tr>
					<tr>
					<th>Issue</th>
						<td>1</td>
					</tr>
					<tr>
					<th>Journal Abbr</th>
						<td>EPJ Data Sci.</td>
					</tr>
					<tr>
					<th>ISSN</th>
						<td>2193-1127</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>1/3/2025, 12:44:52 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_VMHTY2HC">Full Text PDF					</li>
				</ul>
			</li>


			<li id="item_9468Q3SF" class="item journalArticle">
			<h2>Arab reactions towards Russo-Ukrainian war</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Moayadeldin Tamer</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Mohamed A. Khamis</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Abdallah Yahia</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>SeifALdin Khaled</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Abdelrahman Ashraf</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Walid Gomaa</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>The aim of this paper is to analyze the Arab peoples reactions
 and attitudes towards the Russo-Ukraine War through the social media of
 posted tweets, as a fast means to express opinions. We scrapped over 3 
million tweets using some keywords that are related to the war and 
performed sentiment, emotion, and partiality analyses. For sentiment 
analysis, we employed a voting technique of several pre-trained Arabic 
language foundational models. For emotion analysis, we utilized a 
pre-constructed emotion lexicon. The partiality is analyzed through 
classifying tweets as being ‘Pro-Russia’, ‘Pro-Ukraine’, or ‘Neither’; 
and it indicates the bias or empathy towards either of the conflicting 
parties. This was achieved by constructing a weighted lexicon of n-grams
 related to either side. We found that the majority of the tweets 
carried ‘Negative’ sentiment. Emotions were not that obvious with a lot 
of tweets carrying ‘Mixed Feelings’. The more decisive tweets conveyed 
either ‘Joy’ or ‘Anger’ emotions. This may be attributed to celebrating 
victory (‘Joy’) or complaining from destruction (‘Anger’). Finally, for 
partiality analysis, the amount of tweets classified as being 
‘Pro-Ukraine’ was slightly greater than Pro-Russia’ at the beginning of 
the war (specifically from Feb 2022 till April 2022) then slowly began 
to decrease until they nearly converged at the start of June 2022 with a
 shift happening in the empathy towards Russia in August 2022. Our 
Interpretation for that is with the initial Russian fierce and surprise 
attack at the beginning and the amount of refugees who escaped to 
neighboring countries, Ukraine gained much empathy. However, by April 
2022, Russian intensity has been decreased and with heavy sanctions the 
U.S. and West have applied on Russia, Russia has begun to gain such 
empathy with decrease on the Ukrainian side.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2023/12</td>
					</tr>
					<tr>
					<th>Language</th>
						<td>en</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>epjdatascience.springeropen.com</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://epjdatascience.springeropen.com/articles/10.1140/epjds/s13688-023-00415-4">https://epjdatascience.springeropen.com/articles/10.1140/epjds/s13688-023-00415-4</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>1/3/2025, 12:40:44 PM</td>
					</tr>
					<tr>
					<th>Rights</th>
						<td>2023 The Author(s)</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>Number: 1
Publisher: SpringerOpen</td>
					</tr>
					<tr>
					<th>Volume</th>
						<td>12</td>
					</tr>
					<tr>
					<th>Pages</th>
						<td>1-32</td>
					</tr>
					<tr>
					<th>Publication</th>
						<td>EPJ Data Science</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.1140/epjds/s13688-023-00415-4">10.1140/epjds/s13688-023-00415-4</a></td>
					</tr>
					<tr>
					<th>Issue</th>
						<td>1</td>
					</tr>
					<tr>
					<th>Journal Abbr</th>
						<td>EPJ Data Sci.</td>
					</tr>
					<tr>
					<th>ISSN</th>
						<td>2193-1127</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>1/3/2025, 12:40:44 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_GNVNPD5J">Full Text PDF					</li>
				</ul>
			</li>


			<li id="item_UVWUEY3K" class="item preprint">
			<h2>Artificial Artificial Artificial Intelligence: Crowd Workers Widely Use Large Language Models for Text Production Tasks</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Preprint</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Veniamin Veselovsky</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Manoel Horta Ribeiro</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Robert West</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Large language models (LLMs) are remarkable data annotators. 
They can be used to generate high-fidelity supervised training data, as 
well as survey and experimental data. With the widespread adoption of 
LLMs, human gold--standard annotations are key to understanding the 
capabilities of LLMs and the validity of their results. However, 
crowdsourcing, an important, inexpensive way to obtain human 
annotations, may itself be impacted by LLMs, as crowd workers have 
financial incentives to use LLMs to increase their productivity and 
income. To investigate this concern, we conducted a case study on the 
prevalence of LLM usage by crowd workers. We reran an abstract 
summarization task from the literature on Amazon Mechanical Turk and, 
through a combination of keystroke detection and synthetic text 
classification, estimate that 33-46% of crowd workers used LLMs when 
completing the task. Although generalization to other, less LLM-friendly
 tasks is unclear, our results call for platforms, researchers, and 
crowd workers to find new ways to ensure that human data remain human, 
perhaps using the methodology proposed here as a stepping stone. 
Code/data: https://github.com/epfl-dlab/GPTurk</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2023-06-13</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>Artificial Artificial Artificial Intelligence</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>arXiv.org</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://arxiv.org/abs/2306.07899">http://arxiv.org/abs/2306.07899</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>1/6/2025, 11:43:37 AM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>arXiv:2306.07899 [cs]</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.48550/arXiv.2306.07899">10.48550/arXiv.2306.07899</a></td>
					</tr>
					<tr>
					<th>Repository</th>
						<td>arXiv</td>
					</tr>
					<tr>
					<th>Archive ID</th>
						<td>arXiv:2306.07899</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>1/6/2025, 11:43:37 AM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Computer Science - Computation and Language</li>
					<li>Computer Science - Computers and Society</li>
				</ul>
				<h3 class="notes">Notes:</h3>
				<ul class="notes">
					<li id="item_NI76879N">
<p class="plaintext">Comment: 9 pages, 4 figures</p>
					</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_8SJMH64T">Preprint PDF					</li>
					<li id="item_V6EGETDD">Snapshot					</li>
				</ul>
			</li>


			<li id="item_34Q8M63Q" class="item preprint">
			<h2>Artificial Intelligence (AI) and the Future of (Quantitative) Social Science</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Preprint</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>David Bann</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Liam Wright</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>This briefing note explores the transformative potential of 
Artificial Intelligence (AI) for quantitative social science. It 
provides an overview of recent advancements, outlines possible future 
scenarios, and examines the opportunities and challenges that AI 
introduces to the field. Opportunities include substantial gains in 
productivity across the research process: notably in data analysis, 
visualisation, and dissemination. Researchers could thus spend more time
 focusing on research and less time on administrative or repetitive 
tasks. Other opportunities include using AI algorithms to analyse the 
increasing vastness of data obtained in surveys; using AI to improve 
prediction; and efficiency gains in survey or experimental 
design/deployment. Challenges include the potential for a vast increase 
in low-quality publications, especially if academic incentives remain 
misaligned toward quantity rather than quality of research outputs, as 
well as the broader existential risks AI may pose. We also include a 
list of resources and additional reading. This brief was written jointly
 by the authors; readers can compare with briefs entirely written by 
Generative AI (GenAI) systems, and listen to an AI generated podcast 
based on this brief here: https://osf.io/ew6z8/. As AI reshapes research
 and society, social scientists are needed to understand AI's 
implications and deploy AI to better achieve our goals.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2024-11-08</td>
					</tr>
					<tr>
					<th>Language</th>
						<td>en-us</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>OSF Preprints</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://osf.io/6dw5f">https://osf.io/6dw5f</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>1/6/2025, 11:22:22 AM</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.31235/osf.io/6dw5f">10.31235/osf.io/6dw5f</a></td>
					</tr>
					<tr>
					<th>Repository</th>
						<td>OSF</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>1/6/2025, 11:22:22 AM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Artificial Intelligence</li>
					<li>Automation</li>
					<li>GenAI</li>
					<li>Large Language Models</li>
					<li>Quantitative research</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_B2XYL4D7">NCRM_futures_AI_v0007.docx					</li>
				</ul>
			</li>


			<li id="item_GI5IDW3D" class="item journalArticle">
			<h2>Artificial intelligence ethics by design. Evaluating public 
perception on the importance of ethical design principles of artificial 
intelligence</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Kimon Kieslich</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Birte Keller</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Christopher Starke</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Despite the immense societal importance of ethically designing
 artificial intelligence, little research on the public perceptions of 
ethical artificial intelligence principles exists. This becomes even 
more striking when considering that ethical artificial intelligence 
development has the aim to be human-centric and of benefit for the whole
 society. In this study, we investigate how ethical principles 
(explainability, fairness, security, accountability, accuracy, privacy, 
and machine autonomy) are weighted in comparison to each other. This is 
especially important, since simultaneously considering ethical 
principles is not only costly, but sometimes even impossible, as 
developers must make specific trade-off decisions. In this paper, we 
give first answers on the relative importance of ethical principles 
given a specific use case—the use of artificial intelligence in tax 
fraud detection. The results of a large conjoint survey (n=1099) suggest
 that, by and large, German respondents evaluate the ethical principles 
as equally important. However, subsequent cluster analysis shows that 
different preference models for ethically designed systems exist among 
the German population. These clusters substantially differ not only in 
the preferred ethical principles but also in the importance levels of 
the principles themselves. We further describe how these groups are 
constituted in terms of sociodemographics as well as opinions on 
artificial intelligence. Societal implications, as well as design 
challenges, are discussed.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2022-01-01</td>
					</tr>
					<tr>
					<th>Language</th>
						<td>en</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>SAGE Journals</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://doi.org/10.1177/20539517221092956">https://doi.org/10.1177/20539517221092956</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>12/23/2024, 5:59:30 PM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>Publisher: SAGE Publications Ltd</td>
					</tr>
					<tr>
					<th>Volume</th>
						<td>9</td>
					</tr>
					<tr>
					<th>Pages</th>
						<td>20539517221092956</td>
					</tr>
					<tr>
					<th>Publication</th>
						<td>Big Data &amp; Society</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.1177/20539517221092956">10.1177/20539517221092956</a></td>
					</tr>
					<tr>
					<th>Issue</th>
						<td>1</td>
					</tr>
					<tr>
					<th>ISSN</th>
						<td>2053-9517</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>12/23/2024, 5:59:30 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_I9PSX5WL">SAGE PDF Full Text					</li>
				</ul>
			</li>


			<li id="item_7PAVLL3X" class="item journalArticle">
			<h2>Artificial intelligence, human intelligence and hybrid intelligence based on mutual augmentation</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Mohammad Hossein Jarrahi</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Christoph Lutz</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Gemma Newlands</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>There is little consensus on what artificial intelligence (AI)
 systems may or may not embrace. Although this may point to multiplicity
 of interpretations and backgrounds, a lack of conceptual clarity could 
thwart the development of common ground around the concept among 
researchers, practitioners and users of AI and pave the way for 
misinterpretation and abuse of the concept. This article argues that one
 of the effective ways to delineate the concept of AI is to compare and 
contrast it with human intelligence. In doing so, the article broaches 
the unique capabilities of humans and AI in relation to one another 
(human and machine tacit knowledge), as well as two types of AI systems:
 one that goes beyond human intelligence and one that is necessarily and
 inherently tied to it. It finally highlights how humans and AI can 
augment their capabilities and intelligence through synergistic human–AI
 interactions (i.e., human-augmented AI and augmented human 
intelligence), resulting in hybrid intelligence, and concludes with a 
future-looking research agenda.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2022-07-01</td>
					</tr>
					<tr>
					<th>Language</th>
						<td>en</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>SAGE Journals</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://doi.org/10.1177/20539517221142824">https://doi.org/10.1177/20539517221142824</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>12/23/2024, 5:47:48 PM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>Publisher: SAGE Publications Ltd</td>
					</tr>
					<tr>
					<th>Volume</th>
						<td>9</td>
					</tr>
					<tr>
					<th>Pages</th>
						<td>20539517221142824</td>
					</tr>
					<tr>
					<th>Publication</th>
						<td>Big Data &amp; Society</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.1177/20539517221142824">10.1177/20539517221142824</a></td>
					</tr>
					<tr>
					<th>Issue</th>
						<td>2</td>
					</tr>
					<tr>
					<th>ISSN</th>
						<td>2053-9517</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>12/23/2024, 5:47:48 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_HNDAFYID">SAGE PDF Full Text					</li>
				</ul>
			</li>


			<li id="item_94QFK329" class="item journalArticle">
			<h2>Artificial Intelligence, Rationalization, and the Limits of Control in the Public Sector: The Case of Tax Policy Optimization</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Jakob Mökander</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Ralph Schroeder</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>In this paper, we first frame the use of artificial 
intelligence (AI) systems in the public sector as a continuation and 
intensification of long-standing rationalization and bureaucratization 
processes. Drawing on Weber, we understand the core of these processes 
to be the replacement of traditions with instrumental rationality, that 
is, the most calculable and efficient way of achieving any given policy 
objective. Second, we demonstrate how much of the criticisms, both among
 the public and in scholarship, directed towards AI systems spring from 
well-known tensions at the heart of Weberian rationalization. To 
illustrate this point, we introduce a thought experiment whereby AI 
systems are used to optimize tax policy to advance a specific normative 
end: reducing economic inequality. Our analysis shows that building a 
machine-like tax system that promotes social and economic equality is 
possible. However, our analysis also highlights that AI-driven policy 
optimization (i) comes at the exclusion of other competing political 
values, (ii) overrides citizens’ sense of their (non-instrumental) 
obligations to each other, and (iii) undermines the notion of humans as 
self-determining beings. Third, we observe that contemporary scholarship
 and advocacy directed towards ensuring that AI systems are legal, 
ethical, and safe build on and reinforce central assumptions that 
underpin the process of rationalization, including the modern idea that 
science can sweep away oppressive systems and replace them with a rule 
of reason that would rescue humans from moral injustices. That is overly
 optimistic: science can only provide the means – it cannot dictate the 
ends. Nonetheless, the use of AI in the public sector can also benefit 
the institutions and processes of liberal democracies. Most importantly,
 AI-driven policy optimization demands that normative ends are made 
explicit and formalized, thereby subjecting them to public scrutiny, 
deliberation, and debate.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2024-12-01</td>
					</tr>
					<tr>
					<th>Language</th>
						<td>en</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>Artificial Intelligence, Rationalization, and the Limits of Control in the Public Sector</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>SAGE Journals</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://doi.org/10.1177/08944393241235175">https://doi.org/10.1177/08944393241235175</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>12/24/2024, 5:45:59 PM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>Publisher: SAGE Publications Inc</td>
					</tr>
					<tr>
					<th>Volume</th>
						<td>42</td>
					</tr>
					<tr>
					<th>Pages</th>
						<td>1359-1378</td>
					</tr>
					<tr>
					<th>Publication</th>
						<td>Social Science Computer Review</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.1177/08944393241235175">10.1177/08944393241235175</a></td>
					</tr>
					<tr>
					<th>Issue</th>
						<td>6</td>
					</tr>
					<tr>
					<th>ISSN</th>
						<td>0894-4393</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>12/24/2024, 5:45:59 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_SBVHPS5Q">SAGE PDF Full Text					</li>
				</ul>
			</li>


			<li id="item_N4RKAHKS" class="item preprint">
			<h2>Artificial Leviathan: Exploring Social Evolution of LLM Agents Through the Lens of Hobbesian Social Contract Theory</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Preprint</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Gordon Dai</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Weijia Zhang</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Jinhan Li</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Siqi Yang</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Chidera Onochie lbe</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Srihas Rao</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Arthur Caetano</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Misha Sra</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>The emergence of Large Language Models (LLMs) and advancements
 in Artificial Intelligence (AI) offer an opportunity for computational 
social science research at scale. Building upon prior explorations of 
LLM agent design, our work introduces a simulated agent society where 
complex social relationships dynamically form and evolve over time. 
Agents are imbued with psychological drives and placed in a sandbox 
survival environment. We conduct an evaluation of the agent society 
through the lens of Thomas Hobbes's seminal Social Contract Theory 
(SCT). We analyze whether, as the theory postulates, agents seek to 
escape a brutish "state of nature" by surrendering rights to an absolute
 sovereign in exchange for order and security. Our experiments unveil an
 alignment: Initially, agents engage in unrestrained conflict, mirroring
 Hobbes's depiction of the state of nature. However, as the simulation 
progresses, social contracts emerge, leading to the authorization of an 
absolute sovereign and the establishment of a peaceful commonwealth 
founded on mutual cooperation. This congruence between our LLM agent 
society's evolutionary trajectory and Hobbes's theoretical account 
indicates LLMs' capability to model intricate social dynamics and 
potentially replicate forces that shape human societies. By enabling 
such insights into group behavior and emergent societal phenomena, 
LLM-driven multi-agent simulations, while unable to simulate all the 
nuances of human behavior, may hold potential for advancing our 
understanding of social structures, group dynamics, and complex human 
systems.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2024-07-01</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>Artificial Leviathan</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>arXiv.org</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://arxiv.org/abs/2406.14373">http://arxiv.org/abs/2406.14373</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>1/7/2025, 5:27:22 PM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>arXiv:2406.14373 [cs]</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.48550/arXiv.2406.14373">10.48550/arXiv.2406.14373</a></td>
					</tr>
					<tr>
					<th>Repository</th>
						<td>arXiv</td>
					</tr>
					<tr>
					<th>Archive ID</th>
						<td>arXiv:2406.14373</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>1/7/2025, 5:27:22 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Computer Science - Artificial Intelligence</li>
					<li>Computer Science - Computation and Language</li>
					<li>Computer Science - Computers and Society</li>
					<li>Computer Science - Human-Computer Interaction</li>
					<li>Computer Science - Multiagent Systems</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_BTM8LJ89">Preprint PDF					</li>
					<li id="item_YQLZA2K6">Snapshot					</li>
				</ul>
			</li>


			<li id="item_FH8MBP3R" class="item preprint">
			<h2>Assessing Bias in LLM-Generated Synthetic Datasets: The Case of German Voter Behavior</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Preprint</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Leah von der Heyde</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Anna-Carolina Haensch</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Alexander Wenz</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>[For the more comprehensive version of this work, please see 
https://arxiv.org/abs/2407.08563] The rise of large language models 
(LLMs) like GPT-3 has sparked interest in their potential for creating 
synthetic datasets, particularly in the realm of privacy research. This 
study critically evaluates the use of LLMs in generating synthetic 
public opinion data, pointing out the biases inherent in the data 
generation process. While LLMs, trained on vast internet datasets, can 
mimic societal attitudes and behaviors, their application in 
synthesizing data poses significant privacy and accuracy challenges. We 
investigate these issues using the case of vote choice prediction in the
 2017 German federal elections.
Employing GPT-3, we construct synthetic personas based on the German 
Longitudinal Election Study, prompting the LLM to predict voting 
behavior. Our analysis compares these LLM-generated predictions with 
actual survey data, focusing on the implications of using such synthetic
 data and the biases it may contain. The results demonstrate GPT-3’s 
propensity to inaccurately predict voter choices, with biases favoring 
certain political groups and more predictable voter profiles. This 
outcome raises critical questions about the reliability and ethical use 
of LLMs in generating synthetic data.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2023-12-01</td>
					</tr>
					<tr>
					<th>Language</th>
						<td>en-us</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>Assessing Bias in LLM-Generated Synthetic Datasets</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>OSF Preprints</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://osf.io/97r8s">https://osf.io/97r8s</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>1/6/2025, 11:45:34 AM</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.31235/osf.io/97r8s">10.31235/osf.io/97r8s</a></td>
					</tr>
					<tr>
					<th>Repository</th>
						<td>OSF</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>1/6/2025, 11:45:34 AM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>AI</li>
					<li>artificial intelligence</li>
					<li>bias</li>
					<li>LLM</li>
					<li>public opinion</li>
					<li>survey data</li>
					<li>synthethic data</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_HCK6QMZA">OSF Preprint					</li>
				</ul>
			</li>


			<li id="item_5REGHMQR" class="item preprint">
			<h2>Automated Annotation with Generative AI Requires Validation</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Preprint</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Nicholas Pangakis</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Samuel Wolken</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Neil Fasching</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Generative large language models (LLMs) can be a powerful tool
 for augmenting text annotation procedures, but their performance varies
 across annotation tasks due to prompt quality, text data 
idiosyncrasies, and conceptual difficulty. Because these challenges will
 persist even as LLM technology improves, we argue that any automated 
annotation process using an LLM must validate the LLM's performance 
against labels generated by humans. To this end, we outline a workflow 
to harness the annotation potential of LLMs in a principled, efficient 
way. Using GPT-4, we validate this approach by replicating 27 annotation
 tasks across 11 datasets from recent social science articles in 
high-impact journals. We find that LLM performance for text annotation 
is promising but highly contingent on both the dataset and the type of 
annotation task, which reinforces the necessity to validate on a 
task-by-task basis. We make available easy-to-use software designed to 
implement our workflow and streamline the deployment of LLMs for 
automated annotation.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2023-05-31</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>arXiv.org</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://arxiv.org/abs/2306.00176">http://arxiv.org/abs/2306.00176</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>1/7/2025, 5:38:47 PM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>arXiv:2306.00176 [cs]</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.48550/arXiv.2306.00176">10.48550/arXiv.2306.00176</a></td>
					</tr>
					<tr>
					<th>Repository</th>
						<td>arXiv</td>
					</tr>
					<tr>
					<th>Archive ID</th>
						<td>arXiv:2306.00176</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>1/7/2025, 5:38:47 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Computer Science - Artificial Intelligence</li>
					<li>Computer Science - Computation and Language</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_NUXNH2YC">Preprint PDF					</li>
					<li id="item_EWTQXDVA">Snapshot					</li>
				</ul>
			</li>


			<li id="item_6B2TIMLW" class="item journalArticle">
			<h2>Automated Coding Using Machine Learning and Remapping the U.S. Nonprofit Sector: A Guide and Benchmark</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Ji Ma</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>This research developed a machine-learning classiﬁer that 
reliably automates the coding process using the National Taxonomy of 
Exempt Entities as a schema and remapped the U.S. nonproﬁt sector. I 
achieved 90% overall accuracy for classifying the nonproﬁts into nine 
broad categories and 88% for classifying them into 25 major groups. The 
intercoder reliabilities between algorithms and human coders measured by
 kappa statistics are in the “almost perfect” range of 0.80–1.00. The 
results suggest that a state-of-theart machine-learning algorithm can 
approximate human coders and substantially improve researchers’ 
productivity. I also reassigned multiple category codes to over 439 
thousand nonproﬁts and discovered a considerable amount of 
organizational activities that were previously ignored. The classiﬁer is
 an essential methodological prerequisite for large-N and Big Data 
analyses, and the remapped U.S. nonproﬁt sector can serve as an 
important instrument for asking or reexamining fundamental questions of 
nonproﬁt studies. The working directory with all data sets, source 
codes, and historical versions are available on GitHub 
(https://github.com/ma-ji/npo classiﬁer).</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>06/2021</td>
					</tr>
					<tr>
					<th>Language</th>
						<td>en</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>Automated Coding Using Machine Learning and Remapping the U.S. Nonprofit Sector</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>DOI.org (Crossref)</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://journals.sagepub.com/doi/10.1177/0899764020968153">http://journals.sagepub.com/doi/10.1177/0899764020968153</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>5/5/2024, 1:59:31 PM</td>
					</tr>
					<tr>
					<th>Volume</th>
						<td>50</td>
					</tr>
					<tr>
					<th>Pages</th>
						<td>662-687</td>
					</tr>
					<tr>
					<th>Publication</th>
						<td>Nonprofit and Voluntary Sector Quarterly</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.1177/0899764020968153">10.1177/0899764020968153</a></td>
					</tr>
					<tr>
					<th>Issue</th>
						<td>3</td>
					</tr>
					<tr>
					<th>Journal Abbr</th>
						<td>Nonprofit and Voluntary Sector Quarterly</td>
					</tr>
					<tr>
					<th>ISSN</th>
						<td>0899-7640, 1552-7395</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>5/5/2024, 1:59:31 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>5/5/2024, 1:59:32 PM</td>
					</tr>
				</tbody></table>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_4NRE6JCZ">Ma - 2021 - Automated Coding Using Machine Learning and Remapp.pdf					</li>
				</ul>
			</li>


			<li id="item_MMYXFWY2" class="item journalArticle">
			<h2>Automated measures of sentiment via transformer- and lexicon-based sentiment analysis (TLSA)</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Xinyan Zhao</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Chau-Wai Wong</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>The last decade witnessed the proliferation of automated 
content analysis in communication research. However, existing 
computational tools have been taken up unevenly, with powerful deep 
learning algorithms such as transformers rarely applied as compared to 
lexicon-based dictionaries. To enable social scientists to adopt modern 
computational methods for valid and reliable sentiment analysis of 
English text, we propose an open and free web service named transformer-
 and lexicon-based sentiment analysis (TLSA). TLSA integrates diverse 
tools and offers validation metrics, empowering users with limited 
computational knowledge and resources to reap the benefit of 
state-of-the-art computational methods. Two cases demonstrate the 
functionality and usability of TLSA. The performance of different tools 
varied to a large extent based on the dataset, supporting the importance
 of validating various sentiment tools in a specific context.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2024-04-01</td>
					</tr>
					<tr>
					<th>Language</th>
						<td>en</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>Springer Link</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://doi.org/10.1007/s42001-023-00233-8">https://doi.org/10.1007/s42001-023-00233-8</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>1/3/2025, 2:59:19 PM</td>
					</tr>
					<tr>
					<th>Volume</th>
						<td>7</td>
					</tr>
					<tr>
					<th>Pages</th>
						<td>145-170</td>
					</tr>
					<tr>
					<th>Publication</th>
						<td>Journal of Computational Social Science</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.1007/s42001-023-00233-8">10.1007/s42001-023-00233-8</a></td>
					</tr>
					<tr>
					<th>Issue</th>
						<td>1</td>
					</tr>
					<tr>
					<th>Journal Abbr</th>
						<td>J Comput Soc Sc</td>
					</tr>
					<tr>
					<th>ISSN</th>
						<td>2432-2725</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>1/3/2025, 2:59:19 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Automated content analysis</li>
					<li>Deep learning</li>
					<li>Dictionaries</li>
					<li>Emotions</li>
					<li>Sentiment analysis</li>
					<li>Software engineering</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_RVLNC55P">Submitted Version					</li>
				</ul>
			</li>


			<li id="item_TU93Y6ZW" class="item journalArticle">
			<h2>Automatically Finding Actors in Texts: A Performance Review of Multilingual Named Entity Recognition Tools</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Paul Balluff</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Hajo G. Boomgaarden</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Annie Waldherr</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Named Entity Recognition (NER) is a crucial task in natural 
language processing and has a wide range of applications in 
communication science. However, there is a lack of systematic 
evaluations of available NER tools in the field. In this study, we 
evaluate the performance of various multilingual NER tools, including 
rule-based and transformer-based models. We conducted experiments on 
corpora containing texts in multiple languages and evaluated the 
F1-score, speed, and features of each tool. Our results show that 
transformer-based language models outperform rule-based models and other
 NER tools in most languages. However, we found that the performance of 
the transformer-based models varies depending on the language and the 
corpus. Our study provides insights into the strengths and weaknesses of
 NER tools and their suitability for specific languages, which can 
inform the selection of appropriate tools for future studies and 
applications in communication science.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2024-10-01</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>Automatically Finding Actors in Texts</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>Taylor and Francis+NEJM</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://doi.org/10.1080/19312458.2024.2324789">https://doi.org/10.1080/19312458.2024.2324789</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>1/6/2025, 1:58:14 PM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>Publisher: Routledge
_eprint: https://doi.org/10.1080/19312458.2024.2324789</td>
					</tr>
					<tr>
					<th>Volume</th>
						<td>18</td>
					</tr>
					<tr>
					<th>Pages</th>
						<td>371-389</td>
					</tr>
					<tr>
					<th>Publication</th>
						<td>Communication Methods and Measures</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.1080/19312458.2024.2324789">10.1080/19312458.2024.2324789</a></td>
					</tr>
					<tr>
					<th>Issue</th>
						<td>4</td>
					</tr>
					<tr>
					<th>ISSN</th>
						<td>1931-2458</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>1/6/2025, 1:58:14 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_IDY33RD9">Full Text PDF					</li>
				</ul>
			</li>


			<li id="item_79FSPL2R" class="item journalArticle">
			<h2>Best Practices for Text Annotation with Large Language Models</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Petter Törnberg</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Large Language Models (LLMs) have ushered in a new era of text
 annotation, as their ease-ofuse, high accuracy, and relatively low 
costs have meant that their use has exploded in recent months. However, 
the rapid growth of the field has meant that LLM-based annotation has 
become something of an academic Wild West: the lack of established 
practices and standards has led to concerns about the quality and 
validity of research. Researchers have warned that the ostensible 
simplicity of LLMs can be misleading, as they are prone to bias, 
misunderstandings, and unreliable results. Recognizing the 
transformative potential of LLMs, this paper proposes a comprehensive 
set of standards and best practices for their reliable, reproducible, 
and ethical use. These guidelines span critical areas such as model 
selection, prompt engineering, structured prompting, prompt stability 
analysis, rigorous model validation, and the consideration of ethical 
and legal implications. The paper emphasizes the need for a structured, 
directed, and formalized approach to using LLMs, aiming to ensure the 
integrity and robustness of text annotation practices, and advocates for
 a nuanced and critical engagement with LLMs in social scientific 
research.</td>
					</tr>
					<tr>
					<th>Language</th>
						<td>en</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>Zotero</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>9/20/2024, 5:08:43 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>9/20/2024, 5:08:43 PM</td>
					</tr>
				</tbody></table>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_KAC35CTM">Törnberg - Best Practices for Text Annotation with Large Lang.pdf					</li>
				</ul>
			</li>


			<li id="item_3HEAXV9H" class="item journalArticle">
			<h2>Beyond artificial intelligence controversies: What are algorithms doing in the scientific literature?</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Anders Kristian Munk</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Mathieu Jacomy</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Matilde Ficozzi</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Torben Elgaard Jensen</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Mounting critique of the way AI is framed in mainstream media 
calls for less sensationalist coverage, be it jubilant or apocalyptic, 
and more attention to the concrete situations in which AI becomes 
controversial in different ways. This is supposedly achieved by making 
coverage more expert-informed. We therefore explore how experts 
contribute to the issuefication of AI through the scientific literature.
 We provide a semantic, visual network analysis of a corpus of 1M 
scientific abstracts about machine learning algorithms and artificial 
intelligence. Through a systematic quali-quantitative exploration of 235
 co-word clusters and a subsequent structured search for 18 
issue-specific queries, for which we devise a novel method with a 
custom-built datascape, we explore how algorithms have agency. We find 
that scientific discourse is highly situated and rarely about AI in 
general. It overwhelmingly charges algorithms with the capacity to solve
 problems and these problems are rarely about algorithms in their 
origin. Conversely, it rarely charges algorithms with the capacity to 
cause problems and when it does, other algorithms are typically charged 
with the capacity to solve them. Based on these findings, we argue that 
while a more expert-informed coverage of AI is likely to be less 
sensationalist and show greater attention to the specific situations 
where algorithms make a difference, it is unlikely to stage AI as 
particularly controversial. Consequently, we suggest conceptualising AI 
as a political situation rather than something inherently controversial.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2024-09-01</td>
					</tr>
					<tr>
					<th>Language</th>
						<td>en</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>Beyond artificial intelligence controversies</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>SAGE Journals</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://doi.org/10.1177/20539517241255107">https://doi.org/10.1177/20539517241255107</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>12/23/2024, 4:37:47 PM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>Publisher: SAGE Publications Ltd</td>
					</tr>
					<tr>
					<th>Volume</th>
						<td>11</td>
					</tr>
					<tr>
					<th>Pages</th>
						<td>20539517241255107</td>
					</tr>
					<tr>
					<th>Publication</th>
						<td>Big Data &amp; Society</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.1177/20539517241255107">10.1177/20539517241255107</a></td>
					</tr>
					<tr>
					<th>Issue</th>
						<td>3</td>
					</tr>
					<tr>
					<th>ISSN</th>
						<td>2053-9517</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>12/23/2024, 4:37:47 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_DYK4YWQZ">SAGE PDF Full Text					</li>
				</ul>
			</li>


			<li id="item_25ELVE8U" class="item preprint">
			<h2>Beyond Bias: Studying ‘culture’ in LLMs and AI chatbots</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Preprint</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Mark Friis Hau</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Christian Hendriksen</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>This paper argues for a conceptual shift in the understanding of Large Language Models (LLMs) within
ethnographic and organizational studies, proposing a framework that interprets biases in LLMs not as
extrinsic flaws, but as intrinsic culture. Drawing from Bruno Latour's Actor-Network Theory (ANT), this
study conceptualizes LLMs as actants that participate dynamically within networks of human and nonhuman
entities. By recognizing biases as reflective of the training data's cultural imprints, this
framework positions LLMs as embedded within the cultural, social, and ideological currents that shape
and are shaped by these technologies. The paper argues for a reconceptualization of AI's role in research
and practice, urging a methodological and ethical engagement that embraces the constitutive nature of
‘AI culture’ in shaping organizational and societal dynamics.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2024-07-02</td>
					</tr>
					<tr>
					<th>Language</th>
						<td>en-us</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>Beyond Bias</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>OSF Preprints</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://osf.io/kz2xc">https://osf.io/kz2xc</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>1/6/2025, 11:47:31 AM</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.31235/osf.io/kz2xc">10.31235/osf.io/kz2xc</a></td>
					</tr>
					<tr>
					<th>Repository</th>
						<td>OSF</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>1/6/2025, 11:47:31 AM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Bias and culture</li>
					<li>GenAI</li>
					<li>Human-Technology Interaction</li>
					<li>Organizational ethnography</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_LFHBQXGI">OSF Preprint					</li>
				</ul>
			</li>


			<li id="item_UBQTT7A8" class="item journalArticle">
			<h2>Beyond sentiment: an algorithmic strategy for identifying evaluations within large text corpora</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Maximilian Overbeck</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Christian Baden</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Tali Aharoni</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Eedan Amit-Danhi</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Keren Tenenboim-Weinblatt</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>In this paper, we propose a new strategy for classifying 
evaluations in large text corpora, using supervised machine learning 
(SML). Departing from a conceptual and methodological critique of the 
use of sentiment measures to recognize object-specific evaluations, we 
argue that a key challenge consists in determining whether a semantic 
relationship exists between evaluative expressions and evaluated 
objects. Regarding sentiment terms as merely potentially evaluative 
expressions, we thus use a SML classifier to decide whether recognized 
terms have an evaluative function in relation to the evaluated object. 
We train and test our classifier on a corpus of 10,004 segments of 
election coverage from 16 major U.S. news outlets and Tweets by 10 
prominent U.S. politicians and journalists. Specifically, we focus on 
evaluations of political predictions about the outcomes and implications
 of the 2016 and 2020 U.S. presidential elections. We show that our 
classifier consistently outperforms both off-the-shelf sentiment tools 
and a pre-trained transformer-based sentiment classifier. Critically, 
our classifier correctly discards numerous non-evaluative uses of common
 sentiment terms, whose inclusion in conventional analyses generates 
large amounts of false positives. We discuss contributions of our 
approach to the measurement of object-specific evaluations and highlight
 challenges for future research.</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>Beyond sentiment</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>Taylor and Francis+NEJM</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://doi.org/10.1080/19312458.2023.2285783">https://doi.org/10.1080/19312458.2023.2285783</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>1/6/2025, 2:28:40 PM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>Publisher: Routledge
_eprint: https://doi.org/10.1080/19312458.2023.2285783</td>
					</tr>
					<tr>
					<th>Volume</th>
						<td>0</td>
					</tr>
					<tr>
					<th>Pages</th>
						<td>1-22</td>
					</tr>
					<tr>
					<th>Publication</th>
						<td>Communication Methods and Measures</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.1080/19312458.2023.2285783">10.1080/19312458.2023.2285783</a></td>
					</tr>
					<tr>
					<th>Issue</th>
						<td>0</td>
					</tr>
					<tr>
					<th>ISSN</th>
						<td>1931-2458</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>1/6/2025, 2:28:40 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_AVBDTPL9">Full Text PDF					</li>
				</ul>
			</li>


			<li id="item_EHWQR5VA" class="item preprint">
			<h2>Bias and Error Mitigation in Software-Generated Data: An Advanced
 Search and Optimization Framework Leveraging Generative Code Models</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Preprint</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Ernesto Giralt Hernández</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Data generation and analysis is a fundamental aspect of many 
industries and disciplines, from strategic decision making in business 
to research in the physical and social sciences. However, data generated
 using software and algorithms can be subject to biases and errors. 
These can be due to problems with the original software, default 
settings that do not align with the specific needs of the situation, or 
even deeper problems with the underlying theories and models. This paper
 proposes an advanced search and optimization framework aimed at 
generating and choosing optimal source code capable of correcting errors
 and biases from previous versions to address typical problems in 
software systems specializing in data analysis and generation, 
especially those in the corporate and data science world. Applying this 
framework multiple times on the same software system would incrementally
 improve the quality of the output results. It uses Solomonoff Induction
 as a sound theoretical basis, extending it with Kolmogorov Conditional 
Complexity, a novel adaptation, to evaluate a set of candidate programs.
 We propose the use of generative models for the creation of this set of
 programs, with special emphasis on the capabilities of Large Language 
Models (LLMs) to generate high quality code.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2023-10-17</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>Bias and Error Mitigation in Software-Generated Data</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>arXiv.org</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://arxiv.org/abs/2310.11546">http://arxiv.org/abs/2310.11546</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>1/7/2025, 5:42:22 PM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>arXiv:2310.11546 [cs]</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.48550/arXiv.2310.11546">10.48550/arXiv.2310.11546</a></td>
					</tr>
					<tr>
					<th>Repository</th>
						<td>arXiv</td>
					</tr>
					<tr>
					<th>Archive ID</th>
						<td>arXiv:2310.11546</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>1/7/2025, 5:42:22 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Computer Science - Information Theory</li>
					<li>Computer Science - Machine Learning</li>
					<li>Computer Science - Software Engineering</li>
					<li>Mathematics - Information Theory</li>
					<li>Mathematics - Optimization and Control</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_8DDD9WNU">Preprint PDF					</li>
					<li id="item_AISUNFDZ">Snapshot					</li>
				</ul>
			</li>


			<li id="item_BVZNL2Z7" class="item journalArticle">
			<h2>Bridging Qualitative Data Silos: The Potential of Reusing Codings Through Machine Learning Based Cross-Study Code Linking</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Sergej Wildemann</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Claudia Niederée</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Erick Elejalde</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>For qualitative data analysis (QDA), researchers assign codes 
to text segments to arrange the information into topics or concepts. 
These annotations facilitate information retrieval and the 
identification of emerging patterns in unstructured data. However, this 
metadata is typically not published or reused after the research. 
Subsequent studies with similar research questions require a new 
definition of codes and do not benefit from other analysts’ experience. 
Machine learning (ML) based classification seeded with such data remains
 a challenging task due to the ambiguity of code definitions and the 
inherent subjectivity of the exercise. Previous attempts to support QDA 
using ML rely on linear models and only examined individual datasets 
that were either smaller or coded specifically for this purpose. 
However, we show that modern approaches effectively capture at least 
part of the codes’ semantics and may generalize to multiple studies. We 
analyze the performance of multiple classifiers across three large 
real-world datasets. Furthermore, we propose an ML-based approach to 
identify semantic relations of codes in different studies to show 
thematic faceting, enhance retrieval of related content, or bootstrap 
the coding process. These are encouraging results that suggest how 
analysts might benefit from prior interpretation efforts, potentially 
yielding new insights into qualitative data.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2024-06-01</td>
					</tr>
					<tr>
					<th>Language</th>
						<td>en</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>Bridging Qualitative Data Silos</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>SAGE Journals</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://doi.org/10.1177/08944393231215459">https://doi.org/10.1177/08944393231215459</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>12/24/2024, 6:53:15 PM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>Publisher: SAGE Publications Inc</td>
					</tr>
					<tr>
					<th>Volume</th>
						<td>42</td>
					</tr>
					<tr>
					<th>Pages</th>
						<td>760-776</td>
					</tr>
					<tr>
					<th>Publication</th>
						<td>Social Science Computer Review</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.1177/08944393231215459">10.1177/08944393231215459</a></td>
					</tr>
					<tr>
					<th>Issue</th>
						<td>3</td>
					</tr>
					<tr>
					<th>ISSN</th>
						<td>0894-4393</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>12/24/2024, 6:53:15 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_7W63T9N7">SAGE PDF Full Text					</li>
				</ul>
			</li>


			<li id="item_RI4XUUCC" class="item preprint">
			<h2>Can AI Replace Human Subjects? A Large-Scale Replication of Psychological Experiments with LLMs</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Preprint</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Ziyan Cui</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Ning Li</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Huaikang Zhou</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Artificial Intelligence (AI) is increasingly being integrated 
into scientific research, particularly in the social sciences, where 
understanding human behavior is critical. Large Language Models (LLMs) 
like GPT-4 have shown promise in replicating human-like responses in 
various psychological experiments. However, the extent to which LLMs can
 effectively replace human subjects across diverse experimental contexts
 remains unclear. Here, we conduct a large-scale study replicating 154 
psychological experiments from top social science journals with 618 main
 effects and 138 interaction effects using GPT-4 as a simulated 
participant. We find that GPT-4 successfully replicates 76.0 percent of 
main effects and 47.0 percent of interaction effects observed in the 
original studies, closely mirroring human responses in both direction 
and significance. However, only 19.44 percent of GPT-4's replicated 
confidence intervals contain the original effect sizes, with the 
majority of replicated effect sizes exceeding the 95 percent confidence 
interval of the original studies. Additionally, there is a 71.6 percent 
rate of unexpected significant results where the original studies 
reported null findings, suggesting potential overestimation or false 
positives. Our results demonstrate the potential of LLMs as powerful 
tools in psychological research but also emphasize the need for caution 
in interpreting AI-driven findings. While LLMs can complement human 
studies, they cannot yet fully replace the nuanced insights provided by 
human subjects.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2024-09-04</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>Can AI Replace Human Subjects?</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>arXiv.org</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://arxiv.org/abs/2409.00128">http://arxiv.org/abs/2409.00128</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>1/7/2025, 5:09:17 PM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>arXiv:2409.00128 [cs]</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.48550/arXiv.2409.00128">10.48550/arXiv.2409.00128</a></td>
					</tr>
					<tr>
					<th>Repository</th>
						<td>arXiv</td>
					</tr>
					<tr>
					<th>Archive ID</th>
						<td>arXiv:2409.00128</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>1/7/2025, 5:09:17 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Computer Science - Artificial Intelligence</li>
					<li>Computer Science - Computation and Language</li>
					<li>Economics - General Economics</li>
					<li>Quantitative Finance - Economics</li>
				</ul>
				<h3 class="notes">Notes:</h3>
				<ul class="notes">
					<li id="item_M5EDXVKB">
<p class="plaintext">Comment: 5 figures, 2 tables</p>
					</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_2QYWEZNJ">Preprint PDF					</li>
					<li id="item_RP5BTR9S">Snapshot					</li>
				</ul>
			</li>


			<li id="item_4CY4HATH" class="item preprint">
			<h2>Can Generative AI Improve Social Science?</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Preprint</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Christopher A. Bail</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Artificial intelligence that can produce realistic text,
images, and other human-like outputs is currently transforming many 
different industries. Yet it is not yet known how such tools might 
transform social science research. In the first section of this article,
 I assess the potential of Generative AI to improve online experiments, 
agent-based models, and automated content analyses.
I also discuss whether these tools may help social scientists perform 
literature reviews, identify novel research questions, and develop 
hypotheses to explain them. Next, I evaluate whether Generative AI can 
help social scientists with more mundane tasks such as acquiring 
advanced
programming skills or writing more effective prose. In the second 
section of this article I discuss the limitations of Generative AI as 
well as how these tools might be employed by researchers in an ethical 
manner. I discuss how bias in the processes and data used to train these
 tools can negatively impact social science research as well as a range
of other challenges related to accuracy, reproducibility,
interpretability, and efficiency. I conclude by highlighting the need 
for increased collaboration between social scientists and artificial 
intelligence researchers--- not only to ensure that such tools are used 
in a safe and ethical manner, but also because the progress of 
artificial intelligence may require deeper understanding of theories of 
human behavior</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2023-05-12</td>
					</tr>
					<tr>
					<th>Language</th>
						<td>en-us</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>OSF Preprints</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://osf.io/rwtzs">https://osf.io/rwtzs</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>1/6/2025, 11:18:44 AM</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.31235/osf.io/rwtzs">10.31235/osf.io/rwtzs</a></td>
					</tr>
					<tr>
					<th>Repository</th>
						<td>OSF</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>1/6/2025, 11:18:44 AM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Agent Based Models</li>
					<li>Computational Social Science</li>
					<li>Content Analysis</li>
					<li>Ethics</li>
					<li>Generative Artificial Intelligence</li>
					<li>Online Experiments</li>
					<li>Political Science</li>
					<li>Social Psychology</li>
					<li>Sociology</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_X8SI2G5X">OSF Preprint					</li>
				</ul>
			</li>


			<li id="item_EMGBJ4CJ" class="item preprint">
			<h2>Can Large Language Models (LLM) label topics from a topic model?</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Preprint</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Dai Li</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Bolun Zhang</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Yimang Zhou</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>ChatGPT is a general application based on a Large Language 
Model (LLM) developed by OpenAI. It is useful in many tasks in Natural 
Language Processing (NLP). As NLP tasks are prevalent in the practice of
 social science, ChatGPT is potentially a significant tool for social 
science. This study puts its focus on Topic Modeling, especially whether
 ChatGPT can generate convincing labels for topics. We sampled articles 
published in English sociological journals that use Topic Modeling, 
extracted the terms of topics and their labels, and asked Amazon 
Mechanic Turk users to choose between the original labels and ChatGPT 
labels via an online survey platform. The results show that general 
users do not significantly choose original labels more often; on the 
contrary, ChatGPT labels are more likely to be prefered in most labels. 
This indicates that ChatGPT can be used in generating labels for topics 
as heuristics for researchers.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2023-07-21</td>
					</tr>
					<tr>
					<th>Language</th>
						<td>en-us</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>OSF Preprints</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://osf.io/23x4m">https://osf.io/23x4m</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>1/3/2025, 3:59:33 PM</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.31235/osf.io/23x4m">10.31235/osf.io/23x4m</a></td>
					</tr>
					<tr>
					<th>Repository</th>
						<td>OSF</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>1/3/2025, 3:59:33 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_YWEBRZ3D">llm_author_detail.docx					</li>
				</ul>
			</li>


			<li id="item_LT2RBW24" class="item preprint">
			<h2>Can Large Language Models (or Humans) Disentangle Text?</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Preprint</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Nicolas Audinet de Pieuchon</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Adel Daoud</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Connor Thomas Jerzak</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Moa Johansson</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Richard Johansson</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>We investigate the potential of large language models (LLMs) 
to disentangle text variables--to remove the textual traces of an 
undesired forbidden variable in a task sometimes known as text 
distillation and closely related to the fairness in AI and causal 
inference literature. We employ a range of various LLM approaches in an 
attempt to disentangle text by identifying and removing information 
about a target variable while preserving other relevant signals. We show
 that in the strong test of removing sentiment, the statistical 
association between the processed text and sentiment is still detectable
 to machine learning classifiers post-LLM-disentanglement. Furthermore, 
we find that human annotators also struggle to disentangle sentiment 
while preserving other semantic content. This suggests there may be 
limited separability between concept variables in some text contexts, 
highlighting limitations of methods relying on text-level 
transformations and also raising questions about the robustness of 
disentanglement methods that achieve statistical independence in 
representation space.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2024-05-03</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>arXiv.org</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://arxiv.org/abs/2403.16584">http://arxiv.org/abs/2403.16584</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>1/7/2025, 5:29:58 PM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>arXiv:2403.16584 [cs]</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.48550/arXiv.2403.16584">10.48550/arXiv.2403.16584</a></td>
					</tr>
					<tr>
					<th>Repository</th>
						<td>arXiv</td>
					</tr>
					<tr>
					<th>Archive ID</th>
						<td>arXiv:2403.16584</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>1/7/2025, 5:29:58 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Computer Science - Computation and Language</li>
				</ul>
				<h3 class="notes">Notes:</h3>
				<ul class="notes">
					<li id="item_HZVBGR3I">
<p class="plaintext">Comment: To appear as: Nicolas Audinet de Pieuchon, Adel Daoud, Connor T. Jerzak, Moa Johansson, Richard Johansson. Can Large Language Models (or Humans) Disentangle Text? In: Sixth Workshop on NLP and Computational Social Science at NAACL, 2024</p>
					</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_5WV5AFNX">Preprint PDF					</li>
					<li id="item_D746UNEF">Snapshot					</li>
				</ul>
			</li>


			<li id="item_39YJVWKD" class="item journalArticle">
			<h2>Can Large Language Models Capture Public Opinion about Global Warming? An Empirical Assessment of Algorithmic Fidelity and Bias</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>S. Lee</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>T. Q. Peng</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>M. H. Goldberg</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>S. A. Rosenthal</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>J. E. Kotcher</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>E. W. Maibach</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>A. Leiserowitz</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Large language models (LLMs) have demonstrated their potential
 in social science research by emulating human perceptions and 
behaviors, a concept referred to as algorithmic fidelity. This study 
assesses the algorithmic fidelity and bias of LLMs by utilizing two 
nationally representative climate change surveys. The LLMs were 
conditioned on demographics and/or psychological covariates to simulate 
survey responses. The findings indicate that LLMs can effectively 
capture presidential voting behaviors but encounter challenges in 
accurately representing global warming perspectives when relevant 
covariates are not included. GPT-4 exhibits improved performance when 
conditioned on both demographics and covariates. However, disparities 
emerge in LLM estimations of the views of certain groups, with LLMs 
tending to underestimate worry about global warming among Black 
Americans. While highlighting the potential of LLMs to aid social 
science research, these results underscore the importance of meticulous 
conditioning, model selection, survey question format, and bias 
assessment when employing LLMs for survey simulation. Further 
investigation into prompt engineering and algorithm auditing is 
essential to harness the power of LLMs while addressing their inherent 
limitations.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2024-8-7</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>Can Large Language Models Capture Public Opinion about Global Warming?</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>arXiv.org</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://arxiv.org/abs/2311.00217">http://arxiv.org/abs/2311.00217</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>1/7/2025, 5:42:35 PM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>arXiv:2311.00217 [cs]</td>
					</tr>
					<tr>
					<th>Volume</th>
						<td>3</td>
					</tr>
					<tr>
					<th>Pages</th>
						<td>e0000429</td>
					</tr>
					<tr>
					<th>Publication</th>
						<td>PLOS Climate</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.1371/journal.pclm.0000429">10.1371/journal.pclm.0000429</a></td>
					</tr>
					<tr>
					<th>Issue</th>
						<td>8</td>
					</tr>
					<tr>
					<th>Journal Abbr</th>
						<td>PLOS Clim</td>
					</tr>
					<tr>
					<th>ISSN</th>
						<td>2767-3200</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>1/7/2025, 5:42:35 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Computer Science - Artificial Intelligence</li>
					<li>Computer Science - Computers and Society</li>
				</ul>
				<h3 class="notes">Notes:</h3>
				<ul class="notes">
					<li id="item_U9XBEAYJ">
<p class="plaintext">Comment: 34 pages, 6 figures, 1 table</p>
					</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_CUTVZ3RV">Preprint PDF					</li>
					<li id="item_DS5KJBLM">Snapshot					</li>
				</ul>
			</li>


			<li id="item_BKQL7C75" class="item preprint">
			<h2>Can Large Language Models Discern Evidence for Scientific Hypotheses? Case Studies in the Social Sciences</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Preprint</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Sai Koneru</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Jian Wu</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Sarah Rajtmajer</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Hypothesis formulation and testing are central to empirical 
research. A strong hypothesis is a best guess based on existing evidence
 and informed by a comprehensive view of relevant literature. However, 
with exponential increase in the number of scientific articles published
 annually, manual aggregation and synthesis of evidence related to a 
given hypothesis is a challenge. Our work explores the ability of 
current large language models (LLMs) to discern evidence in support or 
refute of specific hypotheses based on the text of scientific abstracts.
 We share a novel dataset for the task of scientific hypothesis 
evidencing using community-driven annotations of studies in the social 
sciences. We compare the performance of LLMs to several state-of-the-art
 benchmarks and highlight opportunities for future research in this 
area. The dataset is available at 
https://github.com/Sai90000/ScientificHypothesisEvidencing.git</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2024-03-26</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>Can Large Language Models Discern Evidence for Scientific Hypotheses?</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>arXiv.org</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://arxiv.org/abs/2309.06578">http://arxiv.org/abs/2309.06578</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>1/7/2025, 5:41:03 PM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>arXiv:2309.06578 [cs]</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.48550/arXiv.2309.06578">10.48550/arXiv.2309.06578</a></td>
					</tr>
					<tr>
					<th>Repository</th>
						<td>arXiv</td>
					</tr>
					<tr>
					<th>Archive ID</th>
						<td>arXiv:2309.06578</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>1/7/2025, 5:41:03 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Computer Science - Artificial Intelligence</li>
					<li>Computer Science - Computation and Language</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_7HISKUBE">Preprint PDF					</li>
					<li id="item_LCFR2I44">Snapshot					</li>
				</ul>
			</li>


			<li id="item_ZFRDWPGL" class="item preprint">
			<h2>Can Large Language Models emulate an inductive Thematic Analysis 
of semi-structured interviews? An exploration and provocation on the 
limits of the approach and the model</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Preprint</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Stefano De Paoli</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Large Language Models (LLMs) have emerged as powerful 
generative Artificial Intelligence solutions which can be applied to 
several fields and areas of work. This paper presents results and 
reflection of an experiment done to use the model GPT 3.5-Turbo to 
emulate some aspects of an inductive Thematic Analysis. Previous 
research on this subject has largely worked on conducting deductive 
analysis. Thematic Analysis is a qualitative method for analysis 
commonly used in social sciences and it is based on interpretations made
 by the human analyst(s) and the identification of explicit and latent 
meanings in qualitative data. Attempting an analysis based on human 
interpretation with an LLM clearly is a provocation but also a way to 
learn something about how these systems can or cannot be used in 
qualitative research. The paper presents the motivations for attempting 
this emulation, it reflects on how the six steps to a Thematic Analysis 
proposed by Braun and Clarke can at least partially be reproduced with 
the LLM and it also reflects on what are the outputs produced by the 
model. The paper used two existing datasets of open access 
semi-structured interviews, previously analysed with Thematic Analysis 
by other researchers. It used the previously produced analysis (and the 
related themes) to compare with the results produced by the LLM. The 
results show that the model can infer at least partially some of the 
main Themes. The objective of the paper is not to replace human analysts
 in qualitative analysis but to learn if some elements of LLM data 
manipulation can to an extent be of support for qualitative research.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2023-12-11</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>Can Large Language Models emulate an inductive Thematic Analysis of semi-structured interviews?</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>arXiv.org</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://arxiv.org/abs/2305.13014">http://arxiv.org/abs/2305.13014</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>1/7/2025, 5:39:17 PM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>arXiv:2305.13014 [cs]</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.48550/arXiv.2305.13014">10.48550/arXiv.2305.13014</a></td>
					</tr>
					<tr>
					<th>Repository</th>
						<td>arXiv</td>
					</tr>
					<tr>
					<th>Archive ID</th>
						<td>arXiv:2305.13014</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>1/7/2025, 5:39:17 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Computer Science - Computation and Language</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_NCSGVU83">Preprint PDF					</li>
					<li id="item_28RZI5R5">Snapshot					</li>
				</ul>
			</li>


			<li id="item_S6M6RHAZ" class="item preprint">
			<h2>Can Large Language Models Serve as Rational Players in Game Theory? A Systematic Analysis</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Preprint</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Caoyun Fan</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Jindou Chen</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Yaohui Jin</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Hao He</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Game theory, as an analytical tool, is frequently utilized to 
analyze human behavior in social science research. With the high 
alignment between the behavior of Large Language Models (LLMs) and 
humans, a promising research direction is to employ LLMs as substitutes 
for humans in game experiments, enabling social science research. 
However, despite numerous empirical researches on the combination of 
LLMs and game theory, the capability boundaries of LLMs in game theory 
remain unclear. In this research, we endeavor to systematically analyze 
LLMs in the context of game theory. Specifically, rationality, as the 
fundamental principle of game theory, serves as the metric for 
evaluating players' behavior -- building a clear desire, refining belief
 about uncertainty, and taking optimal actions. Accordingly, we select 
three classical games (dictator game, Rock-Paper-Scissors, and 
ring-network game) to analyze to what extent LLMs can achieve 
rationality in these three aspects. The experimental results indicate 
that even the current state-of-the-art LLM (GPT-4) exhibits substantial 
disparities compared to humans in game theory. For instance, LLMs 
struggle to build desires based on uncommon preferences, fail to refine 
belief from many simple patterns, and may overlook or modify refined 
belief when taking actions. Therefore, we consider that introducing LLMs
 into game experiments in the field of social science should be 
approached with greater caution.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2023-12-12</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>Can Large Language Models Serve as Rational Players in Game Theory?</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>arXiv.org</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://arxiv.org/abs/2312.05488">http://arxiv.org/abs/2312.05488</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>1/7/2025, 5:35:42 PM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>arXiv:2312.05488 [cs]</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.48550/arXiv.2312.05488">10.48550/arXiv.2312.05488</a></td>
					</tr>
					<tr>
					<th>Repository</th>
						<td>arXiv</td>
					</tr>
					<tr>
					<th>Archive ID</th>
						<td>arXiv:2312.05488</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>1/7/2025, 5:35:42 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Computer Science - Artificial Intelligence</li>
					<li>Computer Science - Computation and Language</li>
					<li>Computer Science - Computer Science and Game Theory</li>
				</ul>
				<h3 class="notes">Notes:</h3>
				<ul class="notes">
					<li id="item_WS8BMNXG">
<p class="plaintext">Comment: AAAI 2024</p>
					</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_3BAKECDT">Preprint PDF					</li>
					<li id="item_3X7IYN5M">Snapshot					</li>
				</ul>
			</li>


			<li id="item_RHSYEGLX" class="item preprint">
			<h2>Can Large Language Models Transform Computational Social Science?</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Preprint</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Caleb Ziems</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>William Held</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Omar Shaikh</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Jiaao Chen</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Zhehao Zhang</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Diyi Yang</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Large Language Models (LLMs) are capable of successfully 
performing many language processing tasks zero-shot (without training 
data). If zero-shot LLMs can also reliably classify and explain social 
phenomena like persuasiveness and political ideology, then LLMs could 
augment the Computational Social Science (CSS) pipeline in important 
ways. This work provides a road map for using LLMs as CSS tools. Towards
 this end, we contribute a set of prompting best practices and an 
extensive evaluation pipeline to measure the zero-shot performance of 13
 language models on 25 representative English CSS benchmarks. On 
taxonomic labeling tasks (classification), LLMs fail to outperform the 
best fine-tuned models but still achieve fair levels of agreement with 
humans. On free-form coding tasks (generation), LLMs produce 
explanations that often exceed the quality of crowdworkers' gold 
references. We conclude that the performance of today's LLMs can augment
 the CSS research pipeline in two ways: (1) serving as zero-shot data 
annotators on human annotation teams, and (2) bootstrapping challenging 
creative generation tasks (e.g., explaining the underlying attributes of
 a text). In summary, LLMs are posed to meaningfully participate in 
social science analysis in partnership with humans.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2024-02-26</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>arXiv.org</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://arxiv.org/abs/2305.03514">http://arxiv.org/abs/2305.03514</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>1/7/2025, 5:39:46 PM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>arXiv:2305.03514 [cs]</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.48550/arXiv.2305.03514">10.48550/arXiv.2305.03514</a></td>
					</tr>
					<tr>
					<th>Repository</th>
						<td>arXiv</td>
					</tr>
					<tr>
					<th>Archive ID</th>
						<td>arXiv:2305.03514</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>1/7/2025, 5:39:46 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Computer Science - Computation and Language</li>
					<li>Computer Science - Machine Learning</li>
				</ul>
				<h3 class="notes">Notes:</h3>
				<ul class="notes">
					<li id="item_A7NRS5M3">
<p class="plaintext">Comment: To appear in "Computational Linguistics" (CL)</p>
					</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_PQUHL8MQ">Preprint PDF					</li>
					<li id="item_YCPFNHIV">Snapshot					</li>
				</ul>
			</li>


			<li id="item_RMNMA66Z" class="item preprint">
			<h2>Can Machines Think Like Humans? A Behavioral Evaluation of LLM-Agents in Dictator Games</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Preprint</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Ji Ma</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>As Large Language Model (LLM)-based agents increasingly 
undertake real-world tasks and engage with human society, how well do we
 understand their behaviors? We (1) investigate how LLM agents' 
prosocial behaviors---a fundamental social norm---can be induced by 
different personas and benchmarked against human behaviors; and (2) 
introduce a behavioral and social science approach to evaluate LLM 
agents' decision-making. We explored how different personas and 
experimental framings affect these AI agents' altruistic behavior in 
dictator games and compared their behaviors within the same LLM family, 
across various families, and with human behaviors. The findings reveal 
substantial variations and inconsistencies among LLMs and notable 
differences compared to human behaviors. Merely assigning a human-like 
identity to LLMs does not produce human-like behaviors. Despite being 
trained on extensive human-generated data, these AI agents are unable to
 capture the internal processes of human decision-making. Their 
alignment with human is highly variable and dependent on specific model 
architectures and prompt formulations; even worse, such dependence does 
not follow a clear pattern. LLMs can be useful task-specific tools but 
are not yet intelligent human-like agents.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2024-10-29</td>
					</tr>
					<tr>
					<th>Language</th>
						<td>en-us</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>Can Machines Think Like Humans?</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>OSF Preprints</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://osf.io/s37ax">https://osf.io/s37ax</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>1/3/2025, 4:24:20 PM</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.31235/osf.io/s37ax">10.31235/osf.io/s37ax</a></td>
					</tr>
					<tr>
					<th>Repository</th>
						<td>OSF</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>1/3/2025, 4:24:20 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_4UXLYRNG">OSF Preprint					</li>
				</ul>
			</li>


			<li id="item_7NQDG2MC" class="item preprint">
			<h2>Can Unconfident LLM Annotations Be Used for Confident Conclusions?</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Preprint</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Kristina Gligorić</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Tijana Zrnic</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Cinoo Lee</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Emmanuel J. Candès</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Dan Jurafsky</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Large language models (LLMs) have shown high agreement with 
human raters across a variety of tasks, demonstrating potential to ease 
the challenges of human data collection. In computational social science
 (CSS), researchers are increasingly leveraging LLM annotations to 
complement slow and expensive human annotations. Still, guidelines for 
collecting and using LLM annotations, without compromising the validity 
of downstream conclusions, remain limited. We introduce 
Confidence-Driven Inference: a method that combines LLM annotations and 
LLM confidence indicators to strategically select which human 
annotations should be collected, with the goal of producing accurate 
statistical estimates and provably valid confidence intervals while 
reducing the number of human annotations needed. Our approach comes with
 safeguards against LLM annotations of poor quality, guaranteeing that 
the conclusions will be both valid and no less accurate than if we only 
relied on human annotations. We demonstrate the effectiveness of 
Confidence-Driven Inference over baselines in statistical estimation 
tasks across three CSS settings--text politeness, stance, and 
bias--reducing the needed number of human annotations by over 25% in 
each. Although we use CSS settings for demonstration, Confidence-Driven 
Inference can be used to estimate most standard quantities across a 
broad range of NLP problems.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2024-08-27</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>arXiv.org</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://arxiv.org/abs/2408.15204">http://arxiv.org/abs/2408.15204</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>1/7/2025, 5:11:58 PM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>arXiv:2408.15204 [cs]</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.48550/arXiv.2408.15204">10.48550/arXiv.2408.15204</a></td>
					</tr>
					<tr>
					<th>Repository</th>
						<td>arXiv</td>
					</tr>
					<tr>
					<th>Archive ID</th>
						<td>arXiv:2408.15204</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>1/7/2025, 5:11:58 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Computer Science - Artificial Intelligence</li>
					<li>Computer Science - Computation and Language</li>
					<li>Computer Science - Human-Computer Interaction</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_T652KL9E">Preprint PDF					</li>
					<li id="item_6JZHG27A">Snapshot					</li>
				</ul>
			</li>


			<li id="item_28M3WHNC" class="item conferencePaper">
			<h2>Can we trust the evaluation on ChatGPT?</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Conference Paper</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Rachith Aiyappa</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Jisun An</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Haewoon Kwak</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Yong-Yeol Ahn</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>ChatGPT, the first large language model (LLM) with mass 
adoption, has demonstrated remarkable performance in numerous natural 
language tasks. Despite its evident usefulness, evaluating ChatGPT's 
performance in diverse problem domains remains challenging due to the 
closed nature of the model and its continuous updates via Reinforcement 
Learning from Human Feedback (RLHF). We highlight the issue of data 
contamination in ChatGPT evaluations, with a case study of the task of 
stance detection. We discuss the challenge of preventing data 
contamination and ensuring fair model evaluation in the age of closed 
and continuously trained models.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2023</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>arXiv.org</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://arxiv.org/abs/2303.12767">http://arxiv.org/abs/2303.12767</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>1/8/2025, 1:11:10 PM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>arXiv:2303.12767 [cs]</td>
					</tr>
					<tr>
					<th>Pages</th>
						<td>47-54</td>
					</tr>
					<tr>
					<th>Proceedings Title</th>
						<td>Proceedings of the 3rd Workshop on Trustworthy Natural Language Processing (TrustNLP 2023)</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.18653/v1/2023.trustnlp-1.5">10.18653/v1/2023.trustnlp-1.5</a></td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>1/8/2025, 1:11:10 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Computer Science - Artificial Intelligence</li>
					<li>Computer Science - Computation and Language</li>
					<li>Computer Science - Machine Learning</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_H7IYC5RN">Preprint PDF					</li>
					<li id="item_9TT3RG6U">Snapshot					</li>
				</ul>
			</li>


			<li id="item_URFWKPVZ" class="item journalArticle">
			<h2>Capturing a News Frame – Comparing Machine-Learning Approaches to Frame Analysis with Different Degrees of Supervision</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Olga Eisele</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Tobias Heidenreich</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Olga Litvyak</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Hajo G. Boomgaarden</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>The empirical identification of frames drawing on automated 
text analysis has been discussed intensely with regard to the validity 
of measurements. Adding to an evolving discussion on automated frame 
identification, we systematically contrast different machine-learning 
approaches with a manually coded gold standard to shed light on the 
implications of using one or the other: (1) topic modeling, (2) 
keyword-assisted topic modeling (keyATM), and (3) supervised machine 
learning as three popular and/or promising approaches. Manual coding is 
based on the Policy Frames codebook, providing an established base that 
allows future research to dovetail our contribution. Analysing a large 
dataset of 12 Austrian newspapers’ EU coverage over 11 years 
(2009–2019), we contribute to addressing the methodological challenges 
that have emerged for social scientists interested in employing 
automated tools for frame analysis. While results confirm the 
superiority of supervised machine-learning, the semi-supervised approach
 (keyATM) seems unfit for frame analysis, whereas the topic model covers
 the middle ground. Results are extensively discussed regarding their 
implications for the validity of approaches.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2023-07-03</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>Taylor and Francis+NEJM</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://doi.org/10.1080/19312458.2023.2230560">https://doi.org/10.1080/19312458.2023.2230560</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>1/6/2025, 2:18:58 PM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>Publisher: Routledge
_eprint: https://doi.org/10.1080/19312458.2023.2230560</td>
					</tr>
					<tr>
					<th>Volume</th>
						<td>17</td>
					</tr>
					<tr>
					<th>Pages</th>
						<td>205-226</td>
					</tr>
					<tr>
					<th>Publication</th>
						<td>Communication Methods and Measures</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.1080/19312458.2023.2230560">10.1080/19312458.2023.2230560</a></td>
					</tr>
					<tr>
					<th>Issue</th>
						<td>3</td>
					</tr>
					<tr>
					<th>ISSN</th>
						<td>1931-2458</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>1/6/2025, 2:18:58 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_ZZB9VWLT">Full Text PDF					</li>
				</ul>
			</li>


			<li id="item_72BR2NV9" class="item journalArticle">
			<h2>CERN for AI: A Theoretical Framework for Autonomous Simulation-Based Artificial Intelligence Testing and Alignment</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Ljubisa Bojic</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Matteo Cinelli</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Dubravko Culibrk</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Boris Delibasic</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>This paper explores the potential of a multidisciplinary 
approach to testing and aligning artificial intelligence (AI), 
specifically focusing on large language models (LLMs). Due to the rapid 
development and wide application of LLMs, challenges such as ethical 
alignment, controllability, and predictability of these models emerged 
as global risks. This study investigates an innovative simulation-based 
multi-agent system within a virtual reality framework that replicates 
the real-world environment. The framework is populated by automated 
'digital citizens,' simulating complex social structures and 
interactions to examine and optimize AI. Application of various theories
 from the fields of sociology, social psychology, computer science, 
physics, biology, and economics demonstrates the possibility of a more 
human-aligned and socially responsible AI. The purpose of such a digital
 environment is to provide a dynamic platform where advanced AI agents 
can interact and make independent decisions, thereby mimicking realistic
 scenarios. The actors in this digital city, operated by the LLMs, serve
 as the primary agents, exhibiting high degrees of autonomy. While this 
approach shows immense potential, there are notable challenges and 
limitations, most significantly the unpredictable nature of real-world 
social dynamics. This research endeavors to contribute to the 
development and refinement of AI, emphasizing the integration of social,
 ethical, and theoretical dimensions for future research.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2024-08-24</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>CERN for AI</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>arXiv.org</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://arxiv.org/abs/2312.09402">http://arxiv.org/abs/2312.09402</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>1/7/2025, 4:53:53 PM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>arXiv:2312.09402 [cs]</td>
					</tr>
					<tr>
					<th>Volume</th>
						<td>12</td>
					</tr>
					<tr>
					<th>Pages</th>
						<td>15</td>
					</tr>
					<tr>
					<th>Publication</th>
						<td>European Journal of Futures Research</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.1186/s40309-024-00238-0">10.1186/s40309-024-00238-0</a></td>
					</tr>
					<tr>
					<th>Issue</th>
						<td>1</td>
					</tr>
					<tr>
					<th>Journal Abbr</th>
						<td>Eur J Futures Res</td>
					</tr>
					<tr>
					<th>ISSN</th>
						<td>2195-2248</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>1/7/2025, 4:53:53 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Computer Science - Artificial Intelligence</li>
					<li>Computer Science - Computer Science and Game Theory</li>
					<li>Computer Science - Computers and Society</li>
				</ul>
				<h3 class="notes">Notes:</h3>
				<ul class="notes">
					<li id="item_6XA7RCG7">
<p class="plaintext">Comment: 32 pages, 4 figures, 2 tables</p>
					</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_BZ6BL8V3">Preprint PDF					</li>
					<li id="item_L8AMK7A5">Snapshot					</li>
				</ul>
			</li>


			<li id="item_FJ8SKWCL" class="item conferencePaper">
			<h2>Chain of Explanation: New Prompting Method to Generate Quality Natural Language Explanation for Implicit Hate Speech</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Conference Paper</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Fan Huang</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Haewoon Kwak</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Jisun An</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Recent studies have exploited advanced generative language 
models to generate Natural Language Explanations (NLE) for why a certain
 text could be hateful. We propose the Chain of Explanation (CoE) 
Prompting method, using the heuristic words and target group, to 
generate high-quality NLE for implicit hate speech. We improved the BLUE
 score from 44.0 to 62.3 for NLE generation by providing accurate target
 information. We then evaluate the quality of generated NLE using 
various automatic metrics and human annotations of informativeness and 
clarity scores.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>April 30, 2023</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>Chain of Explanation</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>ACM Digital Library</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://doi.org/10.1145/3543873.3587320">https://doi.org/10.1145/3543873.3587320</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>1/7/2025, 7:00:00 PM</td>
					</tr>
					<tr>
					<th>Place</th>
						<td>New York, NY, USA</td>
					</tr>
					<tr>
					<th>Publisher</th>
						<td>Association for Computing Machinery</td>
					</tr>
					<tr>
					<th>ISBN</th>
						<td>978-1-4503-9419-2</td>
					</tr>
					<tr>
					<th>Pages</th>
						<td>90–93</td>
					</tr>
					<tr>
					<th>Series</th>
						<td>WWW '23 Companion</td>
					</tr>
					<tr>
					<th>Proceedings Title</th>
						<td>Companion Proceedings of the ACM Web Conference 2023</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.1145/3543873.3587320">10.1145/3543873.3587320</a></td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>1/8/2025, 1:11:39 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_E7RXKREJ">Submitted Version					</li>
				</ul>
			</li>


			<li id="item_MGPEAFVM" class="item preprint">
			<h2>Chatbots Are Not Reliable Text Annotators</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Preprint</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Ross Deans Kristensen-McLachlan</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Miceal Canavan</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Márton Kardos</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Mia Jacobsen</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Lene Aarøe</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Recent research highlights the significant potential of 
ChatGPT for text annotation in social science research. However, ChatGPT
 is a closed-source product which has major drawbacks with regards to 
transparency, reproducibility, cost, and data protection. Recent 
advances in open-source (OS) large language models (LLMs) offer 
alternatives which remedy these challenges. This means that it is 
important to evaluate the performance of OS LLMs relative to ChatGPT and
 standard approaches to supervised machine learning classification. We 
conduct a systematic comparative evaluation of the performance of a 
range of OS LLM models alongside ChatGPT, using both zero- and few-shot 
learning as well as generic and custom prompts, with results compared to
 more traditional supervised classification models. Using a new dataset 
of Tweets from US news media, and focusing on simple binary text 
annotation tasks for standard social science concepts, we find 
significant variation in the performance of ChatGPT and OS models across
 the tasks, and that supervised classifiers consistently outperform 
both. Given the unreliable performance of ChatGPT and the significant 
challenges it poses to Open Science we advise against using ChatGPT for 
substantive text annotation tasks in social science research.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2023-11-09</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>arXiv.org</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://arxiv.org/abs/2311.05769">http://arxiv.org/abs/2311.05769</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>1/7/2025, 5:42:57 PM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>arXiv:2311.05769 [cs]</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.48550/arXiv.2311.05769">10.48550/arXiv.2311.05769</a></td>
					</tr>
					<tr>
					<th>Repository</th>
						<td>arXiv</td>
					</tr>
					<tr>
					<th>Archive ID</th>
						<td>arXiv:2311.05769</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>1/7/2025, 5:42:57 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Computer Science - Artificial Intelligence</li>
					<li>Computer Science - Computation and Language</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_VCQ94EUL">Preprint PDF					</li>
					<li id="item_J3U4ADHZ">Snapshot					</li>
				</ul>
			</li>


			<li id="item_WHU4GY8V" class="item preprint">
			<h2>ChatGPT for Text Annotation? Mind the Hype!</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Preprint</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Etienne Ollion</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Rubing Shen</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Ana Macanovic</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Arnault Chatelain</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>In the past months, researchers have enthusiastically 
discussed the relevance of zero- or few-shot classifiers like ChatGPT 
for text annotation. Should these models prove to be performant, they 
would open up new continents for research, and beyond. To assess the 
merits and limits of this approach, we conducted a systematic literature
 review. Reading all the articles doing zero or few-shot text annotation
 in the human and social sciences, we found that these fewshot learners 
offer enticing, yet mixed results on text annotation tasks. The 
performance scores can vary widely, with some being average and some 
being very low. Besides, zero or few-shot models are often outperformed 
by models fine-tuned with human annotations. Our findings thus suggest 
that, to date, the evidence about their effectiveness remains partial, 
but also that their use raises several important questions about the 
reproducibility of results, about privacy and copyright issues, and 
about the primacy of the English language. While we definitely believe 
that there are numerous ways to harness this powerful technology 
productively, we also need to harness it without falling for the hype.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2023-10-04</td>
					</tr>
					<tr>
					<th>Language</th>
						<td>en</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>ChatGPT for Text Annotation?</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>SocArXiv</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://osf.io/x58kn">https://osf.io/x58kn</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>5/6/2024, 12:37:28 PM</td>
					</tr>
					<tr>
					<th>Rights</th>
						<td>https://creativecommons.org/licenses/by-nd/4.0/legalcode</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.31235/osf.io/x58kn">10.31235/osf.io/x58kn</a></td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>5/6/2024, 12:37:28 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>5/6/2024, 12:37:28 PM</td>
					</tr>
				</tbody></table>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_BRK95UK9">Ollion et al. - 2023 - ChatGPT for Text Annotation Mind the Hype!.pdf					</li>
				</ul>
			</li>


			<li id="item_WJRNQC89" class="item preprint">
			<h2>ChatGPT Rates Natural Language Explanation Quality Like Humans: But on Which Scales?</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Preprint</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Fan Huang</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Haewoon Kwak</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Kunwoo Park</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Jisun An</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>As AI becomes more integral in our lives, the need for 
transparency and responsibility grows. While natural language 
explanations (NLEs) are vital for clarifying the reasoning behind AI 
decisions, evaluating them through human judgments is complex and 
resource-intensive due to subjectivity and the need for fine-grained 
ratings. This study explores the alignment between ChatGPT and human 
assessments across multiple scales (i.e., binary, ternary, and 7-Likert 
scale). We sample 300 data instances from three NLE datasets and collect
 900 human annotations for both informativeness and clarity scores as 
the text quality measurement. We further conduct paired comparison 
experiments under different ranges of subjectivity scores, where the 
baseline comes from 8,346 human annotations. Our results show that 
ChatGPT aligns better with humans in more coarse-grained scales. Also, 
paired comparisons and dynamic prompting (i.e., providing semantically 
similar examples in the prompt) improve the alignment. This research 
advances our understanding of large language models' capabilities to 
assess the text explanation quality in different configurations for 
responsible AI development.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2024-03-26</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>ChatGPT Rates Natural Language Explanation Quality Like Humans</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>arXiv.org</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://arxiv.org/abs/2403.17368">http://arxiv.org/abs/2403.17368</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>1/8/2025, 1:10:57 PM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>arXiv:2403.17368 [cs]</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.48550/arXiv.2403.17368">10.48550/arXiv.2403.17368</a></td>
					</tr>
					<tr>
					<th>Repository</th>
						<td>arXiv</td>
					</tr>
					<tr>
					<th>Archive ID</th>
						<td>arXiv:2403.17368</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>1/8/2025, 1:10:57 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Computer Science - Artificial Intelligence</li>
					<li>Computer Science - Computation and Language</li>
				</ul>
				<h3 class="notes">Notes:</h3>
				<ul class="notes">
					<li id="item_LPPXXAL2">
<p class="plaintext">Comment: Accpeted by LREC-COLING 2024 main conference, long paper</p>
					</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_PS2GBPR4">Preprint PDF					</li>
					<li id="item_PKYVKF7F">Snapshot					</li>
				</ul>
			</li>


			<li id="item_9FLLZWTA" class="item preprint">
			<h2>ChatGPT-4 Outperforms Experts and Crowd Workers in Annotating Political Twitter Messages with Zero-Shot Learning</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Preprint</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Petter Törnberg</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>This paper assesses the accuracy, reliability and bias of the 
Large Language Model (LLM) ChatGPT-4 on the text analysis task of 
classifying the political affiliation of a Twitter poster based on the 
content of a tweet. The LLM is compared to manual annotation by both 
expert classifiers and crowd workers, generally considered the gold 
standard for such tasks. We use Twitter messages from United States 
politicians during the 2020 election, providing a ground truth against 
which to measure accuracy. The paper finds that ChatGPT-4 has achieves 
higher accuracy, higher reliability, and equal or lower bias than the 
human classifiers. The LLM is able to correctly annotate messages that 
require reasoning on the basis of contextual knowledge, and inferences 
around the author's intentions - traditionally seen as uniquely human 
abilities. These findings suggest that LLM will have substantial impact 
on the use of textual data in the social sciences, by enabling 
interpretive research at a scale.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2023-04-13</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>arXiv.org</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://arxiv.org/abs/2304.06588">http://arxiv.org/abs/2304.06588</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>1/7/2025, 5:40:06 PM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>arXiv:2304.06588 [cs]</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.48550/arXiv.2304.06588">10.48550/arXiv.2304.06588</a></td>
					</tr>
					<tr>
					<th>Repository</th>
						<td>arXiv</td>
					</tr>
					<tr>
					<th>Archive ID</th>
						<td>arXiv:2304.06588</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>1/7/2025, 5:40:06 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Computer Science - Artificial Intelligence</li>
					<li>Computer Science - Computation and Language</li>
					<li>Computer Science - Social and Information Networks</li>
				</ul>
				<h3 class="notes">Notes:</h3>
				<ul class="notes">
					<li id="item_EMA35YHI">
<p class="plaintext">Comment: 5 pages, 3 figures</p>
					</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_YYC3HNKB">Preprint PDF					</li>
					<li id="item_W79ZL6E3">Snapshot					</li>
				</ul>
			</li>


			<li id="item_Q893RBXV" class="item journalArticle">
			<h2>Cheap, Quick, and Rigorous: Artificial Intelligence and the Systematic Literature Review</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Cameron F. Atkinson</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>The systematic literature review (SLR) is the gold standard in
 providing research a firm evidence foundation to support 
decision-making. Researchers seeking to increase the rigour, 
transparency, and replicability of their SLRs are provided a range of 
guidelines towards these ends. Artificial Intelligence (AI) and Machine 
Learning Techniques (MLTs) developed with computer programming languages
 can provide methods to increase the speed, rigour, transparency, and 
repeatability of SLRs. Aimed towards researchers with coding experience,
 and who want to utilise AI and MLTs to synthesise and abstract data 
obtained through a SLR, this article sets out how computer languages can
 be used to facilitate unsupervised machine learning for synthesising 
and abstracting data sets extracted during a SLR. Utilising an already 
known qualitative method, Deductive Qualitative Analysis, this article 
illustrates the supportive role that AI and MLTs can play in the coding 
and categorisation of extracted SLR data, and in synthesising SLR data. 
Using a data set extracted during a SLR as a proof of concept, this 
article will include the coding used to create a well-established MLT, 
Topic Modelling using Latent Dirichlet allocation. This technique 
provides a working example of how researchers can use AI and MLTs to 
automate the data synthesis and abstraction stage of their SLR, and aide
 in increasing the speed, frugality, and rigour of research projects.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2024-04-01</td>
					</tr>
					<tr>
					<th>Language</th>
						<td>en</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>Cheap, Quick, and Rigorous</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>SAGE Journals</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://doi.org/10.1177/08944393231196281">https://doi.org/10.1177/08944393231196281</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>12/24/2024, 7:09:58 PM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>Publisher: SAGE Publications Inc</td>
					</tr>
					<tr>
					<th>Volume</th>
						<td>42</td>
					</tr>
					<tr>
					<th>Pages</th>
						<td>376-393</td>
					</tr>
					<tr>
					<th>Publication</th>
						<td>Social Science Computer Review</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.1177/08944393231196281">10.1177/08944393231196281</a></td>
					</tr>
					<tr>
					<th>Issue</th>
						<td>2</td>
					</tr>
					<tr>
					<th>ISSN</th>
						<td>0894-4393</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>12/24/2024, 7:09:58 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_6XLATYZB">SAGE PDF Full Text					</li>
				</ul>
			</li>


			<li id="item_FK5VVHSG" class="item conferencePaper">
			<h2>Classifying Sentiments on Social Media Texts: A GPT-4 Preliminary Study</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Conference Paper</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Lany Laguna Maceda</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Jennifer Laraya Llovido</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Miles Biago Artiaga</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Mideth Balawiswis Abisado</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>In today's digital age, social media has become a hub for 
people to express their thoughts and feelings. Sentiment classification 
discerns public opinions and trends to understand their sentiments 
towards a certain topic. Often, achieving accurate sentiment 
classifications in large datasets necessitate the use of human-annotated
 training data which can be costly and time-consuming. Large Language 
Models (LLMs) like the Generative Pre-trained models by OpenAI have 
surged in popularity due to its capabilities in understanding the given 
tasks. In this preliminary study, we report the performance of the 
latest OpenAI GPT-4 using zero- and one-shot learning approaches on 
classifying sentiments when fed with social media dataset. Notably, the 
latter approach written in English which mimics the instructions 
designed for human annotators, achieved a substantial agreement (k = 
0.77) with human annotations, displaying high accuracy, precision, and 
recall accordingly even without explicit training data. Meanwhile, the 
fine-tuned mBERT resulted to lower evaluation scores than the GPT-4. Our
 findings provide foundational insights into the strengths and 
limitations of GPT-4 for sentiment classification in a social media 
dataset, setting the groundwork for broad future research in this field.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>March 5, 2024</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>Classifying Sentiments on Social Media Texts</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>ACM Digital Library</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://doi.org/10.1145/3639233.3639353">https://doi.org/10.1145/3639233.3639353</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>5/4/2024, 8:00:00 PM</td>
					</tr>
					<tr>
					<th>Place</th>
						<td>New York, NY, USA</td>
					</tr>
					<tr>
					<th>Publisher</th>
						<td>Association for Computing Machinery</td>
					</tr>
					<tr>
					<th>ISBN</th>
						<td>9798400709227</td>
					</tr>
					<tr>
					<th>Pages</th>
						<td>19–24</td>
					</tr>
					<tr>
					<th>Series</th>
						<td>NLPIR '23</td>
					</tr>
					<tr>
					<th>Proceedings Title</th>
						<td>Proceedings of the 2023 7th International Conference on Natural Language Processing and Information Retrieval</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.1145/3639233.3639353">10.1145/3639233.3639353</a></td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>5/5/2024, 6:40:55 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>5/5/2024, 6:40:55 PM</td>
					</tr>
				</tbody></table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>GPT-4</li>
					<li>LLM Prompting</li>
					<li>Sentiment Annotation</li>
					<li>Social Media Data</li>
				</ul>
			</li>


			<li id="item_68VJRGUV" class="item journalArticle">
			<h2>Claude 2.0 large language model: Tackling a real-world classification problem with a new iterative prompt engineering approach</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Loredana Caruccio</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Stefano Cirillo</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Giuseppe Polese</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Giandomenico Solimando</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Shanmugam Sundaramurthy</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Genoveffa Tortora</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>In the last year, Large Language Models (LLMs) have 
transformed the way of tackling problems, opening up new perspectives in
 various works and research fields, due to their ability to generate and
 understand human languages. In this regard, the recent release of 
Claude 2.0 has contributed to the processing of more complex prompts. In
 this scenario, the goal of this paper is to evaluate the effectiveness 
of Claude 2.0 in a specific classification task. In particular, we 
considered the Forest cover-type problem, concerning the prediction of a
 cover-type value according to the geospatial characterization of target
 worldwide areas. To this end, we propose a novel iterative prompt 
template engineering approach, which integrates files by exploiting 
prompts and evaluates the quality of responses provided by the LLM. 
Moreover, we conducted several comparative analyses to evaluate the 
effectiveness of Claude 2.0 with respect to online and batch learning 
models. The results demonstrated that, although some online and batch 
models performed better than Claude 2.0, the new iterative prompt 
engineering approach improved the quality of responses, leading to 
better performance with increases ranging from 14% to 32% in terms of 
accuracy, precision, recall, and F1-score.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2024-03-01</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>Claude 2.0 large language model</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>ScienceDirect</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://www.sciencedirect.com/science/article/pii/S2667305324000127">https://www.sciencedirect.com/science/article/pii/S2667305324000127</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>1/8/2025, 3:28:37 PM</td>
					</tr>
					<tr>
					<th>Volume</th>
						<td>21</td>
					</tr>
					<tr>
					<th>Pages</th>
						<td>200336</td>
					</tr>
					<tr>
					<th>Publication</th>
						<td>Intelligent Systems with Applications</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.1016/j.iswa.2024.200336">10.1016/j.iswa.2024.200336</a></td>
					</tr>
					<tr>
					<th>Journal Abbr</th>
						<td>Intelligent Systems with Applications</td>
					</tr>
					<tr>
					<th>ISSN</th>
						<td>2667-3053</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>1/8/2025, 3:28:37 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Claude 2.0</li>
					<li>Forest cover-type</li>
					<li>Large language model</li>
					<li>Machine learning</li>
					<li>Massive online analytics</li>
					<li>Online learning</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_QZIW66G5">ScienceDirect Snapshot					</li>
				</ul>
			</li>


			<li id="item_7PPLD427" class="item webpage">
			<h2>Comparing automated text classification methods - ScienceDirect</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Web Page</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Online social media drive the growth of unstructured text 
data. Many marketing applications require structuring this data at 
scales non-accessible to human coding, e.g., to detect communication 
shifts in sentiment or other researcher-defined content categories. 
Several methods have been proposed to automatically classify 
unstructured text. This paper compares the performance of ten such 
approaches (five lexicon-based, five machine learning algorithms) across
 41 social media datasets covering major social media platforms, various
 sample sizes, and languages. So far, marketing research relies 
predominantly on support vector machines (SVM) and Linguistic Inquiry 
and Word Count (LIWC). Across all tasks we study, either random forest 
(RF) or naive Bayes (NB) performs best in terms of correctly uncovering 
human intuition. In particular, RF exhibits consistently high 
performance for three-class sentiment, NB for small samples sizes. SVM 
never outperform the remaining methods. All lexicon-based approaches, 
LIWC in particular, perform poorly compared with machine learning. In 
some applications, accuracies only slightly exceed chance. Since 
additional considerations of text classification choice are also in 
favor of NB and RF, our results suggest that marketing research can 
benefit from considering these alternatives.</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://www.sciencedirect.com/science/article/pii/S0167811618300545?ref=pdf_download&amp;fr=RR-2&amp;rr=87d01bb9aa469020">https://www.sciencedirect.com/science/article/pii/S0167811618300545?ref=pdf_download&amp;fr=RR-2&amp;rr=87d01bb9aa469020</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>5/1/2024, 9:27:26 AM</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>5/1/2024, 9:27:26 AM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 8:05:38 PM</td>
					</tr>
				</tbody></table>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_D8YCLZ2S">Comparing automated text classification methods - ScienceDirect					</li>
				</ul>
			</li>


			<li id="item_3DCSRMFG" class="item journalArticle">
			<h2>Comparing Chatbots and Online Surveys for (Longitudinal) Data 
Collection: An Investigation of Response Characteristics, Data Quality, 
and User Evaluation</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Brahim Zarouali</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Theo Araujo</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Jakob Ohme</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Claes de Vreese</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>As chatbots are gaining more popularity than ever, they have 
recently been considered as interesting tools for survey administration 
in social science research. To explore this idea, we investigated the 
extent to which there are differences in response characteristics and 
data quality between a traditional, web-based survey and a 
conversational, chatbot-based survey (which we integrated in an instant 
messaging app). In addition, we zoomed into how respondents evaluate 
both survey modes. Using a longitudinal design, we also explored how 
response characteristics evolved over a period of two weeks. Overall, we
 did not find evidence that chatbots might be better survey 
administration tools than web surveys. On the contrary, the web survey 
often seemed to generate more favorable response characteristics and 
data quality. Finally, when it comes to user perceptions, we found that 
the chatbot survey was evaluated less favorably in terms of perceived 
enjoyment, usefulness, and security. Based on these results, we draw 
conclusions about whether chatbots can be considered as valid 
alternatives for traditional web survey methods.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2024-01-02</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>Comparing Chatbots and Online Surveys for (Longitudinal) Data Collection</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>Taylor and Francis+NEJM</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://doi.org/10.1080/19312458.2022.2156489">https://doi.org/10.1080/19312458.2022.2156489</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>1/6/2025, 2:00:14 PM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>Publisher: Routledge
_eprint: https://doi.org/10.1080/19312458.2022.2156489</td>
					</tr>
					<tr>
					<th>Volume</th>
						<td>18</td>
					</tr>
					<tr>
					<th>Pages</th>
						<td>72-91</td>
					</tr>
					<tr>
					<th>Publication</th>
						<td>Communication Methods and Measures</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.1080/19312458.2022.2156489">10.1080/19312458.2022.2156489</a></td>
					</tr>
					<tr>
					<th>Issue</th>
						<td>1</td>
					</tr>
					<tr>
					<th>ISSN</th>
						<td>1931-2458</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>1/6/2025, 2:00:14 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_X2RHKKS2">Accepted Version					</li>
				</ul>
			</li>


			<li id="item_QQGG8HUC" class="item journalArticle">
			<h2>Comparing public communication in democracies and autocracies: automated text analyses of speeches by heads of government</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Seraphine F. Maerz</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Carsten Q. Schneider</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Renewed efforts at empirically distinguishing between 
different forms of political regimes leave out the cultural dimension. 
In this article, we demonstrate how modern computational tools can be 
used to fill this gap. We employ web-scraping techniques to generate a 
data set of speeches by heads of government in European democracies and 
autocratic regimes around the globe. Our data set includes 4740 speeches
 delivered between 1999 and 2019 by 40 political leaders of 27 
countries. By scaling the results of a dictionary application, we show 
how, in comparative terms, liberal or illiberal the leaders present 
themselves to their national and international audience. In order to 
gauge whether our liberalness scale reveals meaningful distinctions, we 
perform a series of validity tests: criterion validity, qualitative 
hand-coding, unsupervised topic modeling, and network analysis. All 
tests suggest that our liberalness scale does capture meaningful 
differences between political regimes despite the large heterogeneity of
 our data.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2020-04-01</td>
					</tr>
					<tr>
					<th>Language</th>
						<td>en</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>Comparing public communication in democracies and autocracies</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>Springer Link</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://doi.org/10.1007/s11135-019-00885-7">https://doi.org/10.1007/s11135-019-00885-7</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>5/5/2024, 7:15:12 PM</td>
					</tr>
					<tr>
					<th>Volume</th>
						<td>54</td>
					</tr>
					<tr>
					<th>Pages</th>
						<td>517-545</td>
					</tr>
					<tr>
					<th>Publication</th>
						<td>Quality &amp; Quantity</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.1007/s11135-019-00885-7">10.1007/s11135-019-00885-7</a></td>
					</tr>
					<tr>
					<th>Issue</th>
						<td>2</td>
					</tr>
					<tr>
					<th>Journal Abbr</th>
						<td>Qual Quant</td>
					</tr>
					<tr>
					<th>ISSN</th>
						<td>1573-7845</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>5/5/2024, 7:15:12 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>5/5/2024, 7:15:12 PM</td>
					</tr>
				</tbody></table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Democratic backsliding</li>
					<li>Dictionary</li>
					<li>Political speeches of autocrats and democrats</li>
					<li>Qualities of democracy</li>
					<li>Text-as-data</li>
					<li>Topic models</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_X92DBJYK">Full Text PDF					</li>
				</ul>
			</li>


			<li id="item_XTAEL6DD" class="item preprint">
			<h2>CoMPosT: Characterizing and Evaluating Caricature in LLM Simulations</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Preprint</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Myra Cheng</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Tiziano Piccardi</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Diyi Yang</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Recent work has aimed to capture nuances of human behavior by 
using LLMs to simulate responses from particular demographics in 
settings like social science experiments and public opinion surveys. 
However, there are currently no established ways to discuss or evaluate 
the quality of such LLM simulations. Moreover, there is growing concern 
that these LLM simulations are flattened caricatures of the personas 
that they aim to simulate, failing to capture the multidimensionality of
 people and perpetuating stereotypes. To bridge these gaps, we present 
CoMPosT, a framework to characterize LLM simulations using four 
dimensions: Context, Model, Persona, and Topic. We use this framework to
 measure open-ended LLM simulations' susceptibility to caricature, 
defined via two criteria: individuation and exaggeration. We evaluate 
the level of caricature in scenarios from existing work on LLM 
simulations. We find that for GPT-4, simulations of certain demographics
 (political and marginalized groups) and topics (general, 
uncontroversial) are highly susceptible to caricature.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2023-10-17</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>CoMPosT</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>arXiv.org</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://arxiv.org/abs/2310.11501">http://arxiv.org/abs/2310.11501</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>1/7/2025, 5:42:05 PM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>arXiv:2310.11501 [cs]</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.48550/arXiv.2310.11501">10.48550/arXiv.2310.11501</a></td>
					</tr>
					<tr>
					<th>Repository</th>
						<td>arXiv</td>
					</tr>
					<tr>
					<th>Archive ID</th>
						<td>arXiv:2310.11501</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>1/7/2025, 5:42:05 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Computer Science - Artificial Intelligence</li>
					<li>Computer Science - Computation and Language</li>
					<li>Computer Science - Computers and Society</li>
				</ul>
				<h3 class="notes">Notes:</h3>
				<ul class="notes">
					<li id="item_YSUQC9QU">
<p class="plaintext">Comment: To appear at EMNLP 2023 (Main)</p>
					</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_RQEMW7LQ">Preprint PDF					</li>
					<li id="item_IJSB5C3B">Snapshot					</li>
				</ul>
			</li>


			<li id="item_28Q7JPWF" class="item journalArticle">
			<h2>Compression ensembles quantify aesthetic complexity and the evolution of visual art</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Andres Karjus</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Mar Canet Solà</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Tillmann Ohm</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Sebastian E. Ahnert</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Maximilian Schich</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>To the human eye, different images appear more or less 
complex, but capturing this intuition in a single aesthetic measure is 
considered hard. Here, we propose a computationally simple, transparent 
method for modeling aesthetic complexity as a multidimensional 
algorithmic phenomenon, which enables the systematic analysis of large 
image datasets. The approach captures visual family resemblance via a 
multitude of image transformations and subsequent compressions, yielding
 explainable embeddings. It aligns well with human judgments of visual 
complexity, and performs well in authorship and style recognition tasks.
 Showcasing the functionality, we apply the method to 125,000 artworks, 
recovering trends and revealing new insights regarding historical art, 
artistic careers over centuries, and emerging aesthetics in a 
contemporary NFT art market. Our approach, here applied to images but 
applicable more broadly, provides a new perspective to quantitative 
aesthetics, connoisseurship, multidimensional meaning spaces, and the 
study of cultural complexity.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2023/12</td>
					</tr>
					<tr>
					<th>Language</th>
						<td>en</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>epjdatascience.springeropen.com</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://epjdatascience.springeropen.com/articles/10.1140/epjds/s13688-023-00397-3">https://epjdatascience.springeropen.com/articles/10.1140/epjds/s13688-023-00397-3</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>1/3/2025, 12:41:33 PM</td>
					</tr>
					<tr>
					<th>Rights</th>
						<td>2023 The Author(s)</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>Number: 1
Publisher: SpringerOpen</td>
					</tr>
					<tr>
					<th>Volume</th>
						<td>12</td>
					</tr>
					<tr>
					<th>Pages</th>
						<td>1-23</td>
					</tr>
					<tr>
					<th>Publication</th>
						<td>EPJ Data Science</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.1140/epjds/s13688-023-00397-3">10.1140/epjds/s13688-023-00397-3</a></td>
					</tr>
					<tr>
					<th>Issue</th>
						<td>1</td>
					</tr>
					<tr>
					<th>Journal Abbr</th>
						<td>EPJ Data Sci.</td>
					</tr>
					<tr>
					<th>ISSN</th>
						<td>2193-1127</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>1/3/2025, 12:41:33 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_4LN2AHUW">Full Text PDF					</li>
				</ul>
			</li>


			<li id="item_7KGXLZ47" class="item journalArticle">
			<h2>Computation and the Sociological Imagination</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>James Evans</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Jacob G. Foster</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Computational sociology leverages new tools and data sources 
to expand the scope and scale of sociological inquiry. It’s opening up 
an exciting frontier for sociologists of every stripe—from theorists and
 ethnographers to experimentalists and survey researchers. It expands 
the sociological imagination.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2019-11-01</td>
					</tr>
					<tr>
					<th>Language</th>
						<td>en</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>SAGE Journals</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://doi.org/10.1177/1536504219883850">https://doi.org/10.1177/1536504219883850</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>9/24/2024, 2:28:44 PM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>Publisher: SAGE Publications</td>
					</tr>
					<tr>
					<th>Volume</th>
						<td>18</td>
					</tr>
					<tr>
					<th>Pages</th>
						<td>10-15</td>
					</tr>
					<tr>
					<th>Publication</th>
						<td>Contexts</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.1177/1536504219883850">10.1177/1536504219883850</a></td>
					</tr>
					<tr>
					<th>Issue</th>
						<td>4</td>
					</tr>
					<tr>
					<th>ISSN</th>
						<td>1536-5042</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>9/24/2024, 2:28:44 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>9/24/2024, 2:28:44 PM</td>
					</tr>
				</tbody></table>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_7CJQYI4T">SAGE PDF Full Text					</li>
				</ul>
			</li>


			<li id="item_ERVP8MSC" class="item journalArticle">
			<h2>Computational challenges to test and revitalize Claude Lévi-Strauss transformational methodology</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Albert Doja</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Laurent Capocchi</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Jean-François Santucci</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>The ambition and proposal for data modeling of myths presented
 in this paper is to link contemporary technical affordances to some 
canonical projects developed in structural anthropology. To articulate 
the theoretical promise and innovation of this proposal, we present a 
discrete-event system specification modeling and simulation approach in 
order to perform a generative analysis and a dynamic visualization of 
selected narratives, aimed at validating and revitalizing the 
transformational and morphodynamic theory and methodology proposed by 
Claude Lévi-Strauss in his structural analysis of myth. After an 
analysis of Lévi-Strauss’s transformational methodology, we describe in 
detail how discrete-event system specification models are implemented 
and developed in the framework of a DEVSimPy software environment. The 
validation of the method involves a discrete-event system specification 
simulation based on the extension of discrete-event system specification
 models dedicated to provide a dynamic Google Earth visualization of the
 selected myth. Future work around the discrete-event system 
specification formalism in anthropology is described as well as future 
applications regarding the impact of computational models 
(discrete-event system specification formalism, Bayesian inferences, and
 object-oriented features) to new contemporary anthropological domains.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2021-07-01</td>
					</tr>
					<tr>
					<th>Language</th>
						<td>en</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>SAGE Journals</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://doi.org/10.1177/20539517211037862">https://doi.org/10.1177/20539517211037862</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>12/23/2024, 6:02:50 PM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>Publisher: SAGE Publications Ltd</td>
					</tr>
					<tr>
					<th>Volume</th>
						<td>8</td>
					</tr>
					<tr>
					<th>Pages</th>
						<td>20539517211037862</td>
					</tr>
					<tr>
					<th>Publication</th>
						<td>Big Data &amp; Society</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.1177/20539517211037862">10.1177/20539517211037862</a></td>
					</tr>
					<tr>
					<th>Issue</th>
						<td>2</td>
					</tr>
					<tr>
					<th>ISSN</th>
						<td>2053-9517</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>12/23/2024, 6:02:50 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_3CBA54TM">SAGE PDF Full Text					</li>
				</ul>
			</li>


			<li id="item_H9UMTYM5" class="item journalArticle">
			<h2>Computational ethnography:  A view from sociology</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Phillip Brooker</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>This commentary elaborates on the ideas and projects outlined 
in this special issue, from a specifically sociological perspective. 
Much recent work in sociology proposes ‘methods mashups’ of ethnography 
and digital data/computational tools in different and diverse ways. 
However, typically, these have taken the form of applying (with or 
without tweaks) the principles of ethnography to new domains and data 
types, as if ethnography itself is stable and immutable; that it has a 
universal set of methodological principles that unify ethnographic 
practice. Returning to anthropology (whence, arguably, ethnography 
originally came) is, therefore, a useful way to extend our 
methodological thinking to (re)consider what ethnography is and how it 
operates, and from there think more clearly about how it may be 
effectively combined with digital data/computational tools in an 
emerging ‘Computational Anthropology’.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2022-01-01</td>
					</tr>
					<tr>
					<th>Language</th>
						<td>en</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>Computational ethnography</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>SAGE Journals</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://doi.org/10.1177/20539517211069892">https://doi.org/10.1177/20539517211069892</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>12/23/2024, 4:13:05 PM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>Publisher: SAGE Publications Ltd</td>
					</tr>
					<tr>
					<th>Volume</th>
						<td>9</td>
					</tr>
					<tr>
					<th>Pages</th>
						<td>20539517211069892</td>
					</tr>
					<tr>
					<th>Publication</th>
						<td>Big Data &amp; Society</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.1177/20539517211069892">10.1177/20539517211069892</a></td>
					</tr>
					<tr>
					<th>Issue</th>
						<td>1</td>
					</tr>
					<tr>
					<th>ISSN</th>
						<td>2053-9517</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>12/23/2024, 4:13:05 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_5D7WIT3W">SAGE PDF Full Text					</li>
				</ul>
			</li>


			<li id="item_8H5CTL5Z" class="item journalArticle">
			<h2>Computational grounded theory revisited: From computer-led to computer-assisted text analysis</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Hjalmar Bang Carlsen</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Snorre Ralund</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>The size and variation in both meaning-making and populations 
that characterize much contemporary text data demand research processes 
that support both discovery, interpretation and measurement. We assess 
one dominant strategy within the social sciences that takes a 
computer-led approach to text analysis. The approach is coined 
computational grounded theory. This strategy, we argue, relies on a set 
of unwarranted assumptions, namely, that unsupervised models return 
natural clusters of meaning, that the researcher can understand text 
with limited immersion and that indirect validation is sufficient for 
ensuring unbiased and precise measurement. In response to this 
criticism, we develop a framework that is computer assisted. We argue 
that our reformulation of computational grounded theory better aligns 
with the principles within grounded theory, anthropological theory 
generation and ethnography.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2022-01-01</td>
					</tr>
					<tr>
					<th>Language</th>
						<td>en</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>Computational grounded theory revisited</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>SAGE Journals</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://doi.org/10.1177/20539517221080146">https://doi.org/10.1177/20539517221080146</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>12/23/2024, 4:10:49 PM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>Publisher: SAGE Publications Ltd</td>
					</tr>
					<tr>
					<th>Volume</th>
						<td>9</td>
					</tr>
					<tr>
					<th>Pages</th>
						<td>20539517221080146</td>
					</tr>
					<tr>
					<th>Publication</th>
						<td>Big Data &amp; Society</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.1177/20539517221080146">10.1177/20539517221080146</a></td>
					</tr>
					<tr>
					<th>Issue</th>
						<td>1</td>
					</tr>
					<tr>
					<th>ISSN</th>
						<td>2053-9517</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>12/23/2024, 4:10:49 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_MQS57WLW">SAGE PDF Full Text					</li>
				</ul>
			</li>


			<li id="item_U6X5MFTF" class="item journalArticle">
			<h2>Computational Grounded Theory: A Methodological Framework</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Laura K. Nelson</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>This article proposes a three-step methodological framework 
called computational grounded theory, which combines expert human 
knowledge and hermeneutic skills with the processing power and pattern 
recognition of computers, producing a more methodologically rigorous but
 interpretive approach to content analysis. The first, pattern detection
 step, involves inductive computational exploration of text, using 
techniques such as unsupervised machine learning and word scores to help
 researchers to see novel patterns in their data. The second, pattern 
refinement step, returns to an interpretive engagement with the data 
through qualitative deep reading or further exploration of the data. The
 third, pattern confirmation step, assesses the inductively identified 
patterns using further computational and natural language processing 
techniques. The result is an efficient, rigorous, and fully reproducible
 computational grounded theory. This framework can be applied to any 
qualitative text as data, including transcribed speeches, interviews, 
open-ended survey data, or ethnographic field notes, and can address 
many potential research questions.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>02/2020</td>
					</tr>
					<tr>
					<th>Language</th>
						<td>en</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>Computational Grounded Theory</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>DOI.org (Crossref)</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://journals.sagepub.com/doi/10.1177/0049124117729703">http://journals.sagepub.com/doi/10.1177/0049124117729703</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>4/22/2024, 11:06:17 AM</td>
					</tr>
					<tr>
					<th>Volume</th>
						<td>49</td>
					</tr>
					<tr>
					<th>Pages</th>
						<td>3-42</td>
					</tr>
					<tr>
					<th>Publication</th>
						<td>Sociological Methods &amp; Research</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.1177/0049124117729703">10.1177/0049124117729703</a></td>
					</tr>
					<tr>
					<th>Issue</th>
						<td>1</td>
					</tr>
					<tr>
					<th>Journal Abbr</th>
						<td>Sociological Methods &amp; Research</td>
					</tr>
					<tr>
					<th>ISSN</th>
						<td>0049-1241, 1552-8294</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>4/22/2024, 11:06:17 AM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="notes">Notes:</h3>
				<ul class="notes">
					<li id="item_HXVTXY8S">
<p class="plaintext"></p>
					</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_LY3XGDW4">Nelson - 2020 - Computational Grounded Theory A Methodological Fr.pdf					</li>
				</ul>
			</li>


			<li id="item_DDU5JM6Y" class="item journalArticle">
			<h2>Computational Social Science Methodology, Anyone?</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Joop J. Hox</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>. This article reviews computational social science methods 		
			and their relation to conventional methodology and statistics. 
Computational 					social science has three important features. Firstly,
 it often involves big 					data; data sets so large that conventional 
database and analysis techniques 					cannot handle them with ease. 
Secondly, dealing with these big data sets has 					given rise to 
analysis techniques that are specially developed for big data. 					
Given the size of the data, resampling and cross-validation approaches 
become 					feasible that allow both data-driven exploration and checks 
on overfitting the 					data. A third important feature is simulation, 
especially agent-based 					simulation. Here size also matters. 
Agent-based simulation is well known in 					social science, but modern 
computer equipment and software allows simulations of 					unprecedented
 scale. Many of these techniques, especially the resampling and 					
cross-validation approaches, are potentially very useful for social 
scientists. 					Given the relatively small size of social science “big 
data” is 					useful to explore how well these techniques perform with 
smaller data sets. 					Social science methodology can contribute to 
this field by exploring if 					well-known methodological distinctions 
between external validity, internal 					validity, and construct 
validity can help clear up discussions on data quality 					(veracity) 
in computational social science.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2017-06</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>econtent.hogrefe.com (Atypon)</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://econtent.hogrefe.com/doi/10.1027/1614-2241/a000127">https://econtent.hogrefe.com/doi/10.1027/1614-2241/a000127</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>12/23/2024, 3:51:13 PM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>Publisher: Hogrefe Publishing</td>
					</tr>
					<tr>
					<th>Volume</th>
						<td>13</td>
					</tr>
					<tr>
					<th>Pages</th>
						<td>3-12</td>
					</tr>
					<tr>
					<th>Publication</th>
						<td>Methodology</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.1027/1614-2241/a000127">10.1027/1614-2241/a000127</a></td>
					</tr>
					<tr>
					<th>Issue</th>
						<td>Supplement 1</td>
					</tr>
					<tr>
					<th>ISSN</th>
						<td>1614-1881</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>12/23/2024, 3:51:13 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>analytics</li>
					<li>Big Data</li>
					<li>computational social science</li>
					<li>data science</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_UVNCPHK7">Full Text PDF					</li>
				</ul>
			</li>


			<li id="item_2Z79A6GU" class="item webpage">
			<h2>Concept-Guided Chain-of-Thought Prompting for Pairwise Comparison Scaling of Texts with Large Language Models</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Web Page</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Existing text scaling methods often require a large corpus, 
struggle with short texts, or require labeled data. We develop a text 
scaling method that leverages the pattern recognition capabilities of 
generative large language models (LLMs). Specifically, we propose 
concept-guided chain-of-thought (CGCoT), which uses prompts designed to 
summarize ideas and identify target parties in texts to generate 
concept-specific breakdowns, in many ways similar to guidance for human 
coder content analysis. CGCoT effectively shifts pairwise text 
comparisons from a reasoning problem to a pattern recognition problem. 
We then pairwise compare concept-specific breakdowns using an LLM. We 
use the results of these pairwise comparisons to estimate a scale using 
the Bradley-Terry model. We use this approach to scale affective speech 
on Twitter. Our measures correlate more strongly with human judgments 
than alternative approaches like Wordfish. Besides a small set of pilot 
data to develop the CGCoT prompts, our measures require no additional 
labeled data and produce binary predictions comparable to a 
RoBERTa-Large model fine-tuned on thousands of human-labeled tweets. We 
demonstrate how combining substantive knowledge with LLMs can create 
state-of-the-art measures of abstract concepts.</td>
					</tr>
					<tr>
					<th>Language</th>
						<td>en</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://csmapnyu.org/research/academic-research/concept-guided-chain-of-thought-prompting-for-pairwise-comparison-scaling-of-texts-with-large-language-models">https://csmapnyu.org/research/academic-research/concept-guided-chain-of-thought-prompting-for-pairwise-comparison-scaling-of-texts-with-large-language-models</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>1/8/2025, 2:22:06 PM</td>
					</tr>
					<tr>
					<th>Website Title</th>
						<td>NYU’s Center for Social Media and Politics</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>1/8/2025, 2:22:06 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_5A6AJ2DE">Snapshot					</li>
				</ul>
			</li>


			<li id="item_CJV3TGH2" class="item journalArticle">
			<h2>Conceptualizing and Examining Change in Communication Research</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Miriam Brinberg</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>David M. Lydon-Staley</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Communication research often focuses on processes of 
communication, such as how messages impact individuals over time or how 
interpersonal relationships develop and change. Despite their 
importance, these change processes are often implicit in much 
theoretical and empirical work in communication. Intensive longitudinal 
data are becoming increasingly feasible to collect and, when coupled 
with appropriate analytic frameworks, enable researchers to better 
explore and articulate the types of changes underlying communication 
processes. To facilitate the study of change processes, we (a) describe 
advances in data collection and analytic methods that allow researchers 
to articulate complex change processes of phenomena in communication 
research, (b) provide an overview of change processes and how they may 
be captured with intensive longitudinal methods, and (c) discuss 
considerations of capturing change when designing and implementing 
studies. We are excited about the future of studying processes of change
 in communication research, and we look forward to iterations between 
empirical tests and theory revision that will occur as researchers delve
 into studying change.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2023-01-02</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>Taylor and Francis+NEJM</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://doi.org/10.1080/19312458.2023.2167197">https://doi.org/10.1080/19312458.2023.2167197</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>1/6/2025, 1:59:32 PM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>Publisher: Routledge
_eprint: https://doi.org/10.1080/19312458.2023.2167197</td>
					</tr>
					<tr>
					<th>Volume</th>
						<td>17</td>
					</tr>
					<tr>
					<th>Pages</th>
						<td>59-82</td>
					</tr>
					<tr>
					<th>Publication</th>
						<td>Communication Methods and Measures</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.1080/19312458.2023.2167197">10.1080/19312458.2023.2167197</a></td>
					</tr>
					<tr>
					<th>Issue</th>
						<td>1</td>
					</tr>
					<tr>
					<th>ISSN</th>
						<td>1931-2458</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>1/6/2025, 1:59:32 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_QHUGZQQS">Full Text PDF					</li>
				</ul>
			</li>


			<li id="item_D6JAHFRC" class="item preprint">
			<h2>“Conversing” with Qualitative Data: Enhancing Qualitative Research through Large Language Models (LLMs)</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Preprint</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Adam Hayes</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>In this paper, I explore the transformative potential of Large
 Language Models (LLMs) such as ChatGPT in the realm of qualitative 
research, particularly in the social sciences. These generative AI 
models, trained on extensive textual data, have the unique ability to 
"understand," generate, and manipulate human-like text, offering 
unprecedented opportunities for data analysis and interpretation. I 
argue that LLMs, with this capacity, can significantly enhance the depth
 and efficiency of qualitative analysis. They can quickly identify 
patterns, themes, and sentiments in the data, providing a level of 
nuance that can be challenging to achieve with manual coding. 
Furthermore, their ability to generate human-like text can be used to 
simulate social interactions, create engaging presentations of research 
findings, and even "converse" with the data in a natural and flexible 
way. Indeed a central contribution of this paper lies in exploring this 
novel concept of "asking questions of" or "conversing with" text-based 
data, which opens up new avenues for qualitative research and analysis. 
This interactive capability of LLMs provides a transformative approach 
to topic coding and content analysis, allowing researchers to pose 
complex, nuanced questions to their data and receive responses in 
natural language. Ethical considerations and limitations are also 
discussed.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2023-06-15</td>
					</tr>
					<tr>
					<th>Language</th>
						<td>en-us</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>“Conversing” with Qualitative Data</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>OSF Preprints</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://osf.io/yms8p">https://osf.io/yms8p</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>1/3/2025, 4:05:35 PM</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.31235/osf.io/yms8p">10.31235/osf.io/yms8p</a></td>
					</tr>
					<tr>
					<th>Repository</th>
						<td>OSF</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>1/3/2025, 4:05:35 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>ChatGPT</li>
					<li>Computational Social Science</li>
					<li>Large Language Models</li>
					<li>LLMs</li>
					<li>qualitative analysis</li>
					<li>Social Science</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_8PR5SVQT">OSF Preprint					</li>
				</ul>
			</li>


			<li id="item_6WLSIEFM" class="item journalArticle">
			<h2>Covering the Campaign: Computational Tools for Measuring 
Differences in Candidate and Party News Coverage With Application to an 
Emerging Democracy</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Aaron Erlich</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Danielle F. Jung</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>James D. Long</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>How does media coverage of electoral campaigns distinguish 
parties and candidates in emerging democracies? To answer, we present a 
multi-step procedure that we apply in South Africa. First, we develop a 
theoretically informed classification of election coverage as either 
“narrow” or “broad” from within the entire corpus of news coverage 
during an electoral campaign. Second, to deploy our classification 
scheme, we use a supervised machine learning approach to classify news 
as “broad,” “narrow,” or “not election-related.” Finally, we combine our
 supervised classification with a topic modeling algorithm (BERTTopic) 
that is based on Bidirectional Encoder Representations from Transformers
 (BERT), in addition to other statistical and machine learning methods. 
The combination of our classification scheme, BERTTopic, and associated 
methods allows us to identify the main election-related themes among 
broad and narrow election-related coverage, and how different candidates
 and parties are associated with these themes. We provide an in-depth 
discussion of our method for interested users in the social sciences. We
 then apply our proposed techniques on text from nearly 100,000 news 
articles during South Africa’s 2014 campaign and test our empirical 
predictions about candidate and party coverage of corruption, the 
economy, health, public infrastructure, and security. The application of
 our method highlights a nuanced campaign environment in South Africa; 
candidates and parties frequently receive distinct and substantive 
coverage on key campaign themes.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2024-12-01</td>
					</tr>
					<tr>
					<th>Language</th>
						<td>en</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>Covering the Campaign</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>SAGE Journals</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://doi.org/10.1177/08944393241247420">https://doi.org/10.1177/08944393241247420</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>12/24/2024, 5:44:38 PM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>Publisher: SAGE Publications Inc</td>
					</tr>
					<tr>
					<th>Volume</th>
						<td>42</td>
					</tr>
					<tr>
					<th>Pages</th>
						<td>1313-1337</td>
					</tr>
					<tr>
					<th>Publication</th>
						<td>Social Science Computer Review</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.1177/08944393241247420">10.1177/08944393241247420</a></td>
					</tr>
					<tr>
					<th>Issue</th>
						<td>6</td>
					</tr>
					<tr>
					<th>ISSN</th>
						<td>0894-4393</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>12/24/2024, 5:44:38 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_XLTNBTHL">SAGE PDF Full Text					</li>
				</ul>
			</li>


			<li id="item_IHHAJQD6" class="item preprint">
			<h2>Decolonial AI Alignment: Openness, Viśe\d{s}a-Dharma, and Including Excluded Knowledges</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Preprint</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Kush R. Varshney</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Prior work has explicated the coloniality of artificial 
intelligence (AI) development and deployment through mechanisms such as 
extractivism, automation, sociological essentialism, surveillance, and 
containment. However, that work has not engaged much with alignment: 
teaching behaviors to a large language model (LLM) in line with desired 
values, and has not considered a mechanism that arises within that 
process: moral absolutism -- a part of the coloniality of knowledge. 
Colonialism has a history of altering the beliefs and values of 
colonized peoples; in this paper, I argue that this history is 
recapitulated in current LLM alignment practices and technologies. 
Furthermore, I suggest that AI alignment be decolonialized using three 
forms of openness: openness of models, openness to society, and openness
 to excluded knowledges. This suggested approach to decolonial AI 
alignment uses ideas from the argumentative moral philosophical 
tradition of Hinduism, which has been described as an open-source 
religion. One concept used is vi\'{s}e\d{s}a-dharma, or particular 
context-specific notions of right and wrong. At the end of the paper, I 
provide a suggested reference architecture to work toward the proposed 
framework.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2024-05-02</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>Decolonial AI Alignment</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>arXiv.org</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://arxiv.org/abs/2309.05030">http://arxiv.org/abs/2309.05030</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>1/7/2025, 4:54:53 PM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>arXiv:2309.05030 [cs]</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.48550/arXiv.2309.05030">10.48550/arXiv.2309.05030</a></td>
					</tr>
					<tr>
					<th>Repository</th>
						<td>arXiv</td>
					</tr>
					<tr>
					<th>Archive ID</th>
						<td>arXiv:2309.05030</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>1/7/2025, 4:54:53 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Computer Science - Artificial Intelligence</li>
					<li>Computer Science - Computers and Society</li>
					<li>Statistics - Machine Learning</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_BYHKRD5W">Preprint PDF					</li>
					<li id="item_CBCHED7G">Snapshot					</li>
				</ul>
			</li>


			<li id="item_5QIX38J7" class="item preprint">
			<h2>Deep Learning Without Neural Networks: Fractal-nets for Rare Event Modeling</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Preprint</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Ishanu Chattopadhyay</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Yi Huang</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>James Evans</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Complex  phenomena of societal interest such as weather, 
seismic activity and urban crime, are often punctuated  by rare and 
extreme events, which are difficult to model and predict. Evidence of 
long-range persistence of such events has underscored the need to learn 
deep stochastic structures in data for effective  forecasts. Recently 
neural networks (NN) have emerged as a defacto standard for deep 
learning. However, key  problems remain with NN inference, including a 
high sample complexity, a general lack of transparency, and  a limited 
ability to directly model stochastic phenomena. In this study we suggest
 that deep learning and the NN paradigm are conceptually distinct -- and
 that it is possible to learn ``deep&amp;#039; associations without 
invoking the ubiquitous NN strategy of global optimization via 
back-propagation. We show that deep learning of stochastic phenomena is 
related to uncovering the emergent self-similarities in data, which 
avoids the NN pitfalls offering  crucial insights into underlying 
mechanisms. Using the Fractal Net (FN) architecture introduced here, we 
actionably forecast various categories of rare weather and seismic 
events, and property and violent crimes in major US cities. Compared to 
 carefully tuned NNs, we boost recall at 90% precision by 161.9% for 
extreme weather events, 191.3% for light-to-severe seismic events with 
magnitudes above the local third quartile, and 50.8% - 404.9% for urban 
crime, demonstrating applicability in  diverse systems of societal 
interest. This study opens the door to precise prediction of rare events
 in spatio-temporal phenomena, adding a new tool to the data science 
revolution.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2020-10-26</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>Deep Learning Without Neural Networks</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>Research Square</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://www.researchsquare.com/article/rs-86045/v1">https://www.researchsquare.com/article/rs-86045/v1</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>12/24/2024, 5:22:55 PM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>ISSN: 2693-5015</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.21203/rs.3.rs-86045/v1">10.21203/rs.3.rs-86045/v1</a></td>
					</tr>
					<tr>
					<th>Repository</th>
						<td>Research Square</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>12/24/2024, 5:22:55 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_PDPVDJIQ">Full Text PDF					</li>
				</ul>
			</li>


			<li id="item_C544A8LL" class="item preprint">
			<h2>Designing LLM-Agents with Personalities: A Psychometric Approach</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Preprint</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Muhua Huang</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Xijuan Zhang</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Christopher Soto</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>James Evans</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>This research introduces a novel methodology for assigning 
quantifiable, controllable and psychometrically validated personalities 
to Large Language Models-Based Agents (Agents) using the Big Five 
personality framework. It seeks to overcome the constraints of human 
subject studies, proposing Agents as an accessible tool for social 
science inquiry. Through a series of four studies, this research 
demonstrates the feasibility of assigning psychometrically valid 
personality traits to Agents, enabling them to replicate complex 
human-like behaviors. The first study establishes an understanding of 
personality constructs and personality tests within the semantic space 
of an LLM. Two subsequent studies -- using empirical and simulated data 
-- illustrate the process of creating Agents and validate the results by
 showing strong correspondence between human and Agent answers to 
personality tests. The final study further corroborates this 
correspondence by using Agents to replicate known human correlations 
between personality traits and decision-making behaviors in scenarios 
involving risk-taking and ethical dilemmas, thereby validating the 
effectiveness of the psychometric approach to design Agents and its 
applicability to social and behavioral research.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2024-10-25</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>Designing LLM-Agents with Personalities</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>arXiv.org</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://arxiv.org/abs/2410.19238">http://arxiv.org/abs/2410.19238</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>12/24/2024, 4:59:07 PM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>arXiv:2410.19238 [cs]</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.48550/arXiv.2410.19238">10.48550/arXiv.2410.19238</a></td>
					</tr>
					<tr>
					<th>Repository</th>
						<td>arXiv</td>
					</tr>
					<tr>
					<th>Archive ID</th>
						<td>arXiv:2410.19238</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>12/24/2024, 4:59:07 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Computer Science - Artificial Intelligence</li>
					<li>Computer Science - Computers and Society</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_2GTQPYVY">Preprint PDF					</li>
					<li id="item_33J6H48S">Snapshot					</li>
				</ul>
			</li>


			<li id="item_LUUC5K3E" class="item conferencePaper">
			<h2>Detecting Social Media Manipulation in Low-Resource Languages</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Conference Paper</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Samar Haider</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Luca Luceri</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Ashok Deb</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Adam Badawy</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Nanyun Peng</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Emilio Ferrara</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Social media have been deliberately used for malicious 
purposes, including political manipulation and disinformation. Most 
research focuses on high-resource languages. However, malicious actors 
share content across countries and languages, including low-resource 
ones. Here, we investigate whether and to what extent malicious actors 
can be detected in low-resource language settings. We discovered that a 
high number of accounts posting in Tagalog were suspended as part of 
Twitter’s crackdown on interference operations after the 2016 US 
Presidential election. By combining text embedding and transfer 
learning, our framework can detect, with promising accuracy, malicious 
users posting in Tagalog without any prior knowledge or training on 
malicious content in that language. We first learn an embedding model 
for each language, namely a high-resource language (English) and a 
low-resource one (Tagalog), independently. Then, we learn a mapping 
between the two latent spaces to transfer the detection model. We 
demonstrate that the proposed approach significantly outperforms 
state-of-the-art models and yields marked advantages in settings with 
very limited training data—the norm when dealing with detecting 
malicious activity in online platforms.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2023-04-30</td>
					</tr>
					<tr>
					<th>Language</th>
						<td>en</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>DOI.org (Crossref)</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://dl.acm.org/doi/10.1145/3543873.3587615">https://dl.acm.org/doi/10.1145/3543873.3587615</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>1/8/2025, 2:16:07 PM</td>
					</tr>
					<tr>
					<th>Place</th>
						<td>Austin TX USA</td>
					</tr>
					<tr>
					<th>Publisher</th>
						<td>ACM</td>
					</tr>
					<tr>
					<th>ISBN</th>
						<td>978-1-4503-9419-2</td>
					</tr>
					<tr>
					<th>Pages</th>
						<td>1358-1364</td>
					</tr>
					<tr>
					<th>Proceedings Title</th>
						<td>Companion Proceedings of the ACM Web Conference 2023</td>
					</tr>
					<tr>
					<th>Conference Name</th>
						<td>WWW '23: The ACM Web Conference 2023</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.1145/3543873.3587615">10.1145/3543873.3587615</a></td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>1/8/2025, 2:16:07 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_UTWVFE7J">Available Version (via Google Scholar)					</li>
				</ul>
			</li>


			<li id="item_2CSFGEIE" class="item preprint">
			<h2>Detecting text level intellectual influence with knowledge graph embeddings</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Preprint</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Lucian Li</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Eryclis Silva</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Introduction: Tracing the spread of ideas and the presence of 
influence is a question of special importance across a wide range of 
disciplines, ranging from intellectual history to cultural analytics, 
computational social science, and the science of science. Method: We 
collect a corpus of open source journal articles, generate Knowledge 
Graph representations using the Gemini LLM, and attempt to predict the 
existence of citations between sampled pairs of articles using 
previously published methods and a novel Graph Neural Network based 
embedding model. Results: We demonstrate that our knowledge graph 
embedding method is superior at distinguishing pairs of articles with 
and without citation. Once trained, it runs efficiently and can be 
fine-tuned on specific corpora to suit individual researcher needs. 
Conclusion(s): This experiment demonstrates that the relationships 
encoded in a knowledge graph, especially the types of concepts brought 
together by specific relations can encode information capable of 
revealing intellectual influence. This suggests that further work in 
analyzing document level knowledge graphs to understand latent 
structures could provide valuable insights.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2024-10-31</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>arXiv.org</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://arxiv.org/abs/2410.24021">http://arxiv.org/abs/2410.24021</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>1/7/2025, 5:03:59 PM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>arXiv:2410.24021 [cs]</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.48550/arXiv.2410.24021">10.48550/arXiv.2410.24021</a></td>
					</tr>
					<tr>
					<th>Repository</th>
						<td>arXiv</td>
					</tr>
					<tr>
					<th>Archive ID</th>
						<td>arXiv:2410.24021</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>1/7/2025, 5:03:59 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Computer Science - Computation and Language</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_PRK7Y3QS">Preprint PDF					</li>
					<li id="item_R2CDJ2FC">Snapshot					</li>
				</ul>
			</li>


			<li id="item_TJAF5Q2S" class="item journalArticle">
			<h2>Developing a hierarchical model for unraveling conspiracy theories</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Mohsen Ghasemizade</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Jeremiah Onaolapo</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>A conspiracy theory (CT) suggests covert groups or powerful 
individuals secretly manipulate events. Not knowing about existing 
conspiracy theories could make one more likely to believe them, so this 
work aims to compile a list of CTs shaped as a tree that is as 
comprehensive as possible. We began with a manually curated ‘tree’ of 
CTs from academic papers and Wikipedia. Next, we examined 1769 
CT-related articles from four fact-checking websites, focusing on their 
core content, and used a technique called Keyphrase Extraction to label 
the documents. This process yielded 769 identified conspiracies, each 
assigned a label and a family name. The second goal of this project was 
to detect whether an article is a conspiracy theory, so we built a 
binary classifier with our labeled dataset. This model uses a 
transformer-based machine learning technique and is pre-trained on a 
large corpus called RoBERTa, resulting in an F1 score of 87%. This model
 helps to identify potential conspiracy theories in new articles. We 
used a combination of clustering (HDBSCAN) and a dimension reduction 
technique (UMAP) to assign a label from the tree to these new articles 
detected as conspiracy theories. We then labeled these groups 
accordingly to help us match them to the tree. These can lead us to 
detect new conspiracy theories and expand the tree using computational 
methods. We successfully generated a tree of conspiracy theories and 
built a pipeline to detect and categorize conspiracy theories within any
 text corpora. This pipeline gives us valuable insights through any 
databases formatted as text.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2024/12</td>
					</tr>
					<tr>
					<th>Language</th>
						<td>en</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>epjdatascience.springeropen.com</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://epjdatascience.springeropen.com/articles/10.1140/epjds/s13688-024-00470-5">https://epjdatascience.springeropen.com/articles/10.1140/epjds/s13688-024-00470-5</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>1/3/2025, 12:36:32 PM</td>
					</tr>
					<tr>
					<th>Rights</th>
						<td>2024 The Author(s)</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>Number: 1
Publisher: SpringerOpen</td>
					</tr>
					<tr>
					<th>Volume</th>
						<td>13</td>
					</tr>
					<tr>
					<th>Pages</th>
						<td>1-28</td>
					</tr>
					<tr>
					<th>Publication</th>
						<td>EPJ Data Science</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.1140/epjds/s13688-024-00470-5">10.1140/epjds/s13688-024-00470-5</a></td>
					</tr>
					<tr>
					<th>Issue</th>
						<td>1</td>
					</tr>
					<tr>
					<th>Journal Abbr</th>
						<td>EPJ Data Sci.</td>
					</tr>
					<tr>
					<th>ISSN</th>
						<td>2193-1127</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>1/3/2025, 12:36:32 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_DDQ3MVX5">Full Text PDF					</li>
				</ul>
			</li>


			<li id="item_C49ULRMW" class="item preprint">
			<h2>Developing an occupational prestige scale using Large Language Models</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Preprint</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Robert De Vries</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Mark Hill</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Laura Ruis</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Large Language Models (LLMs), being trained on large fractions
 of all online text, reflect societal biases and stereotypes – such as 
racial and gender biases. In this paper, we propose a method of using 
such models to capture societal perceptions of occupational prestige. We
 create four occupational prestige scales using this method, with each 
tapping a difference facet of prestige perceptions. These scales are 
validated against existing prestige scales based on human data. We 
conclude that it is possible to create valid measures of occupational 
prestige by prompting commercially available LLMs – though with some 
important limitations. Implications for future social stratification 
research are discussed.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2024-06-27</td>
					</tr>
					<tr>
					<th>Language</th>
						<td>en-us</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>OSF Preprints</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://osf.io/ct4vz">https://osf.io/ct4vz</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>1/3/2025, 4:17:24 PM</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.31235/osf.io/ct4vz">10.31235/osf.io/ct4vz</a></td>
					</tr>
					<tr>
					<th>Repository</th>
						<td>OSF</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>1/3/2025, 4:17:24 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>CAMSIS</li>
					<li>ChatGPT</li>
					<li>Large Language Models</li>
					<li>occupational prestige</li>
					<li>SIOPS</li>
					<li>Social status</li>
					<li>social stratification</li>
					<li>Weber</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_HLLCLA4Y">OSF Preprint					</li>
				</ul>
			</li>


			<li id="item_FCAQNBDQ" class="item preprint">
			<h2>Diminished Diversity-of-Thought in a Standard Large Language Model</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Preprint</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Peter S. Park</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Philipp Schoenegger</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Chongyang Zhu</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>We test whether Large Language Models (LLMs) can be used to 
simulate human participants in social-science studies. To do this, we 
run replications of 14 studies from the Many Labs 2 replication project 
with OpenAI's text-davinci-003 model, colloquially known as GPT3.5. 
Based on our pre-registered analyses, we find that among the eight 
studies we could analyse, our GPT sample replicated 37.5% of the 
original results and 37.5% of the Many Labs 2 results. However, we were 
unable to analyse the remaining six studies due to an unexpected 
phenomenon we call the "correct answer" effect. Different runs of GPT3.5
 answered nuanced questions probing political orientation, economic 
preference, judgement, and moral philosophy with zero or near-zero 
variation in responses: with the supposedly "correct answer." In one 
exploratory follow-up study, we found that a "correct answer" was robust
 to changing the demographic details that precede the prompt. In 
another, we found that most but not all "correct answers" were robust to
 changing the order of answer choices. One of our most striking findings
 occurred in our replication of the Moral Foundations Theory survey 
results, where we found GPT3.5 identifying as a political conservative 
in 99.6% of the cases, and as a liberal in 99.3% of the cases in the 
reverse-order condition. However, both self-reported 'GPT conservatives'
 and 'GPT liberals' showed right-leaning moral foundations. Our results 
cast doubts on the validity of using LLMs as a general replacement for 
human participants in the social sciences. Our results also raise 
concerns that a hypothetical AI-led future may be subject to a 
diminished diversity-of-thought.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2023-09-13</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>arXiv.org</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://arxiv.org/abs/2302.07267">http://arxiv.org/abs/2302.07267</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>1/7/2025, 5:40:54 PM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>arXiv:2302.07267 [cs]</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.48550/arXiv.2302.07267">10.48550/arXiv.2302.07267</a></td>
					</tr>
					<tr>
					<th>Repository</th>
						<td>arXiv</td>
					</tr>
					<tr>
					<th>Archive ID</th>
						<td>arXiv:2302.07267</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>1/7/2025, 5:40:54 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Computer Science - Artificial Intelligence</li>
					<li>Computer Science - Computation and Language</li>
					<li>Computer Science - Human-Computer Interaction</li>
				</ul>
				<h3 class="notes">Notes:</h3>
				<ul class="notes">
					<li id="item_LE4MDBYG">
<p class="plaintext">Comment: 67 pages (42-page main text, 25-page SI); 12 visualizations (four tables and three figures in the main text, five figures in the SI); additional exploratory follow-up study varied the demographic details preceding the prompt; preregistered OSF database is available at https://osf.io/dzp8t/</p>
					</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_66445MGQ">Preprint PDF					</li>
					<li id="item_E5BKHAW9">Snapshot					</li>
				</ul>
			</li>


			<li id="item_4RXRY8IV" class="item preprint">
			<h2>Do Large Language Models Discriminate in Hiring Decisions on the Basis of Race, Ethnicity, and Gender?</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Preprint</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Haozhe An</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Christabel Acquaye</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Colin Wang</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Zongxia Li</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Rachel Rudinger</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>We examine whether large language models (LLMs) exhibit race- 
and gender-based name discrimination in hiring decisions, similar to 
classic findings in the social sciences (Bertrand and Mullainathan, 
2004). We design a series of templatic prompts to LLMs to write an email
 to a named job applicant informing them of a hiring decision. By 
manipulating the applicant's first name, we measure the effect of 
perceived race, ethnicity, and gender on the probability that the LLM 
generates an acceptance or rejection email. We find that the hiring 
decisions of LLMs in many settings are more likely to favor White 
applicants over Hispanic applicants. In aggregate, the groups with the 
highest and lowest acceptance rates respectively are masculine White 
names and masculine Hispanic names. However, the comparative acceptance 
rates by group vary under different templatic settings, suggesting that 
LLMs' race- and gender-sensitivity may be idiosyncratic and 
prompt-sensitive.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2024-06-15</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>arXiv.org</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://arxiv.org/abs/2406.10486">http://arxiv.org/abs/2406.10486</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>1/7/2025, 5:28:59 PM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>arXiv:2406.10486 [cs]</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.48550/arXiv.2406.10486">10.48550/arXiv.2406.10486</a></td>
					</tr>
					<tr>
					<th>Repository</th>
						<td>arXiv</td>
					</tr>
					<tr>
					<th>Archive ID</th>
						<td>arXiv:2406.10486</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>1/7/2025, 5:28:59 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Computer Science - Computation and Language</li>
				</ul>
				<h3 class="notes">Notes:</h3>
				<ul class="notes">
					<li id="item_28UHDY9M">
<p class="plaintext">Comment: ACL 2024</p>
					</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_9MS23KC5">Preprint PDF					</li>
					<li id="item_5VAX3V4P">Snapshot					</li>
				</ul>
			</li>


			<li id="item_X8LW96DS" class="item conferencePaper">
			<h2>Does Active Learning Reduce Human Coding?: A Systematic Comparison of Neural Network with nCoder</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Conference Paper</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Jaeyoon Choi</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Andrew R. Ruis</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Zhiqiang Cai</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Brendan Eagan</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>David Williamson Shaffer</td>
					</tr>
					<tr>
						<th class="editor">Editor</th>
						<td>Crina Damşa</td>
					</tr>
					<tr>
						<th class="editor">Editor</th>
						<td>Amanda Barany</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>In quantitative ethnography (QE) studies which often involve 
large datasets that cannot be entirely hand-coded by human raters, 
researchers have used supervised machine learning approaches to develop 
automated classifiers. However, QE researchers are rightly concerned 
with the amount of human coding that may be required to develop 
classifiers that achieve the high levels of accuracy that QE studies 
typically require. In this study, we compare a neural network, a 
powerful traditional supervised learning approach, with nCoder, an 
active learning technique commonly used in QE studies, to determine 
which technique requires the least human coding to produce a 
sufficiently accurate classifier. To do this, we constructed multiple 
training sets from a large dataset used in prior QE studies and designed
 a Monte Carlo simulation to test the performance of the two techniques 
systematically. Our results show that nCoder can achieve high predictive
 accuracy with significantly less human-coded data than a neural 
network.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2023</td>
					</tr>
					<tr>
					<th>Language</th>
						<td>en</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>Does Active Learning Reduce Human Coding?</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>Springer Link</td>
					</tr>
					<tr>
					<th>Place</th>
						<td>Cham</td>
					</tr>
					<tr>
					<th>Publisher</th>
						<td>Springer Nature Switzerland</td>
					</tr>
					<tr>
					<th>ISBN</th>
						<td>978-3-031-31726-2</td>
					</tr>
					<tr>
					<th>Pages</th>
						<td>30-42</td>
					</tr>
					<tr>
					<th>Proceedings Title</th>
						<td>Advances in Quantitative Ethnography</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.1007/978-3-031-31726-2_3">10.1007/978-3-031-31726-2_3</a></td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>5/1/2024, 12:18:16 AM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>5/1/2024, 12:18:16 AM</td>
					</tr>
				</tbody></table>
			</li>


			<li id="item_LN7E9MEG" class="item journalArticle">
			<h2>Drivers of hate speech in political conversations on Twitter: the case of the 2022 Italian general election</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Francesco Pierri</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>We study the prevalence and characteristics of toxic speech on
 Twitter in the run-up to the 2022 Italian General election. We analyzed
 over 8.5 M tweets shared by 450k unique users and employed a machine 
learning classifier to estimate the toxicity score of their messages. We
 found that supporters of different political coalitions exhibit varying
 levels of toxic speech, sending between 6-8% of toxic messages overall.
 Notably, Centre-Left politicians received more toxic messages on 
average, with the largest target of hate receiving over 15,000 toxic 
replies. We employed Generalized Linear Models to study factors that 
drive hate speech to political targets, finding that, importantly, 
politicians employing more abusive and harmful language are also more 
likely to attract more inflammatory speech. Our findings underscore the 
critical need for targeted interventions to address hate speech online, 
fostering healthier dialogue and safeguarding democratic discourse.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2024/12</td>
					</tr>
					<tr>
					<th>Language</th>
						<td>en</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>Drivers of hate speech in political conversations on Twitter</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>epjdatascience.springeropen.com</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://epjdatascience.springeropen.com/articles/10.1140/epjds/s13688-024-00501-1">https://epjdatascience.springeropen.com/articles/10.1140/epjds/s13688-024-00501-1</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>1/3/2025, 12:40:09 PM</td>
					</tr>
					<tr>
					<th>Rights</th>
						<td>2024 The Author(s)</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>Number: 1
Publisher: SpringerOpen</td>
					</tr>
					<tr>
					<th>Volume</th>
						<td>13</td>
					</tr>
					<tr>
					<th>Pages</th>
						<td>1-19</td>
					</tr>
					<tr>
					<th>Publication</th>
						<td>EPJ Data Science</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.1140/epjds/s13688-024-00501-1">10.1140/epjds/s13688-024-00501-1</a></td>
					</tr>
					<tr>
					<th>Issue</th>
						<td>1</td>
					</tr>
					<tr>
					<th>Journal Abbr</th>
						<td>EPJ Data Sci.</td>
					</tr>
					<tr>
					<th>ISSN</th>
						<td>2193-1127</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>1/3/2025, 12:40:09 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_GKHGJWNH">Full Text PDF					</li>
				</ul>
			</li>


			<li id="item_AD7UPWUV" class="item preprint">
			<h2>Dynamic Sentiment Analysis with Local Large Language Models using
 Majority Voting: A Study on Factors Affecting Restaurant Evaluation</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Preprint</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Junichiro Niimi</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>User-generated contents (UGCs) on online platforms allow 
marketing researchers to understand consumer preferences for products 
and services. With the advance of large language models (LLMs), some 
studies utilized the models for annotation and sentiment analysis. 
However, the relationship between the accuracy and the hyper-parameters 
of LLMs is yet to be thoroughly examined. In addition, the issues of 
variability and reproducibility of results from each trial of LLMs have 
rarely been considered in existing literature. Since actual human 
annotation uses majority voting to resolve disagreements among 
annotators, this study introduces a majority voting mechanism to a 
sentiment analysis model using local LLMs. By a series of three analyses
 of online reviews on restaurant evaluations, we demonstrate that 
majority voting with multiple attempts using a medium-sized model 
produces more robust results than using a large model with a single 
attempt. Furthermore, we conducted further analysis to investigate the 
effect of each aspect on the overall evaluation.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2024-07-18</td>
					</tr>
					<tr>
					<th>Language</th>
						<td>en</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>Dynamic Sentiment Analysis with Local Large Language Models using Majority Voting</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>arXiv.org</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://arxiv.org/abs/2407.13069">http://arxiv.org/abs/2407.13069</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>1/8/2025, 3:12:28 PM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>arXiv:2407.13069 [cs]</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.48550/arXiv.2407.13069">10.48550/arXiv.2407.13069</a></td>
					</tr>
					<tr>
					<th>Repository</th>
						<td>arXiv</td>
					</tr>
					<tr>
					<th>Archive ID</th>
						<td>arXiv:2407.13069</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>1/8/2025, 3:12:28 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Computer Science - Computation and Language</li>
					<li>Computer Science - Information Retrieval</li>
				</ul>
				<h3 class="notes">Notes:</h3>
				<ul class="notes">
					<li id="item_8LSLG5KY">
<p class="plaintext">Comment: This manuscript is under peer review</p>
					</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_5DTCCHEB">Niimi - 2024 - Dynamic Sentiment Analysis with Local Large Langua.pdf					</li>
				</ul>
			</li>


			<li id="item_WG44BGA2" class="item journalArticle">
			<h2>Editorial introduction: Towards a machinic anthropology</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Morten Axel Pedersen</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Bringing together a motley crew of social scientists and data 
scientists, the aim of this special theme issue is to explore what an 
integration or even fusion between anthropology and data science might 
look like. Going beyond existing work on the complementarity between 
‘thick’ qualitative and ‘big’ quantitative data, the ambition is to 
unsettle and push established disciplinary, methodological and 
epistemological boundaries by creatively and critically probing various 
computational methods for augmenting and automatizing the collection, 
processing and analysis of ethnographic data, and vice versa. Can 
ethnographic and other qualitative data and methods be integrated with 
natural language processing tools and other machine-learning techniques,
 and if so, to what effect? Does the rise of data science allow for the 
realization of Levi-Strauss’ old dream of a computational structuralism,
 and even if so, should it? Might one even go as far as saying that 
computers are now becoming agents of social scientific analysis or even 
thinking: are we about to witness the birth of distinctly 
anthropological forms of artificial intelligence? By exploring these 
questions, the hope is not only to introduce scholars and students to 
computational anthropological methods, but also to disrupt predominant 
norms and assumptions among computational social scientists and data 
science writ large.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2023-01-01</td>
					</tr>
					<tr>
					<th>Language</th>
						<td>en</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>Editorial introduction</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>SAGE Journals</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://doi.org/10.1177/20539517231153803">https://doi.org/10.1177/20539517231153803</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>12/23/2024, 5:39:53 PM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>Publisher: SAGE Publications Ltd</td>
					</tr>
					<tr>
					<th>Volume</th>
						<td>10</td>
					</tr>
					<tr>
					<th>Pages</th>
						<td>20539517231153803</td>
					</tr>
					<tr>
					<th>Publication</th>
						<td>Big Data &amp; Society</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.1177/20539517231153803">10.1177/20539517231153803</a></td>
					</tr>
					<tr>
					<th>Issue</th>
						<td>1</td>
					</tr>
					<tr>
					<th>ISSN</th>
						<td>2053-9517</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>12/23/2024, 5:39:53 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_9UYQKGG6">SAGE PDF Full Text					</li>
				</ul>
			</li>


			<li id="item_K7BMUYRQ" class="item webpage">
			<h2>Effects of AI Feedback on Learning, the Skill Gap, and Intellectual Diversity</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Web Page</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Can human decision-makers learn from AI feedback? Using data 
on 52,000 decision-makers from a large online chess platform, we 
investigate how their AI use affects three interrelated long-term 
outcomes: Learning, skill gap, and diversity of decision strategies. 
First, we show that individuals are far more likely to seek AI feedback 
in situations in which they experienced success rather than failure. 
This AI feedback seeking strategy turns out to be detrimental to 
learning: Feedback on successes decreases future performance, while 
feedback on failures increases it. Second, higher-skilled 
decision-makers seek AI feedback more often and are far more likely to 
seek AI feedback after a failure, and benefit more from AI feedback than
 lower-skilled individuals. As a result, access to AI feedback 
increases, rather than decreases, the skill gap between high- and 
low-skilled individuals. Finally, we leverage 42 major platform updates 
as natural experiments to show that access to AI feedback causes a 
decrease in intellectual diversity of the population as individuals tend
 to specialize in the same areas. Together, those results indicate that 
learning from AI feedback is not automatic and using AI correctly seems 
to be a skill itself. Furthermore, despite its individual-level 
benefits, access to AI feedback can have significant population-level 
downsides including loss of intellectual diversity and an increasing 
skill gap.</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://www.networkscienceinstitute.org/publications/effects-of-ai-feedback-on-learning-the-skill-gap-and-intellectual-diversity">https://www.networkscienceinstitute.org/publications/effects-of-ai-feedback-on-learning-the-skill-gap-and-intellectual-diversity</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>1/8/2025, 1:18:55 PM</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>1/8/2025, 1:18:55 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_ZQRSXZIX">Snapshot					</li>
				</ul>
			</li>


			<li id="item_JUBG8MFG" class="item journalArticle">
			<h2>Embedding Democratic Values into Social Media AIs via Societal Objective Functions</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Chenyan Jia</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Michelle S. Lam</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Minh Chau Mai</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Jeffrey T. Hancock</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Michael S. Bernstein</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Mounting evidence indicates that the artificial intelligence 
(AI) systems that rank our social media feeds bear nontrivial 
responsibility for amplifying partisan animosity: negative thoughts, 
feelings, and behaviors toward political out-groups. Can we design these
 AIs to consider democratic values such as mitigating partisan animosity
 as part of their objective functions? We introduce a method for 
translating established, vetted social scientific constructs into AI 
objective functions, which we term societal objective functions, and 
demonstrate the method with application to the political science 
construct of anti-democratic attitudes. Traditionally, we have lacked 
observable outcomes to use to train such models-however, the social 
sciences have developed survey instruments and qualitative codebooks for
 these constructs, and their precision facilitates translation into 
detailed prompts for large language models. We apply this method to 
create a democratic attitude model that estimates the extent to which a 
social media post promotes anti-democratic attitudes, and test this 
democratic attitude model across three studies. In Study 1, we first 
test the attitudinal and behavioral effectiveness of the intervention 
among US partisans (N=1,380) by manually annotating (alpha=.895) social 
media posts with anti-democratic attitude scores and testing several 
feed ranking conditions based on these scores. Removal (d=.20) and 
downranking feeds (d=.25) reduced participants' partisan animosity 
without compromising their experience and engagement. In Study 2, we 
scale up the manual labels by creating the democratic attitude model, 
finding strong agreement with manual labels (rho=.75). Finally, in Study
 3, we replicate Study 1 using the democratic attitude model instead of 
manual labels to test its attitudinal and behavioral impact (N=558), and
 again find that the feed downranking using the societal objective 
function reduced partisan animosity (d=.25). This method presents a 
novel strategy to draw on social science theory and methods to mitigate 
societal harms in social media AIs.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2024-04-17</td>
					</tr>
					<tr>
					<th>Language</th>
						<td>en</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>DOI.org (Crossref)</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://dl.acm.org/doi/10.1145/3641002">https://dl.acm.org/doi/10.1145/3641002</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>5/6/2024, 12:24:07 PM</td>
					</tr>
					<tr>
					<th>Volume</th>
						<td>8</td>
					</tr>
					<tr>
					<th>Pages</th>
						<td>1-36</td>
					</tr>
					<tr>
					<th>Publication</th>
						<td>Proceedings of the ACM on Human-Computer Interaction</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.1145/3641002">10.1145/3641002</a></td>
					</tr>
					<tr>
					<th>Issue</th>
						<td>CSCW1</td>
					</tr>
					<tr>
					<th>Journal Abbr</th>
						<td>Proc. ACM Hum.-Comput. Interact.</td>
					</tr>
					<tr>
					<th>ISSN</th>
						<td>2573-0142</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>5/6/2024, 12:24:07 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>5/6/2024, 12:24:07 PM</td>
					</tr>
				</tbody></table>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_BFXWP3QG">Full Text PDF					</li>
				</ul>
			</li>


			<li id="item_N3486U9G" class="item book">
			<h2>Embedding Privacy in Computational Social Science and Artificial Intelligence Research</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Book</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Keenan Jones</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Fatima Zahrah</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Jason R. C. Nurse</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Privacy is a human right. It ensures that individuals are free
 to engage in discussions, participate in groups, and form relationships
 online or offline without fear of their data being inappropriately 
harvested, analyzed, or otherwise used to harm them. Preserving privacy 
has emerged as a critical factor in research, particularly in the 
computational social science (CSS), artificial intelligence (AI) and 
data science domains, given their reliance on individuals' data for 
novel insights. The increasing use of advanced computational models 
stands to exacerbate privacy concerns because, if inappropriately used, 
they can quickly infringe privacy rights and lead to adverse effects for
 individuals -- especially vulnerable groups -- and society. We have 
already witnessed a host of privacy issues emerge with the advent of 
large language models (LLMs), such as ChatGPT, which further demonstrate
 the importance of embedding privacy from the start. This article 
contributes to the field by discussing the role of privacy and the 
issues that researchers working in CSS, AI, data science and related 
domains are likely to face. It then presents several key considerations 
for researchers to ensure participant privacy is best preserved in their
 research design, data collection and use, analysis, and dissemination 
of research results.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>June 1, 2024</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>arXiv.org</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://arxiv.org/abs/2404.11515">http://arxiv.org/abs/2404.11515</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>1/7/2025, 5:29:32 PM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>DOI: 10.36190/2024.18
arXiv:2404.11515 [cs]</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>1/7/2025, 5:29:32 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Computer Science - Artificial Intelligence</li>
					<li>Computer Science - Computers and Society</li>
					<li>Computer Science - Emerging Technologies</li>
					<li>Computer Science - Human-Computer Interaction</li>
				</ul>
				<h3 class="notes">Notes:</h3>
				<ul class="notes">
					<li id="item_3CKIGAMB">
<p class="plaintext">Comment: International Association for the Advancement of Artificial Intelligence (AAAI) Conference on Web and Social Media (ICWSM) Workshops (Disrupt, Ally, Resist, Embrace (DARE) Workshop), 2024</p>
					</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_33TF5E8E">Preprint PDF					</li>
					<li id="item_GGCBJRBX">Snapshot					</li>
				</ul>
			</li>


			<li id="item_8UK3RJBJ" class="item journalArticle">
			<h2>Emergent local structures in an ecosystem of social bots and humans on Twitter</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Abdullah Alrhmoun</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>János Kertész</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Bots in online social networks can be used for good or bad but
 their presence is unavoidable and will increase in the future. To 
investigate how the interaction networks of bots and humans evolve, we 
created six social bots on Twitter with AI language models and let them 
carry out standard user operations. Three different strategies were 
implemented for the bots: a trend-targeting strategy (TTS), a 
keywords-targeting strategy (KTS) and a user-targeting strategy (UTS). 
We examined the interaction patterns such as targeting users, spreading 
messages, propagating relationships, and engagement. We focused on the 
emergent local structures or motifs and found that the strategies of the
 social bots had a significant impact on them. Motifs resulting from 
interactions with bots following TTS or KTS are simple and show 
significant overlap, while those resulting from interactions with 
UTS-governed bots lead to more complex motifs. These findings provide 
insights into human-bot interaction patterns in online social networks, 
and can be used to develop more effective bots for beneficial tasks and 
to combat malicious actors.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2023/12</td>
					</tr>
					<tr>
					<th>Language</th>
						<td>en</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>epjdatascience.springeropen.com</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://epjdatascience.springeropen.com/articles/10.1140/epjds/s13688-023-00406-5">https://epjdatascience.springeropen.com/articles/10.1140/epjds/s13688-023-00406-5</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>1/3/2025, 12:41:08 PM</td>
					</tr>
					<tr>
					<th>Rights</th>
						<td>2023 The Author(s)</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>Number: 1
Publisher: SpringerOpen</td>
					</tr>
					<tr>
					<th>Volume</th>
						<td>12</td>
					</tr>
					<tr>
					<th>Pages</th>
						<td>1-17</td>
					</tr>
					<tr>
					<th>Publication</th>
						<td>EPJ Data Science</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.1140/epjds/s13688-023-00406-5">10.1140/epjds/s13688-023-00406-5</a></td>
					</tr>
					<tr>
					<th>Issue</th>
						<td>1</td>
					</tr>
					<tr>
					<th>Journal Abbr</th>
						<td>EPJ Data Sci.</td>
					</tr>
					<tr>
					<th>ISSN</th>
						<td>2193-1127</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>1/3/2025, 12:41:08 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_N2JQCBPR">Full Text PDF					</li>
				</ul>
			</li>


			<li id="item_HDZZH3WS" class="item journalArticle">
			<h2>Employing large language models in survey research</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Bernard J. Jansen</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Soon-gyo Jung</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Joni Salminen</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>This article discusses the promising potential of employing 
large language models (LLMs) for survey research, including generating 
responses to survey items. LLMs can address some of the challenges 
associated with survey research regarding question-wording and response 
bias. They can address issues relating to a lack of clarity and 
understanding but cannot yet correct for sampling or nonresponse bias 
challenges. While LLMs can assist with some of the challenges with 
survey research, at present, LLMs need to be used in conjunction with 
other methods and approaches. With thoughtful and nuanced approaches to 
development, LLMs can be used responsibly and beneficially while 
minimizing the associated risks.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2023-09-01</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>ScienceDirect</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://www.sciencedirect.com/science/article/pii/S2949719123000171">https://www.sciencedirect.com/science/article/pii/S2949719123000171</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>1/8/2025, 3:22:58 PM</td>
					</tr>
					<tr>
					<th>Volume</th>
						<td>4</td>
					</tr>
					<tr>
					<th>Pages</th>
						<td>100020</td>
					</tr>
					<tr>
					<th>Publication</th>
						<td>Natural Language Processing Journal</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.1016/j.nlp.2023.100020">10.1016/j.nlp.2023.100020</a></td>
					</tr>
					<tr>
					<th>Journal Abbr</th>
						<td>Natural Language Processing Journal</td>
					</tr>
					<tr>
					<th>ISSN</th>
						<td>2949-7191</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>1/8/2025, 3:22:58 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Large language models</li>
					<li>LLM survey respondents</li>
					<li>Survey data</li>
					<li>Survey research</li>
					<li>Surveys</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_X4TR3BR8">Full Text					</li>
					<li id="item_7XJ62VXW">ScienceDirect Snapshot					</li>
				</ul>
			</li>


			<li id="item_VBXNW5A5" class="item journalArticle">
			<h2>Engaging citizens in experiments with computational analysis of 
patient stories: From unwarranted reductions to meaningful insights</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Nada Akrouh</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Rik Wehrens</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Hester van de Bovenkamp</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>In recent years, citizen engagement in policy and research has
 gained considerable momentum. In the healthcare domain, patient 
narratives, through various mediums, have emerged as a valuable source 
of insight into the experiences of patients and the healthcare system. 
Recognizing the value of such textual data, diverse analytical methods 
have been developed, spanning from text mining to narrative analysis. 
This article presents experiments that combine computational methods, 
qualitative methods and citizen science for analyzing patients’ stories.
 In this article, we reflect on two experiments in which we combined 
these approaches, which we analyze through a generative lens. We 
distinguish three main effects of the experiments: they provide a 
platform for discussions as a 'site of controversy'; they act as 
'mediator', fostering new connections and mutual understanding among 
participants; and they serve as 'tin opener', stimulating substantive 
discussions about methodological development and substantive healthcare 
matters. Narrative reduction, which occurs when rich narrative data is 
simplified into structured quantifiable forms, is not inherently 
problematic; instead, it can be meaningful when combined with 
qualitative methods and citizen science, emphasizing the importance of 
utilizing diverse methods to balance authenticity and gaining broader 
insights. The study highlights the significance of collaborative 
sense-making and meaning-making in interdisciplinary research. Engaging 
patients, their relatives, and professionals in the analytical process, 
facilitated by tools like word clouds, promotes engaged discussions that
 yield actionable insights. Further development of such 
interdisciplinary approaches holds promise for a more nuanced 
understanding of patient experiences, fostering epistemological 
pluralism, and refining healthcare practices.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2024-12-01</td>
					</tr>
					<tr>
					<th>Language</th>
						<td>en</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>Engaging citizens in experiments with computational analysis of patient stories</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>SAGE Journals</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://doi.org/10.1177/20539517241290218">https://doi.org/10.1177/20539517241290218</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>12/23/2024, 4:27:13 PM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>Publisher: SAGE Publications Ltd</td>
					</tr>
					<tr>
					<th>Volume</th>
						<td>11</td>
					</tr>
					<tr>
					<th>Pages</th>
						<td>20539517241290218</td>
					</tr>
					<tr>
					<th>Publication</th>
						<td>Big Data &amp; Society</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.1177/20539517241290218">10.1177/20539517241290218</a></td>
					</tr>
					<tr>
					<th>Issue</th>
						<td>4</td>
					</tr>
					<tr>
					<th>ISSN</th>
						<td>2053-9517</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>12/23/2024, 4:27:13 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_6CE89V7U">SAGE PDF Full Text					</li>
				</ul>
			</li>


			<li id="item_QB5FB5LS" class="item preprint">
			<h2>Enriching Datasets with Demographics through Large Language Models: What's in a Name?</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Preprint</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Khaled AlNuaimi</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Gautier Marti</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Mathieu Ravaut</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Abdulla AlKetbi</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Andreas Henschel</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Raed Jaradat</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Enriching datasets with demographic information, such as 
gender, race, and age from names, is a critical task in fields like 
healthcare, public policy, and social sciences. Such demographic 
insights allow for more precise and effective engagement with target 
populations. Despite previous efforts employing hidden Markov models and
 recurrent neural networks to predict demographics from names, 
significant limitations persist: the lack of large-scale, well-curated, 
unbiased, publicly available datasets, and the lack of an approach 
robust across datasets. This scarcity has hindered the development of 
traditional supervised learning approaches. In this paper, we 
demonstrate that the zero-shot capabilities of Large Language Models 
(LLMs) can perform as well as, if not better than, bespoke models 
trained on specialized data. We apply these LLMs to a variety of 
datasets, including a real-life, unlabelled dataset of licensed 
financial professionals in Hong Kong, and critically assess the inherent
 demographic biases in these models. Our work not only advances the 
state-of-the-art in demographic enrichment but also opens avenues for 
future research in mitigating biases in LLMs.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2024-09-17</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>Enriching Datasets with Demographics through Large Language Models</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>arXiv.org</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://arxiv.org/abs/2409.11491">http://arxiv.org/abs/2409.11491</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>1/7/2025, 5:07:13 PM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>arXiv:2409.11491 [cs]</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.48550/arXiv.2409.11491">10.48550/arXiv.2409.11491</a></td>
					</tr>
					<tr>
					<th>Repository</th>
						<td>arXiv</td>
					</tr>
					<tr>
					<th>Archive ID</th>
						<td>arXiv:2409.11491</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>1/7/2025, 5:07:13 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Computer Science - Computation and Language</li>
				</ul>
				<h3 class="notes">Notes:</h3>
				<ul class="notes">
					<li id="item_45VMZQBR">
<p class="plaintext">Comment: 8 pages, 7 Tables, 5 Figures</p>
					</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_YGPK6XGN">Preprint PDF					</li>
					<li id="item_Y7I5YNZV">Snapshot					</li>
				</ul>
			</li>


			<li id="item_N6QK34RF" class="item preprint">
			<h2>Ethical and social risks of harm from Language Models</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Preprint</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Laura Weidinger</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>John Mellor</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Maribeth Rauh</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Conor Griffin</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Jonathan Uesato</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Po-Sen Huang</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Myra Cheng</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Mia Glaese</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Borja Balle</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Atoosa Kasirzadeh</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Zac Kenton</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Sasha Brown</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Will Hawkins</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Tom Stepleton</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Courtney Biles</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Abeba Birhane</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Julia Haas</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Laura Rimell</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Lisa Anne Hendricks</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>William Isaac</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Sean Legassick</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Geoffrey Irving</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Iason Gabriel</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>This paper aims to help structure the risk landscape 
associated with large-scale Language Models (LMs). In order to foster 
advances in responsible innovation, an in-depth understanding of the 
potential risks posed by these models is needed. A wide range of 
established and anticipated risks are analysed in detail, drawing on 
multidisciplinary expertise and literature from computer science, 
linguistics, and social sciences. We outline six specific risk areas: I.
 Discrimination, Exclusion and Toxicity, II. Information Hazards, III. 
Misinformation Harms, V. Malicious Uses, V. Human-Computer Interaction 
Harms, VI. Automation, Access, and Environmental Harms. The first area 
concerns the perpetuation of stereotypes, unfair discrimination, 
exclusionary norms, toxic language, and lower performance by social 
group for LMs. The second focuses on risks from private data leaks or 
LMs correctly inferring sensitive information. The third addresses risks
 arising from poor, false or misleading information including in 
sensitive domains, and knock-on risks such as the erosion of trust in 
shared information. The fourth considers risks from actors who try to 
use LMs to cause harm. The fifth focuses on risks specific to LLMs used 
to underpin conversational agents that interact with human users, 
including unsafe use, manipulation or deception. The sixth discusses the
 risk of environmental harm, job automation, and other challenges that 
may have a disparate effect on different social groups or communities. 
In total, we review 21 risks in-depth. We discuss the points of origin 
of different risks and point to potential mitigation approaches. Lastly,
 we discuss organisational responsibilities in implementing mitigations,
 and the role of collaboration and participation. We highlight 
directions for further research, particularly on expanding the toolkit 
for assessing and evaluating the outlined risks in LMs.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2021-12-08</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>arXiv.org</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://arxiv.org/abs/2112.04359">http://arxiv.org/abs/2112.04359</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>1/7/2025, 5:37:55 PM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>arXiv:2112.04359 [cs]</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.48550/arXiv.2112.04359">10.48550/arXiv.2112.04359</a></td>
					</tr>
					<tr>
					<th>Repository</th>
						<td>arXiv</td>
					</tr>
					<tr>
					<th>Archive ID</th>
						<td>arXiv:2112.04359</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>1/7/2025, 5:37:55 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Computer Science - Artificial Intelligence</li>
					<li>Computer Science - Computation and Language</li>
					<li>Computer Science - Computers and Society</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_ECT6BVGR">Preprint PDF					</li>
					<li id="item_WHD9IRQM">Snapshot					</li>
				</ul>
			</li>


			<li id="item_2P4LAML6" class="item bookSection">
			<h2>Ethnography and Machine Learning: Synergies and New Directions</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Book Section</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Zhuofan Li</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Corey M. Abramson</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Ethnography (social scientific methods that illuminate how 
people understand, navigate and shape the real world contexts in which 
they live their lives) and machine learning (computational techniques 
that use big data and statistical learning models to perform 
quantifiable tasks) are each core to contemporary social science. Yet 
these tools have remained largely separate in practice. This chapter 
draws on a growing body of scholarship that argues that ethnography and 
machine learning can be usefully combined, particularly for large 
comparative studies. Specifically, this paper (a) explains the value 
(and challenges) of using machine learning alongside qualitative field 
research for certain types of projects, (b) discusses recent 
methodological trends to this effect, (c) provides examples that 
illustrate workflow drawn from several large projects, and (d) concludes
 with a roadmap for enabling productive coevolution of field methods and
 machine learning.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2023-12-18</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>Ethnography and Machine Learning</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>arXiv.org</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://arxiv.org/abs/2412.06087">http://arxiv.org/abs/2412.06087</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>1/7/2025, 4:39:02 PM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>DOI: 10.1093/oxfordhb/9780197653609.013.36
arXiv:2412.06087 [cs]</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>1/7/2025, 4:39:02 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Computer Science - Artificial Intelligence</li>
					<li>Computer Science - Computers and Society</li>
					<li>Computer Science - Machine Learning</li>
					<li>Statistics - Methodology</li>
				</ul>
				<h3 class="notes">Notes:</h3>
				<ul class="notes">
					<li id="item_SBMJMTXV">
<p class="plaintext">Comment: 20 pages, 5 figures, 3 tables</p>
					</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_D2Z6C56A">Preprint PDF					</li>
					<li id="item_VAUV49DS">Snapshot					</li>
				</ul>
			</li>


			<li id="item_V9FGSQRX" class="item preprint">
			<h2>Evaluating Generative AI Systems is a Social Science Measurement Challenge</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Preprint</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Hanna Wallach</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Meera Desai</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Nicholas Pangakis</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>A. Feder Cooper</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Angelina Wang</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Solon Barocas</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Alexandra Chouldechova</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Chad Atalla</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Su Lin Blodgett</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Emily Corvi</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>P. Alex Dow</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Jean Garcia-Gathright</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Alexandra Olteanu</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Stefanie Reed</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Emily Sheng</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Dan Vann</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Jennifer Wortman Vaughan</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Matthew Vogel</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Hannah Washington</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Abigail Z. Jacobs</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Across academia, industry, and government, there is an 
increasing awareness that the measurement tasks involved in evaluating 
generative AI (GenAI) systems are especially difficult. We argue that 
these measurement tasks are highly reminiscent of measurement tasks 
found throughout the social sciences. With this in mind, we present a 
framework, grounded in measurement theory from the social sciences, for 
measuring concepts related to the capabilities, impacts, opportunities, 
and risks of GenAI systems. The framework distinguishes between four 
levels: the background concept, the systematized concept, the 
measurement instrument(s), and the instance-level measurements 
themselves. This four-level approach differs from the way measurement is
 typically done in ML, where researchers and practitioners appear to 
jump straight from background concepts to measurement instruments, with 
little to no explicit systematization in between. As well as surfacing 
assumptions, thereby making it easier to understand exactly what the 
resulting measurements do and do not mean, this framework has two 
important implications for evaluating evaluations: First, it can enable 
stakeholders from different worlds to participate in conceptual debates,
 broadening the expertise involved in evaluating GenAI systems. Second, 
it brings rigor to operational debates by offering a set of lenses for 
interrogating the validity of measurement instruments and their 
resulting measurements.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2024-11-17</td>
					</tr>
					<tr>
					<th>Language</th>
						<td>en</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>arXiv.org</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://arxiv.org/abs/2411.10939">http://arxiv.org/abs/2411.10939</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>1/8/2025, 3:17:41 PM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>arXiv:2411.10939 [cs]</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.48550/arXiv.2411.10939">10.48550/arXiv.2411.10939</a></td>
					</tr>
					<tr>
					<th>Repository</th>
						<td>arXiv</td>
					</tr>
					<tr>
					<th>Archive ID</th>
						<td>arXiv:2411.10939</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>1/8/2025, 3:17:41 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Computer Science - Computers and Society</li>
				</ul>
				<h3 class="notes">Notes:</h3>
				<ul class="notes">
					<li id="item_66GGGKFF">
<p class="plaintext">Comment: NeurIPS 2024 Workshop on Evaluating Evaluations (EvalEval)</p>
					</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_GKE2LHW6">Wallach et al. - 2024 - Evaluating Generative AI Systems is a Social Scien.pdf					</li>
				</ul>
			</li>


			<li id="item_D7YP6LVP" class="item webpage">
			<h2>Evaluating Human-AI Collaboration: A Review and Methodological Framework</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Web Page</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>The use of artificial intelligence (AI) in working 
environments with individuals, known as Human-AI Collaboration (HAIC), 
has become essential in a variety of domains, boosting decision-making, 
efficiency, and innovation. Despite HAIC’s wide potential, evaluating 
its effectiveness remains challenging due to the complex interaction of 
components involved. This paper provides a detailed analysis of existing
 HAIC evaluation approaches and develops a fresh paradigm for more 
effectively evaluating these systems. Our framework includes a 
structured decision tree which assists to select relevant metrics based 
on distinct HAIC modes (AI-Centric, Human-Centric, and Symbiotic). By 
including both quantitative and qualitative metrics, the framework seeks
 to represent HAIC’s dynamic and reciprocal nature, enabling the 
assessment of its impact and success. This framework’s practicality can 
be examined by its application in an array of domains, including 
manufacturing, healthcare, finance, and education, each of which has 
unique challenges and requirements. Our hope is that this study will 
facilitate further research on the systematic evaluation of HAIC in 
real-world applications.</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://arxiv.org/html/2407.19098v1">https://arxiv.org/html/2407.19098v1</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>1/8/2025, 3:24:01 PM</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>1/8/2025, 3:24:01 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_P6LGHPFP">Evaluating Human-AI Collaboration: A Review and Methodological Framework					</li>
				</ul>
			</li>


			<li id="item_RS3WL2IA" class="item webpage">
			<h2>Evaluating Literature Reviews Conducted by Humans Versus ChatGPT: Comparative Study - PubMed</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Web Page</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>With the rapid evolution of artificial intelligence (AI), 
particularly large language models (LLMs) such as ChatGPT-4 (OpenAI), 
there is an increasing interest in their potential to assist in 
scholarly tasks, including conducting literature reviews. However, the 
efficacy of AI-generated reviews compared with traditional human-led 
approaches remains underexplored. 
The study suggests that GPT-4, with structured prompt engineering, can 
be a valuable tool for conducting preliminary literature reviews by 
providing a broad overview of topics quickly. However, its limitations 
necessitate careful expert evaluation and refinement, making it an 
assistant rather than a substitute for human expertise in comprehensive 
literature reviews. Moreover, this research highlights the potential and
 limitations of using AI tools like GPT-4 in academic research, 
particularly in the fields of health services and medical research. It 
underscores the necessity of combining AI's rapid information retrieval 
capabilities with human expertise for more accurate and contextually 
rich scholarly outputs.</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://pubmed.ncbi.nlm.nih.gov/39159446/">https://pubmed.ncbi.nlm.nih.gov/39159446/</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>1/8/2025, 3:22:30 PM</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>1/8/2025, 3:22:30 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_8QFEZMR4">Evaluating Literature Reviews Conducted by Humans Versus ChatGPT: Comparative Study - PubMed					</li>
				</ul>
			</li>


			<li id="item_8FD5KFWB" class="item preprint">
			<h2>Evaluating the Potential of LLMs for Thematic Analysis in Czech History Curricula</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Preprint</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Juda Kaleta</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Analyzing Czech school curricula is often difficult due to 
structure, layout, and terminology inconsistencies. This study examines 
how Large Language Models (LLMs) can help address these issues, using 
History curricula as a case study. We evaluated the LLMs' abilities to 
(a) extract relevant data, (b) divide historical content into clear 
topics, and (c) assign accurate historical era labels. Our findings show
 that while LLMs have trouble extracting data from large PDF files, they
 perform very well in dividing content and labelling, achieving results 
similar to those of human coders.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2024-12-27</td>
					</tr>
					<tr>
					<th>Language</th>
						<td>en-us</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>OSF Preprints</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://osf.io/dtj6h">https://osf.io/dtj6h</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>1/6/2025, 11:47:23 AM</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.31235/osf.io/dtj6h">10.31235/osf.io/dtj6h</a></td>
					</tr>
					<tr>
					<th>Repository</th>
						<td>OSF</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>1/6/2025, 11:47:23 AM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Czech School Curricula</li>
					<li>History Content</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_QPE7T8UR">OSF Preprint					</li>
				</ul>
			</li>


			<li id="item_HWX7ENXJ" class="item journalArticle">
			<h2>Excavating awareness and power in data science: A manifesto for trustworthy pervasive data research</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Katie Shilton</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Emanuel Moss</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Sarah A. Gilbert</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Matthew J. Bietz</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Casey Fiesler</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Jacob Metcalf</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Jessica Vitak</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Michael Zimmer</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Frequent public uproar over forms of data science that rely on
 information about people demonstrates the challenges of defining and 
demonstrating trustworthy digital data research practices. This paper 
reviews problems of trustworthiness in what we term pervasive data 
research: scholarship that relies on the rich information generated 
about people through digital interaction. We highlight the entwined 
problems of participant unawareness of such research and the 
relationship of pervasive data research to corporate datafication and 
surveillance. We suggest a way forward by drawing from the history of a 
different methodological approach in which researchers have struggled 
with trustworthy practice: ethnography. To grapple with the colonial 
legacy of their methods, ethnographers have developed analytic lenses 
and researcher practices that foreground relations of awareness and 
power. These lenses are inspiring but also challenging for pervasive 
data research, given the flattening of contexts inherent in digital data
 collection. We propose ways that pervasive data researchers can 
incorporate reflection on awareness and power within their research to 
support the development of trustworthy data science.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2021-07-01</td>
					</tr>
					<tr>
					<th>Language</th>
						<td>en</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>Excavating awareness and power in data science</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>SAGE Journals</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://doi.org/10.1177/20539517211040759">https://doi.org/10.1177/20539517211040759</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>12/23/2024, 6:04:10 PM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>Publisher: SAGE Publications Ltd</td>
					</tr>
					<tr>
					<th>Volume</th>
						<td>8</td>
					</tr>
					<tr>
					<th>Pages</th>
						<td>20539517211040759</td>
					</tr>
					<tr>
					<th>Publication</th>
						<td>Big Data &amp; Society</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.1177/20539517211040759">10.1177/20539517211040759</a></td>
					</tr>
					<tr>
					<th>Issue</th>
						<td>2</td>
					</tr>
					<tr>
					<th>ISSN</th>
						<td>2053-9517</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>12/23/2024, 6:04:10 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_HS2RXSWG">SAGE PDF Full Text					</li>
				</ul>
			</li>


			<li id="item_AAM3RSU3" class="item preprint">
			<h2>Exploring the use of a Large Language Model for data extraction in systematic reviews: a rapid feasibility study</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Preprint</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Lena Schmidt</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Kaitlyn Hair</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Sergio Graziozi</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Fiona Campbell</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Claudia Kapp</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Alireza Khanteymoori</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Dawn Craig</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Mark Engelbert</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>James Thomas</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>This paper describes a rapid feasibility study of using GPT-4,
 a large language model (LLM), to (semi)automate data extraction in 
systematic reviews. Despite the recent surge of interest in LLMs there 
is still a lack of understanding of how to design LLM-based automation 
tools and how to robustly evaluate their performance. During the 2023 
Evidence Synthesis Hackathon we conducted two feasibility studies. 
Firstly, to automatically extract study characteristics from human 
clinical, animal, and social science domain studies. We used two studies
 from each category for prompt-development; and ten for evaluation. 
Secondly, we used the LLM to predict Participants, Interventions, 
Controls and Outcomes (PICOs) labelled within 100 abstracts in the 
EBM-NLP dataset. Overall, results indicated an accuracy of around 80%, 
with some variability between domains (82% for human clinical, 80% for 
animal, and 72% for studies of human social sciences). Causal inference 
methods and study design were the data extraction items with the most 
errors. In the PICO study, participants and intervention/control showed 
high accuracy (&gt;80%), outcomes were more challenging. Evaluation was 
done manually; scoring methods such as BLEU and ROUGE showed limited 
value. We observed variability in the LLMs predictions and changes in 
response quality. This paper presents a template for future evaluations 
of LLMs in the context of data extraction for systematic review 
automation. Our results show that there might be value in using LLMs, 
for example as second or third reviewers. However, caution is advised 
when integrating models such as GPT-4 into tools. Further research on 
stability and reliability in practical settings is warranted for each 
type of data that is processed by the LLM.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2024-05-23</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>Exploring the use of a Large Language Model for data extraction in systematic reviews</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>arXiv.org</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://arxiv.org/abs/2405.14445">http://arxiv.org/abs/2405.14445</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>1/7/2025, 5:29:16 PM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>arXiv:2405.14445 [cs]</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.48550/arXiv.2405.14445">10.48550/arXiv.2405.14445</a></td>
					</tr>
					<tr>
					<th>Repository</th>
						<td>arXiv</td>
					</tr>
					<tr>
					<th>Archive ID</th>
						<td>arXiv:2405.14445</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>1/7/2025, 5:29:16 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Computer Science - Artificial Intelligence</li>
					<li>Computer Science - Computation and Language</li>
				</ul>
				<h3 class="notes">Notes:</h3>
				<ul class="notes">
					<li id="item_GV2MANG9">
<p class="plaintext">Comment: Conference proceedings, peer-reviewed and presented at the 3rd Workshop on Augmented Intelligence for Technology-Assisted Reviews Systems, Glasgow, 2024</p>
					</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_3M48TGFA">Preprint PDF					</li>
					<li id="item_AGP37CUH">Snapshot					</li>
				</ul>
			</li>


			<li id="item_SNU5DGJ6" class="item journalArticle">
			<h2>Exposing influence campaigns in the age of LLMs: a behavioral-based AI approach to detecting state-sponsored trolls</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Fatima Ezzeddine</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Omran Ayoub</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Silvia Giordano</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Gianluca Nogara</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Ihab Sbeity</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Emilio Ferrara</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Luca Luceri</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>The detection of state-sponsored trolls operating in inﬂuence 
campaigns on social media is a critical and unsolved challenge for the 
research community, which has signiﬁcant implications beyond the online 
realm. To address this challenge, we propose a new AI-based solution 
that identiﬁes troll accounts solely through behavioral cues associated 
with their sequences of sharing activity, encompassing both their 
actions and the feedback they receive from others. Our approach does not
 incorporate any textual content shared and consists of two steps: 
First, we leverage an LSTM-based classiﬁer to determine whether account 
sequences belong to a state-sponsored troll or an organic, legitimate 
user. Second, we employ the classiﬁed sequences to calculate a metric 
named the “Troll Score”, quantifying the degree to which an account 
exhibits troll-like behavior. To assess the eﬀectiveness of our method, 
we examine its performance in the context of the 2016 Russian 
interference campaign during the U.S. Presidential election. Our 
experiments yield compelling results, demonstrating that our approach 
can identify account sequences with an AUC close to 99% and accurately 
diﬀerentiate between Russian trolls and organic users with an AUC of 
91%. Notably, our behavioral-based approach holds a signiﬁcant advantage
 in the ever-evolving landscape, where textual and linguistic properties
 can be easily mimicked by Large Language Models (LLMs): In contrast to 
existing language-based techniques, it relies on more 
challenging-to-replicate behavioral cues, ensuring greater resilience in
 identifying inﬂuence campaigns, especially given the potential increase
 in the usage of LLMs for generating inauthentic content. Finally, we 
assessed the generalizability of our solution to various entities 
driving diﬀerent information operations and found promising results that
 will guide future research.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2023-10-09</td>
					</tr>
					<tr>
					<th>Language</th>
						<td>en</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>Exposing influence campaigns in the age of LLMs</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>DOI.org (Crossref)</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://epjdatascience.springeropen.com/articles/10.1140/epjds/s13688-023-00423-4">https://epjdatascience.springeropen.com/articles/10.1140/epjds/s13688-023-00423-4</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>1/8/2025, 2:15:20 PM</td>
					</tr>
					<tr>
					<th>Volume</th>
						<td>12</td>
					</tr>
					<tr>
					<th>Pages</th>
						<td>46</td>
					</tr>
					<tr>
					<th>Publication</th>
						<td>EPJ Data Science</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.1140/epjds/s13688-023-00423-4">10.1140/epjds/s13688-023-00423-4</a></td>
					</tr>
					<tr>
					<th>Issue</th>
						<td>1</td>
					</tr>
					<tr>
					<th>Journal Abbr</th>
						<td>EPJ Data Sci.</td>
					</tr>
					<tr>
					<th>ISSN</th>
						<td>2193-1127</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>1/8/2025, 2:15:20 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_6FQKW9P3">Ezzeddine et al. - 2023 - Exposing influence campaigns in the age of LLMs a.pdf					</li>
				</ul>
			</li>


			<li id="item_YQUNX6R6" class="item preprint">
			<h2>Extracting protest events from newspaper articles with ChatGPT</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Preprint</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Neal Caren</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Kenneth T. Andrews</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Rashawn Ray</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>This research note examines the abilities of a large language 
model (LLM), ChatGPT, to extract structured data on protest events from 
media accounts. Based on our analysis of 500 articles on Black Lives 
Matter protests, after an iterative process of prompt improvement on a 
training dataset, ChatGPT can produce data comparable to or better than a
 hand-coding method with an enormous reduction in time and minimal cost.
 While the technique has limitations, LLMs show promise and deserve 
further study for their use in protest event analysis.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2023-08-08</td>
					</tr>
					<tr>
					<th>Language</th>
						<td>en-us</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>OSF Preprints</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://osf.io/dvht7">https://osf.io/dvht7</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>1/3/2025, 3:55:16 PM</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.31235/osf.io/dvht7">10.31235/osf.io/dvht7</a></td>
					</tr>
					<tr>
					<th>Repository</th>
						<td>OSF</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>1/3/2025, 3:55:16 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>large language model</li>
					<li>social movements</li>
					<li>sociology</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_LTPRWX6T">Extracting protest events from newspaper articles with ChatGPT.docx					</li>
				</ul>
			</li>


			<li id="item_KZ78WNB8" class="item journalArticle">
			<h2>Fiduciary Responsibility: Facilitating Public Trust in Automated Decision Making</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Shannon B. Harper</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Eric S. Weber</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>&lt;p&gt;Automated decision-making systems are being 
increasingly deployed and affect the public in a multitude of positive 
and negative ways. Governmental and private institutions use these 
systems to process information according to certain human-devised rules 
in order to address social problems or organizational challenges. Both 
research and real-world experience indicate that the public lacks trust 
in automated decision-making systems and the institutions that deploy 
them. The recreancy theorem argues that the public is more likely to 
trust and support decisions made or influenced by automated 
decision-making systems if the institutions that administer them meet 
their fiduciary responsibility. However, often the public is never 
informed of how these systems operate and resultant institutional 
decisions are made. A “black box” effect of automated decision-making 
systems reduces the public’s perceptions of integrity and 
trustworthiness. Consequently, the institutions administering these 
systems are less able to assess whether the decisions are just. The 
result is that the public loses the capacity to identify, challenge, and
 rectify unfairness or the costs associated with the loss of public 
goods or benefits. The current position paper defines and explains the 
role of fiduciary responsibility within an automated decision-making 
system. We formulate an automated decision-making system as a data 
science lifecycle (DSL) and examine the implications of fiduciary 
responsibility within the context of the DSL. Fiduciary responsibility 
within DSLs provides a methodology for addressing the public’s lack of 
trust in automated decision-making systems and the institutions that 
employ them to make decisions affecting the public. We posit that 
fiduciary responsibility manifests in several contexts of a DSL, each of
 which requires its own mitigation of sources of mistrust. To 
instantiate fiduciary responsibility, a Los Angeles Police Department 
(LAPD) predictive policing case study is examined. We examine the 
development and deployment by the LAPD of predictive policing technology
 and identify several ways in which the LAPD failed to meet its 
fiduciary responsibility.&lt;/p&gt;</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2022/12/31</td>
					</tr>
					<tr>
					<th>Language</th>
						<td>en</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>Fiduciary Responsibility</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>www.sciopen.com</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://www.sciopen.com/article/10.23919/JSC.2022.0017">https://www.sciopen.com/article/10.23919/JSC.2022.0017</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>12/24/2024, 4:25:02 PM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>Publisher: 清华大学出版社</td>
					</tr>
					<tr>
					<th>Volume</th>
						<td>3</td>
					</tr>
					<tr>
					<th>Pages</th>
						<td>345-362</td>
					</tr>
					<tr>
					<th>Publication</th>
						<td>Journal of Social Computing</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.23919/JSC.2022.0017">10.23919/JSC.2022.0017</a></td>
					</tr>
					<tr>
					<th>Issue</th>
						<td>4</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>12/24/2024, 4:25:02 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_95X73SMU">Full Text PDF					</li>
				</ul>
			</li>


			<li id="item_R8NCCUH9" class="item journalArticle">
			<h2>Fixing Fieldnotes: Developing and Testing a Digital Tool for the Collection, Processing, and Analysis of Ethnographic Data</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Sofie L. Astrupgaard</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>August Lohse</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Emilie M. Gregersen</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Jonathan H. Salka</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Kristoffer Albris</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Morten A. Pedersen</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Ethnographic ﬁeldnotes can contain richer and more thorough 
descriptions of social phenomena compared to other data sources. Their 
open-ended and ﬂexible character makes them especially useful in 
explorative research. However, ﬁeldnotes are typically highly 
unstructured and personalized by individual researchers, which make them
 harder to use as a method for data collection in collaborative and 
mixed methods research. More precisely, the unstructured nature of 
ethnographic ﬁeldnotes presents three distinct challenges: 1) 
Organizability—it can be difﬁcult to search and sort ﬁeldnotes and thus 
to get an overview of them, 2) Integrability—it is difﬁcult to 
meaningfully integrate ﬁeldnotes with other more quantitative data types
 such as more such as surveys or geospatial data, and 3) Computational 
Processability—it is hard to process and analyze ﬁeldnotes with 
computational methods such as topic models and network analysis. To 
solve these three challenges, we present a new digital tool, for the 
systematic collection, processing, and analysis of ethnographic 
ﬁeldnotes. The tool is developed and tested as part of an 
interdisciplinary mixed methods pilot study on attention dynamics at a 
political festival in Denmark. Through case examples from this study, we
 show how adopting this new digital tool allowed our team to overcome 
the three aforementioned challenges of ﬁeldnotes, while retaining the 
ﬂexible and explorative character of ethnographic research, which is a 
key strength of ethnographic ﬁeldwork.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>10/2024</td>
					</tr>
					<tr>
					<th>Language</th>
						<td>en</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>Fixing Fieldnotes</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>DOI.org (Crossref)</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://journals.sagepub.com/doi/10.1177/08944393231220488">https://journals.sagepub.com/doi/10.1177/08944393231220488</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>12/24/2024, 5:51:31 PM</td>
					</tr>
					<tr>
					<th>Volume</th>
						<td>42</td>
					</tr>
					<tr>
					<th>Pages</th>
						<td>1223-1243</td>
					</tr>
					<tr>
					<th>Publication</th>
						<td>Social Science Computer Review</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.1177/08944393231220488">10.1177/08944393231220488</a></td>
					</tr>
					<tr>
					<th>Issue</th>
						<td>5</td>
					</tr>
					<tr>
					<th>Journal Abbr</th>
						<td>Social Science Computer Review</td>
					</tr>
					<tr>
					<th>ISSN</th>
						<td>0894-4393, 1552-8286</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>12/24/2024, 5:51:31 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_CSQ8P4FY">Astrupgaard et al. - 2024 - Fixing Fieldnotes Developing and Testing a Digita.pdf					</li>
				</ul>
			</li>


			<li id="item_WSJDF9FC" class="item journalArticle">
			<h2>Flood of techniques and drought of theories: emotion mining in disasters</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Soheil Shapouri</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Saber Soleymani</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Saed Rezayi</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Emotion mining has become a crucial tool for understanding 
human emotions during disasters, leveraging the extensive data generated
 on social media platforms. This paper aims to summarize existing 
research on emotion mining within disaster contexts, highlighting both 
significant discoveries and persistent issues. On the one hand, emotion 
mining techniques have achieved acceptable accuracy enabling 
applications such as rapid damage assessment and mental health 
surveillance. On the other hand, with many studies adopting data-driven 
approaches, several methodological issues remain. These include 
arbitrary emotion classification, ignoring biases inherent in data 
collection from social media, such as the overrepresentation of 
individuals from higher socioeconomic status on Twitter, and the lack of
 application of theoretical frameworks like cross-cultural comparisons. 
These problems can be summarized as a notable lack of theory-driven 
research and ignoring insights from social and behavioral sciences. This
 paper underscores the need for interdisciplinary collaboration between 
computer scientists and social scientists to develop more robust and 
theoretically grounded approaches in emotion mining. By addressing these
 gaps, we aim to enhance the effectiveness and reliability of emotion 
mining methodologies, ultimately contributing to improved disaster 
preparedness, response, and recovery.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2024-11-29</td>
					</tr>
					<tr>
					<th>Language</th>
						<td>en</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>Flood of techniques and drought of theories</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>Springer Link</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://doi.org/10.1007/s42001-024-00330-2">https://doi.org/10.1007/s42001-024-00330-2</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>1/3/2025, 2:06:50 PM</td>
					</tr>
					<tr>
					<th>Volume</th>
						<td>8</td>
					</tr>
					<tr>
					<th>Pages</th>
						<td>5</td>
					</tr>
					<tr>
					<th>Publication</th>
						<td>Journal of Computational Social Science</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.1007/s42001-024-00330-2">10.1007/s42001-024-00330-2</a></td>
					</tr>
					<tr>
					<th>Issue</th>
						<td>1</td>
					</tr>
					<tr>
					<th>Journal Abbr</th>
						<td>J Comput Soc Sc</td>
					</tr>
					<tr>
					<th>ISSN</th>
						<td>2432-2725</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>1/3/2025, 2:06:50 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Emotion</li>
					<li>Emotion mining</li>
					<li>Natural disasters</li>
					<li>Sentiment analysis</li>
					<li>Technological disasters</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_9GMAQU63">Full Text PDF					</li>
				</ul>
			</li>


			<li id="item_MVKZECSW" class="item journalArticle">
			<h2>For a heterodox computational social science</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Petter Törnberg</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Justus Uitermark</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>The proliferation of digital data has been the impetus for the
 emergence of a new discipline for the study of social life: 
‘computational social science’. Much research in this field is founded 
on the premise that society is a complex system with emergent structures
 that can be modeled or reconstructed through digital data. This paper 
suggests that computational social science serves practical and 
legitimizing functions for digital capitalism in much the same way that 
neoclassical economics does for neoliberalism. In recognition of this 
homology, this paper develops a critique of the complexity perspective 
of computational social science and argues for a heterodox computational
 social science founded on the meta-theory of critical realism that is 
critical, methodological pluralist, interpretative and explanative. This
 implies diverting computational social science’ computational methods 
and digital data so as to not be aimed at identifying invariant laws of 
social life, or optimizing state and corporate practices, but to instead
 be used as part of broader research strategies to identify contingent 
patterns, develop conjunctural explanations, and propose qualitatively 
different ways of organizing social life.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2021-07-01</td>
					</tr>
					<tr>
					<th>Language</th>
						<td>en</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>SAGE Journals</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://doi.org/10.1177/20539517211047725">https://doi.org/10.1177/20539517211047725</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>12/23/2024, 6:05:40 PM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>Publisher: SAGE Publications Ltd</td>
					</tr>
					<tr>
					<th>Volume</th>
						<td>8</td>
					</tr>
					<tr>
					<th>Pages</th>
						<td>20539517211047725</td>
					</tr>
					<tr>
					<th>Publication</th>
						<td>Big Data &amp; Society</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.1177/20539517211047725">10.1177/20539517211047725</a></td>
					</tr>
					<tr>
					<th>Issue</th>
						<td>2</td>
					</tr>
					<tr>
					<th>ISSN</th>
						<td>2053-9517</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>12/23/2024, 6:05:40 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_AV543CBW">SAGE PDF Full Text					</li>
				</ul>
			</li>


			<li id="item_H5FJPDWU" class="item journalArticle">
			<h2>Formally comparing topic models and human-generated qualitative 
coding  of physician mothers’ experiences  of workplace discrimination</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Adam S Miner</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Sheridan A Stewart</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Meghan C Halley</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Laura K Nelson</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Eleni Linos</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Differences between computationally generated and 
human-generated themes in unstructured text are important to understand 
yet difficult to assess formally. In this study, we bridge these 
approaches through two contributions. First, we formally compare a 
primarily computational approach, topic modeling, to a primarily 
human-driven approach, qualitative thematic coding, in an impactful 
context: physician mothers’ experience of workplace discrimination. 
Second, we compare our chosen topic model to a principled alternative 
topic model to make explicit study design decisions meriting 
consideration in future research. By formally contrasting 
computationally generated (i.e. topic modeling) and human-generated 
(i.e. thematic coding) knowledge, we shed light on issues of interest to
 several audiences, notably computational social scientists who wish to 
understand study design tradeoffs, and qualitative researchers who may 
wish to leverage computational methods to improve the speed and 
reproducibility of labor-intensive coding. Although useful in other 
domains, we highlight the value of fast, reproducible methods to better 
understand experiences of workplace discrimination.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2023-01-01</td>
					</tr>
					<tr>
					<th>Language</th>
						<td>en</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>SAGE Journals</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://doi.org/10.1177/20539517221149106">https://doi.org/10.1177/20539517221149106</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>12/23/2024, 5:43:45 PM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>Publisher: SAGE Publications Ltd</td>
					</tr>
					<tr>
					<th>Volume</th>
						<td>10</td>
					</tr>
					<tr>
					<th>Pages</th>
						<td>20539517221149106</td>
					</tr>
					<tr>
					<th>Publication</th>
						<td>Big Data &amp; Society</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.1177/20539517221149106">10.1177/20539517221149106</a></td>
					</tr>
					<tr>
					<th>Issue</th>
						<td>1</td>
					</tr>
					<tr>
					<th>ISSN</th>
						<td>2053-9517</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>12/23/2024, 5:43:45 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_HPA8X5N8">SAGE PDF Full Text					</li>
				</ul>
			</li>


			<li id="item_E6Z7VP5A" class="item journalArticle">
			<h2>Foundation models are platform models: Prompting and the political economy of AI</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Sarah Burkhardt</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Bernhard Rieder</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>A recent innovation in the field of machine learning has been 
the creation of very large pre-trained models, also referred to as 
‘foundation models’, that draw on much larger and broader sets of data 
than typical deep learning systems and can be applied to a wide variety 
of tasks. Underpinning text-based systems such as OpenAI's ChatGPT and 
image generators such as Midjourney, these models have received 
extraordinary amounts of public attention, in part due to their reliance
 on prompting as the main technique to direct and apply them. This paper
 thus uses prompting as an entry point into the critical study of 
foundation models and their implications. The paper proceeds as follows:
 In the first section, we introduce foundation models in more detail, 
outline some of the main critiques, and present our general approach. We
 then discuss prompting as an algorithmic technique, show how it makes 
foundation models programmable, and explain how it enables different 
audiences to use these models as (computational) platforms. In the third
 section, we link the material properties of the technologies under 
scrutiny to questions of political economy, discussing, in turn, deep 
user interactions, reordered cost structures, and centralization and 
lock-in. We conclude by arguing that foundation models and prompting 
further strengthen Big Tech's dominance over the field of computing and,
 through their broad applicability, many other economic sectors, 
challenging our capacities for critical appraisal and regulatory 
response.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2024-06-01</td>
					</tr>
					<tr>
					<th>Language</th>
						<td>en</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>Foundation models are platform models</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>SAGE Journals</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://doi.org/10.1177/20539517241247839">https://doi.org/10.1177/20539517241247839</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>12/23/2024, 4:48:45 PM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>Publisher: SAGE Publications Ltd</td>
					</tr>
					<tr>
					<th>Volume</th>
						<td>11</td>
					</tr>
					<tr>
					<th>Pages</th>
						<td>20539517241247839</td>
					</tr>
					<tr>
					<th>Publication</th>
						<td>Big Data &amp; Society</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.1177/20539517241247839">10.1177/20539517241247839</a></td>
					</tr>
					<tr>
					<th>Issue</th>
						<td>2</td>
					</tr>
					<tr>
					<th>ISSN</th>
						<td>2053-9517</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>12/23/2024, 4:48:45 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_UQHB682W">SAGE PDF Full Text					</li>
				</ul>
			</li>


			<li id="item_YES6DGGM" class="item journalArticle">
			<h2>From Calculations to Reasoning: History, Trends and the Potential
 of Computational Ethnography and Computational Social Anthropology</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Manolis Peponakis</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Sarantos Kapidakis</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Martin Doerr</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Eirini Tountasaki</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>The domains of computational social anthropology and 
computational ethnography refer to the computational processing or 
computational modelling of data for anthropological or ethnographic 
research. In this context, the article surveys the use of computational 
methods regarding the production and the representation of knowledge. 
The ultimate goal of the study is to highlight the significance of 
modelling ethnographic data and anthropological knowledge by harnessing 
the potential of the semantic web. The first objective was to review the
 use of computational methods in anthropological research focusing on 
the last 25&nbsp;years, while the second objective was to explore the 
potential of the semantic web focusing on existing technologies for 
ontological representation. For these purposes, the study explores the 
use of computers in anthropology regarding data processing and data 
modelling for more effective data processing. The survey reveals that 
there is an ongoing transition from the instrumentalisation of computers
 as tools for calculations, to the implementation of information science
 methodologies for analysis, deduction, knowledge representation, and 
reasoning, as part of the research process in social anthropology. 
Finally, it is highlighted that the ecosystem of the semantic web does 
not subserve quantification and metrics but introduces a new 
conceptualisation for addressing and meeting research questions in 
anthropology.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2024-02-01</td>
					</tr>
					<tr>
					<th>Language</th>
						<td>en</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>From Calculations to Reasoning</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>SAGE Journals</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://doi.org/10.1177/08944393231167692">https://doi.org/10.1177/08944393231167692</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>12/24/2024, 7:15:47 PM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>Publisher: SAGE Publications Inc</td>
					</tr>
					<tr>
					<th>Volume</th>
						<td>42</td>
					</tr>
					<tr>
					<th>Pages</th>
						<td>84-102</td>
					</tr>
					<tr>
					<th>Publication</th>
						<td>Social Science Computer Review</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.1177/08944393231167692">10.1177/08944393231167692</a></td>
					</tr>
					<tr>
					<th>Issue</th>
						<td>1</td>
					</tr>
					<tr>
					<th>ISSN</th>
						<td>0894-4393</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>12/24/2024, 7:15:47 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_N6PPBUVL">Submitted Version					</li>
				</ul>
			</li>


			<li id="item_N4E5UITQ" class="item preprint">
			<h2>From Division to Unity: A Large-Scale Study on the Emergence of Computational Social Science, 1990-2021</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Preprint</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Honglin Bao</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Jiawei Zhang</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Mingxuan Cao</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>James A. Evans</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>We present a comprehensive study on the emergence of 
Computational Social Science (CSS) - an interdisciplinary field 
leveraging computational methods to address social science questions - 
and its impact on adjacent social sciences. We trained a robust CSS 
classifier using papers from CSS-focused venues and applied it to 11 
million papers spanning 1990 to 2021. Our analysis yielded three key 
findings. First, there were two critical inflections in the rise of CSS.
 The first occurred around 2005 when psychology, politics, and sociology
 began engaging with CSS. The second emerged in approximately 2014 when 
economics finally joined the trend. Sociology is currently the most 
engaged with CSS. Second, using the density of yearly knowledge 
embeddings constructed by advanced transformer models, we observed that 
CSS initially lacked a cohesive identity. From the early 2000s to 2014, 
however, it began to form a distinct cluster, creating boundaries 
between CSS and other social sciences, particularly in politics and 
sociology. After 2014, these boundaries faded, and CSS increasingly 
blended with the social sciences. Third, shared data-driven methods 
homogenized CSS papers across disciplines, with politics and economics 
showing the most alignment due to the combined influence of CSS and 
causal identification. Nevertheless, non-CSS papers in sociology, 
psychology, and politics became more divergent. Taken together, these 
findings highlight the dynamics of division and unity as new disciplines
 emerge within existing knowledge landscapes. A live demo of CSS 
evolution can be found in https://evolution-css.netlify.app/</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2024-12-16</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>From Division to Unity</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>arXiv.org</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://arxiv.org/abs/2412.08087">http://arxiv.org/abs/2412.08087</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>12/24/2024, 4:45:15 PM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>arXiv:2412.08087 [cs]</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.48550/arXiv.2412.08087">10.48550/arXiv.2412.08087</a></td>
					</tr>
					<tr>
					<th>Repository</th>
						<td>arXiv</td>
					</tr>
					<tr>
					<th>Archive ID</th>
						<td>arXiv:2412.08087</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>12/24/2024, 4:45:15 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Computer Science - Computers and Society</li>
					<li>Computer Science - Digital Libraries</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_Z46MWZA5">Preprint PDF					</li>
					<li id="item_KWFF6UFC">Snapshot					</li>
				</ul>
			</li>


			<li id="item_H93G96S6" class="item preprint">
			<h2>From Division to Unity: A Large-Scale Study on the Emergence of Computational Social Science, 1990-2021</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Preprint</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Honglin Bao</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Jiawei Zhang</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Mingxuan Cao</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>James A. Evans</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>We present a comprehensive study on the emergence of 
Computational Social Science (CSS) - an interdisciplinary field 
leveraging computational methods to address social science questions - 
and its impact on adjacent social sciences. We trained a robust CSS 
classifier using papers from CSS-focused venues and applied it to 11 
million papers spanning 1990 to 2021. Our analysis yielded three key 
findings. First, there were two critical inflections in the rise of CSS.
 The first occurred around 2005 when psychology, politics, and sociology
 began engaging with CSS. The second emerged in approximately 2014 when 
economics finally joined the trend. Sociology is currently the most 
engaged with CSS. Second, using the density of yearly knowledge 
embeddings constructed by advanced transformer models, we observed that 
CSS initially lacked a cohesive identity. From the early 2000s to 2014, 
however, it began to form a distinct cluster, creating boundaries 
between CSS and other social sciences, particularly in politics and 
sociology. After 2014, these boundaries faded, and CSS increasingly 
blended with the social sciences. Third, shared data-driven methods 
homogenized CSS papers across disciplines, with politics and economics 
showing the most alignment due to the combined influence of CSS and 
causal identification. Nevertheless, non-CSS papers in sociology, 
psychology, and politics became more divergent. Taken together, these 
findings highlight the dynamics of division and unity as new disciplines
 emerge within existing knowledge landscapes. A live demo of CSS 
evolution can be found in https://evolution-css.netlify.app/</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2024-12-16</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>From Division to Unity</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>arXiv.org</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://arxiv.org/abs/2412.08087">http://arxiv.org/abs/2412.08087</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>1/8/2025, 12:54:54 PM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>arXiv:2412.08087 [cs]</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.48550/arXiv.2412.08087">10.48550/arXiv.2412.08087</a></td>
					</tr>
					<tr>
					<th>Repository</th>
						<td>arXiv</td>
					</tr>
					<tr>
					<th>Archive ID</th>
						<td>arXiv:2412.08087</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>1/8/2025, 12:54:54 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Computer Science - Computers and Society</li>
					<li>Computer Science - Digital Libraries</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_X7WEABZ4">Preprint PDF					</li>
					<li id="item_RRB5WPWR">Snapshot					</li>
				</ul>
			</li>


			<li id="item_FCJQ3JFK" class="item journalArticle">
			<h2>From Ends to Means: The Promise of Computational Text Analysis for Theoretically Driven Sociological Research</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Bart Bonikowski</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Laura K. Nelson</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>As the ﬁeld of computational text analysis within the social 
sciences is maturing, computational methods are no longer seen as ends 
in themselves, but rather as means toward answering theoretically 
motivated research questions. The objective of this special issue is to 
showcase such research: the use of novel computational methods in the 
service of advancing substantive scientiﬁc knowledge. In presenting the 
contributions to the issue, we discuss several insights that emerge from
 this work, which hold relevance not only for current and aspiring 
practitioners of computational text analysis, but also for its skeptics.
 These concern the central role of theory in designing and executing 
computational research, the selection of appropriate techniques from a 
rapidly growing methodological toolkit, the beneﬁts—and risks—of 
methodological bricolage, and the necessity of validating all aspects of
 the research process. The result is a set of broad considerations 
concerning the effective application of computational methods to 
substantive questions, illustrated by eight exemplary empirical studies.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>11/2022</td>
					</tr>
					<tr>
					<th>Language</th>
						<td>en</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>From Ends to Means</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>DOI.org (Crossref)</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://journals.sagepub.com/doi/10.1177/00491241221123088">https://journals.sagepub.com/doi/10.1177/00491241221123088</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>1/6/2025, 1:17:59 PM</td>
					</tr>
					<tr>
					<th>Volume</th>
						<td>51</td>
					</tr>
					<tr>
					<th>Pages</th>
						<td>1469-1483</td>
					</tr>
					<tr>
					<th>Publication</th>
						<td>Sociological Methods &amp; Research</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.1177/00491241221123088">10.1177/00491241221123088</a></td>
					</tr>
					<tr>
					<th>Issue</th>
						<td>4</td>
					</tr>
					<tr>
					<th>Journal Abbr</th>
						<td>Sociological Methods &amp; Research</td>
					</tr>
					<tr>
					<th>ISSN</th>
						<td>0049-1241, 1552-8294</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>1/6/2025, 1:17:59 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_LC3ZDJT8">Bonikowski and Nelson - 2022 - From Ends to Means The Promise of Computational T.pdf					</li>
				</ul>
			</li>


			<li id="item_ILVW2QW7" class="item preprint">
			<h2>From Explanations to Action: A Zero-Shot, Theory-Driven LLM Framework for Student Performance Feedback</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Preprint</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Vinitra Swamy</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Davide Romano</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Bhargav Srinivasa Desikan</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Oana-Maria Camburu</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Tanja Käser</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Recent advances in eXplainable AI (XAI) for education have 
highlighted a critical challenge: ensuring that explanations for 
state-of-the-art AI models are understandable for non-technical users 
such as educators and students. In response, we introduce iLLuMinaTE, a 
zero-shot, chain-of-prompts LLM-XAI pipeline inspired by Miller's 
cognitive model of explanation. iLLuMinaTE is designed to deliver 
theory-driven, actionable feedback to students in online courses. 
iLLuMinaTE navigates three main stages - causal connection, explanation 
selection, and explanation presentation - with variations drawing from 
eight social science theories (e.g. Abnormal Conditions, Pearl's Model 
of Explanation, Necessity and Robustness Selection, Contrastive 
Explanation). We extensively evaluate 21,915 natural language 
explanations of iLLuMinaTE extracted from three LLMs (GPT-4o, Gemma2-9B,
 Llama3-70B), with three different underlying XAI methods (LIME, 
Counterfactuals, MC-LIME), across students from three diverse online 
courses. Our evaluation involves analyses of explanation alignment to 
the social science theory, understandability of the explanation, and a 
real-world user preference study with 114 university students containing
 a novel actionability simulation. We find that students prefer 
iLLuMinaTE explanations over traditional explainers 89.52% of the time. 
Our work provides a robust, ready-to-use framework for effectively 
communicating hybrid XAI-driven insights in education, with significant 
generalization potential for other human-centric fields.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2024-09-12</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>From Explanations to Action</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>arXiv.org</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://arxiv.org/abs/2409.08027">http://arxiv.org/abs/2409.08027</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>1/7/2025, 5:08:52 PM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>arXiv:2409.08027 [cs]</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.48550/arXiv.2409.08027">10.48550/arXiv.2409.08027</a></td>
					</tr>
					<tr>
					<th>Repository</th>
						<td>arXiv</td>
					</tr>
					<tr>
					<th>Archive ID</th>
						<td>arXiv:2409.08027</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>1/7/2025, 5:08:52 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Computer Science - Computers and Society</li>
					<li>Computer Science - Human-Computer Interaction</li>
					<li>Computer Science - Machine Learning</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_9R4HCZ6K">Preprint PDF					</li>
					<li id="item_CNTMUYIR">Snapshot					</li>
				</ul>
			</li>


			<li id="item_4WQWZF73" class="item preprint">
			<h2>From Individual to Society: A Survey on Social Simulation Driven by Large Language Model-based Agents</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Preprint</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Xinyi Mou</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Xuanwen Ding</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Qi He</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Liang Wang</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Jingcong Liang</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Xinnong Zhang</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Libo Sun</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Jiayu Lin</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Jie Zhou</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Xuanjing Huang</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Zhongyu Wei</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Traditional sociological research often relies on human 
participation, which, though effective, is expensive, challenging to 
scale, and with ethical concerns. Recent advancements in large language 
models (LLMs) highlight their potential to simulate human behavior, 
enabling the replication of individual responses and facilitating 
studies on many interdisciplinary studies. In this paper, we conduct a 
comprehensive survey of this field, illustrating the recent progress in 
simulation driven by LLM-empowered agents. We categorize the simulations
 into three types: (1) Individual Simulation, which mimics specific 
individuals or demographic groups; (2) Scenario Simulation, where 
multiple agents collaborate to achieve goals within specific contexts; 
and (3) Society Simulation, which models interactions within agent 
societies to reflect the complexity and variety of real-world dynamics. 
These simulations follow a progression, ranging from detailed individual
 modeling to large-scale societal phenomena. We provide a detailed 
discussion of each simulation type, including the architecture or key 
components of the simulation, the classification of objectives or 
scenarios and the evaluation method. Afterward, we summarize commonly 
used datasets and benchmarks. Finally, we discuss the trends across 
these three types of simulation. A repository for the related sources is
 at {\url{https://github.com/FudanDISC/SocialAgent}}.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2024-12-04</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>From Individual to Society</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>arXiv.org</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://arxiv.org/abs/2412.03563">http://arxiv.org/abs/2412.03563</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>1/7/2025, 4:35:18 PM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>arXiv:2412.03563 [cs]</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.48550/arXiv.2412.03563">10.48550/arXiv.2412.03563</a></td>
					</tr>
					<tr>
					<th>Repository</th>
						<td>arXiv</td>
					</tr>
					<tr>
					<th>Archive ID</th>
						<td>arXiv:2412.03563</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>1/7/2025, 4:35:18 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Computer Science - Computation and Language</li>
					<li>Computer Science - Computers and Society</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_2SPR472E">Preprint PDF					</li>
					<li id="item_UWR4S7EU">Snapshot					</li>
				</ul>
			</li>


			<li id="item_WXTFSAQ7" class="item conferencePaper">
			<h2>From nCoder to ChatGPT: From Automated Coding to Refining Human Coding</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Conference Paper</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Andres Felipe Zambrano</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Xiner Liu</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Amanda Barany</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Ryan S. Baker</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Juhan Kim</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Nidhi Nasiar</td>
					</tr>
					<tr>
						<th class="editor">Editor</th>
						<td>Golnaz Arastoopour Irgens</td>
					</tr>
					<tr>
						<th class="editor">Editor</th>
						<td>Simon Knight</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>This paper investigates the potential of utilizing ChatGPT 
(GPT-4) as a tool for supporting coding processes for Quantitative 
Ethnography research. We compare the use of ChatGPT and nCoder, the most
 widely used automated coding tool in the QE community, on a dataset of 
press releases and public addresses delivered by governmental leaders 
from seven countries from late February to late March 2020. The study 
assesses the accuracy of the automated coding procedures between the two
 tools, and the role that ChatGPT’s explanations of its coding decisions
 can play in improving the consistency and construct validity of 
human-generated codes. Results suggest that both ChatGPT and nCoder have
 advantages and disadvantages depending on the context, nature of the 
data, and researchers’ goals. While nCoder is useful for straightforward
 coding schemes represented through regular expressions, ChatGPT can 
better capture a variety of language structures. ChatGPT's ability to 
provide explanations for its decisions can also help enhance construct 
validity, identify ambiguity in code definitions, and assist human 
coders in achieving high interrater reliability. Although we identify 
limitations of ChatGPT in coding constructs open to human 
interpretations and encompassing multiple concepts, we highlight 
opportunities and potential benefits provided by ChatGPT as a tool to 
support human researchers in their coding process.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2023</td>
					</tr>
					<tr>
					<th>Language</th>
						<td>en</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>From nCoder to ChatGPT</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>Springer Link</td>
					</tr>
					<tr>
					<th>Place</th>
						<td>Cham</td>
					</tr>
					<tr>
					<th>Publisher</th>
						<td>Springer Nature Switzerland</td>
					</tr>
					<tr>
					<th>ISBN</th>
						<td>978-3-031-47014-1</td>
					</tr>
					<tr>
					<th>Pages</th>
						<td>470-485</td>
					</tr>
					<tr>
					<th>Proceedings Title</th>
						<td>Advances in Quantitative Ethnography</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.1007/978-3-031-47014-1_32">10.1007/978-3-031-47014-1_32</a></td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>4/28/2024, 6:54:30 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>4/28/2024, 6:54:30 PM</td>
					</tr>
				</tbody></table>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_32FKSWH6">Submitted Version					</li>
				</ul>
			</li>


			<li id="item_6AALM3KV" class="item journalArticle">
			<h2>From rules to examples: Machine learning's type of authority</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Alexander Campolo</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Katia Schwerzmann</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>This paper analyzes the effects of a perceived transition from
 a rule-based computer programming paradigm to an example-based paradigm
 associated with machine learning. While both paradigms coexist in 
practice, we critically discuss the distinctive epistemological and 
ethical implications of machine learning's “exemplary” type of 
authority. To capture its logic, we compare it to computer programming 
rules that date to the middle of the 20th century, showing how rules and
 examples have regulated human conduct in significantly different ways. 
In contrast to the highly constructed, explicit, and prescriptive form 
of authority imposed by programming rules, machine learning models are 
trained using data that has been made into examples. These examples 
elicit norms in an implicit, emergent manner to make prediction and 
classification possible. We analyze three ways that examples are 
produced in machine learning: labeling, feature engineering, and 
scaling. We use the phrase “artificial naturalism” to characterize the 
tensions of this type of authority, in which examples sit ambiguously 
between data and norm.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2023-07-01</td>
					</tr>
					<tr>
					<th>Language</th>
						<td>en</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>From rules to examples</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>SAGE Journals</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://doi.org/10.1177/20539517231188725">https://doi.org/10.1177/20539517231188725</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>12/23/2024, 5:33:29 PM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>Publisher: SAGE Publications Ltd</td>
					</tr>
					<tr>
					<th>Volume</th>
						<td>10</td>
					</tr>
					<tr>
					<th>Pages</th>
						<td>20539517231188725</td>
					</tr>
					<tr>
					<th>Publication</th>
						<td>Big Data &amp; Society</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.1177/20539517231188725">10.1177/20539517231188725</a></td>
					</tr>
					<tr>
					<th>Issue</th>
						<td>2</td>
					</tr>
					<tr>
					<th>ISSN</th>
						<td>2053-9517</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>12/23/2024, 5:33:30 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_JMTACMWB">SAGE PDF Full Text					</li>
				</ul>
			</li>


			<li id="item_ZUI2A5P3" class="item book">
			<h2>From Silicon to Solutions: AI's Impending Impact on Research and Discovery</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Book</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>David Markowitz</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Ryan Boyd</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Kate Blackburn</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>The social sciences have long relied on comparative work as 
the foundation upon which we understand the complexities of human 
behavior and society. However, as we delve deeper into the era of 
artificial intelligence (AI), it becomes imperative to move beyond mere 
comparison (e.g., how AI compares to humans across a range of tasks) to 
establish a visionary agenda for AI as collaborative partners in the 
pursuit of knowledge and scientific inquiry. This paper articulates an 
agenda that envisions AI models as the preeminent scientific 
collaborators. We advocate for the profound notion that our thinking 
should evolve to anticipate, and include, AI models as one of the most 
impactful tools in the social scientist's toolbox, offering assistance 
and collaboration with low-level tasks (e.g., analysis and 
interpretation of research findings) and high-level tasks (e.g., the 
discovery of new academic frontiers) alike. This transformation requires
 us to imagine AI's possible/probable roles in the research process. We 
discuss the prospect of AI as active knowledge generators within the 
scientific community — agents who participate in the scientific journey,
 aiming to make complex human issues more tractable and comprehensible. 
We foresee AI tools acting as co-researchers, contributing to research 
proposals and driving breakthrough discoveries. Ethical considerations 
are paramount, encompassing democratizing access to AI tools, fostering 
interdisciplinary collaborations, ensuring transparency, fairness, and 
privacy in AI-driven research, and addressing limitations and biases in 
large language models. Embracing AI as collaborative partners will 
revolutionize the landscape of social sciences, enabling innovative, 
inclusive, and ethically sound research practices.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2024-02-27</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>From Silicon to Solutions</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>ResearchGate</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>DOI: 10.31234/osf.io/dwx5n</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>4/28/2024, 7:32:10 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>4/28/2024, 7:32:10 PM</td>
					</tr>
				</tbody></table>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_QGXHRHV6">Full Text PDF					</li>
					<li id="item_BKU4RR38">ResearchGate Link					</li>
				</ul>
			</li>


			<li id="item_MUDT39ZB" class="item journalArticle">
			<h2>From Strange to Normal: Computational Approaches to Examining Immigrant Incorporation Through Shifts in the Mainstream</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Andrea Voyer</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Zachary D. Kline</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Madison Danton</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Tatiana Volkova</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>This article presents a computational approach to examining 
immigrant incorporation through shifts in the social “mainstream.” 
Analyzing a historical corpus of American etiquette books, texts from 
1922–2017 describing social norms, we identify mainstream shifts related
 to long-standing groups which once were and may currently still be seen
 as immigrant outsiders in the United States: Catholic, Chinese, Irish, 
Italian, Jewish, Mexican, and Muslim groups. The analysis takes a 
computational grounded theory approach, combining qualitative readings 
and computational text analyses. Using word embeddings, we 
operationalize the chosen groups as focal group concepts. We extract 
sections of text that are salient to the focal group concepts to create 
group-specific text corpora. Two computational approaches make it 
possible to examine mainstream shifts in these corpora. First, we use 
sentiment analysis to observe the positive sentiment in each corpus and 
its change over time. Second, we observe changes in each corpus's 
position on a semantic dimension represented by the poles of “strange” 
and “normal.” The results indicate mainstream shifts through increases 
in positive sentiment and movement from strange to normal over time for 
most of the group-specific corpora. These research techniques can be 
adapted to other studies of social sentiment and symbolic inclusion.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2022-11-01</td>
					</tr>
					<tr>
					<th>Language</th>
						<td>en</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>From Strange to Normal</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>SAGE Journals</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://doi.org/10.1177/00491241221122596">https://doi.org/10.1177/00491241221122596</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>1/6/2025, 1:16:13 PM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>Publisher: SAGE Publications Inc</td>
					</tr>
					<tr>
					<th>Volume</th>
						<td>51</td>
					</tr>
					<tr>
					<th>Pages</th>
						<td>1540-1579</td>
					</tr>
					<tr>
					<th>Publication</th>
						<td>Sociological Methods &amp; Research</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.1177/00491241221122596">10.1177/00491241221122596</a></td>
					</tr>
					<tr>
					<th>Issue</th>
						<td>4</td>
					</tr>
					<tr>
					<th>ISSN</th>
						<td>0049-1241</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>1/6/2025, 1:16:13 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_26PVAXRQ">SAGE PDF Full Text					</li>
				</ul>
			</li>


			<li id="item_7JFQA3IR" class="item journalArticle">
			<h2>From Symbols to Embeddings: A Tale of Two Representations in Computational Social Science</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Huimin Chen</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Cheng Yang</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Xuanming Zhang</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Zhiyuan Liu</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Maosong Sun</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Jianbin Jin</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>&lt;p&gt;Computational Social Science (CSS), aiming at 
utilizing computational methods to address social science problems, is a
 recent emerging and fast-developing field. The study of CSS is 
data-driven and significantly benefits from the availability of online 
user-generated contents and social networks, which contain rich text and
 network data for investigation. However, these large-scale and 
multi-modal data also present researchers with a great challenge: how to
 represent data effectively to mine the meanings we want in CSS? To 
explore the answer, we give a thorough review of data representations in
 CSS for both text and network. Specifically, we summarize existing 
representations into two schemes, namely symbol-based and 
embedding-based representations, and introduce a series of typical 
methods for each scheme. Afterwards, we present the applications of the 
above representations based on the investigation of more than 400 
research articles from 6 top venues involved with CSS. From the 
statistics of these applications, we unearth the strength of each kind 
of representations and discover the tendency that embedding-based 
representations are emerging and obtaining increasing attention over the
 last decade. Finally, we discuss several key challenges and open issues
 for future directions. This survey aims to provide a deeper 
understanding and more advisable applications of data representations 
for CSS researchers.&lt;/p&gt;</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2021/06/01</td>
					</tr>
					<tr>
					<th>Language</th>
						<td>en</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>From Symbols to Embeddings</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>www.sciopen.com</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://www.sciopen.com/article/10.23919/JSC.2021.0011">https://www.sciopen.com/article/10.23919/JSC.2021.0011</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>12/24/2024, 4:37:03 PM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>Publisher: 清华大学出版社</td>
					</tr>
					<tr>
					<th>Volume</th>
						<td>2</td>
					</tr>
					<tr>
					<th>Pages</th>
						<td>103-156</td>
					</tr>
					<tr>
					<th>Publication</th>
						<td>Journal of Social Computing</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.23919/JSC.2021.0011">10.23919/JSC.2021.0011</a></td>
					</tr>
					<tr>
					<th>Issue</th>
						<td>2</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>12/24/2024, 4:37:03 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_ZM5BRGCJ">Full Text PDF					</li>
				</ul>
			</li>


			<li id="item_FDK47TCT" class="item journalArticle">
			<h2>From Text Signals to Simulations: A Review and Complement to Text as Data by Grimmer, Roberts &amp; Stewart (PUP 2022)</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>James Evans</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Text as Data represents a major advance for teaching text 
analysis in the social sciences, digital humanities and data science by 
providing an integrated framework for how to conceptualize and deploy 
natural language processing techniques to enrich descriptive and causal 
analyses of social life in and from text. Here I review achievements of 
the book and highlight complementary paths not taken, including 
discussion of recent computational techniques like transformers, which 
have come to dominate automated language understanding and are just 
beginning to find their way into the careful research designs showcased 
in the book. These new methods not only highlight text as a signal from 
society, but textual models as simulations of society, which could fuel 
future advances in causal inference and experimentation. Text as Data's 
focus on textual discovery, measurement and inference points us toward 
this new frontier, cautioning us not to ignore, but build upon social 
scientific interpretation and theory.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2022-11-01</td>
					</tr>
					<tr>
					<th>Language</th>
						<td>en</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>From Text Signals to Simulations</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>SAGE Journals</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://doi.org/10.1177/00491241221123086">https://doi.org/10.1177/00491241221123086</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>9/24/2024, 4:05:22 PM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>Publisher: SAGE Publications Inc</td>
					</tr>
					<tr>
					<th>Volume</th>
						<td>51</td>
					</tr>
					<tr>
					<th>Pages</th>
						<td>1868-1885</td>
					</tr>
					<tr>
					<th>Publication</th>
						<td>Sociological Methods &amp; Research</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.1177/00491241221123086">10.1177/00491241221123086</a></td>
					</tr>
					<tr>
					<th>Issue</th>
						<td>4</td>
					</tr>
					<tr>
					<th>ISSN</th>
						<td>0049-1241</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>9/24/2024, 4:05:22 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>9/24/2024, 4:05:22 PM</td>
					</tr>
				</tbody></table>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_M9PLC2V8">SAGE PDF Full Text					</li>
				</ul>
			</li>


			<li id="item_JSD5YYXJ" class="item journalArticle">
			<h2>Generating reality and silencing debate: Synthetic data as discursive device</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Paula Helm</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Benjamin Lipp</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Roser Pujadas</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>In addition to tapping data from users’ behavioral surplus, by
 drawing on generative adversarial networks, data for artificial 
intelligence is now increasingly being generated through artificial 
intelligence. With this new method of producing data synthetically, the 
data economy is not only shifting from “data collection” to “data 
generation.” Synthetic data is also being employed to address some of 
the most pressing ethical concerns around artificial intelligence. It 
thereby comes with the sociotechnical imaginary that social problems can
 be cut out of artificial intelligence, separating training data from 
real persons. In response to this technical solutionism, this commentary
 aims to initiate a critical debate about synthetic data that goes 
beyond misuse scenarios such as the use of generative adversarial 
networks to create deep fakes or dark patterns. Instead, on a more 
general level, we seek to complicate the idea of “solving,” i.e., 
“closing” and thus “silencing” the ethico-political debates for which 
synthetic data is supposed to be a solution by showing how synthetic 
data itself is political. Drawing on the complex connections between 
recent uses of synthetic data and public debates about artificial 
intelligence, we therefore propose to consider and analyze synthetic 
data not only as a technical device but as a discursive one as well. To 
this end, we shed light on their relationship to three pillars that we 
see associated with them (a) algorithmic bias, (b) privacy, (c) platform
 economy.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2024-06-01</td>
					</tr>
					<tr>
					<th>Language</th>
						<td>en</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>Generating reality and silencing debate</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>SAGE Journals</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://doi.org/10.1177/20539517241249447">https://doi.org/10.1177/20539517241249447</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>12/23/2024, 4:50:52 PM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>Publisher: SAGE Publications Ltd</td>
					</tr>
					<tr>
					<th>Volume</th>
						<td>11</td>
					</tr>
					<tr>
					<th>Pages</th>
						<td>20539517241249447</td>
					</tr>
					<tr>
					<th>Publication</th>
						<td>Big Data &amp; Society</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.1177/20539517241249447">10.1177/20539517241249447</a></td>
					</tr>
					<tr>
					<th>Issue</th>
						<td>2</td>
					</tr>
					<tr>
					<th>ISSN</th>
						<td>2053-9517</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>12/23/2024, 4:50:52 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_YYAG2ZTK">SAGE PDF Full Text					</li>
				</ul>
			</li>


			<li id="item_9LDA2AKS" class="item preprint">
			<h2>Generative Agent-Based Models for Complex Systems Research: a review</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Preprint</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Yikang Lu</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Alberto Aleta</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Chunpeng Du</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Lei Shi</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Yamir Moreno</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>The advent of Large Language Models (LLMs) has significantly 
transformed the fields of natural and social sciences. Generative 
Agent-Based Models (GABMs), which utilize large language models in place
 of real subjects, are gaining increasing public attention. Far from 
aiming for comprehensiveness, this paper aims to offer readers an 
opportunity to understand how large language models are disrupting 
complex systems research and behavioral sciences. In particular, we 
evaluate recent advancements in various domains within complex systems, 
encompassing network science, evolutionary game theory, social dynamics,
 and epidemic propagation. Additionally, we propose possible directions 
for future research to further advance these fields.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2024-08-17</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>Generative Agent-Based Models for Complex Systems Research</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>arXiv.org</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://arxiv.org/abs/2408.09175">http://arxiv.org/abs/2408.09175</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>1/7/2025, 5:12:42 PM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>arXiv:2408.09175 [physics]</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.48550/arXiv.2408.09175">10.48550/arXiv.2408.09175</a></td>
					</tr>
					<tr>
					<th>Repository</th>
						<td>arXiv</td>
					</tr>
					<tr>
					<th>Archive ID</th>
						<td>arXiv:2408.09175</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>1/7/2025, 5:12:42 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Physics - Physics and Society</li>
				</ul>
				<h3 class="notes">Notes:</h3>
				<ul class="notes">
					<li id="item_6Z2MCVR3">
<p class="plaintext">Comment: 12 pages, 6 figures, 156 conference</p>
					</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_JHZWZXCK">Preprint PDF					</li>
					<li id="item_F7YLLXNC">Snapshot					</li>
				</ul>
			</li>


			<li id="item_XBBXMNFF" class="item journalArticle">
			<h2>Generative AI and the politics of visibility</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Tarleton Gillespie</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Proponents of generative AI tools claim they will supplement, 
even replace, the work of cultural production. This raises questions 
about the politics of visibility: what kinds of stories do these tools 
tend to generate, and what do they generally not? Do these tools match 
the kind of diversity of representation that marginalized populations 
and non-normative communities have fought to secure in publishing and 
broadcast media? I tested three widely available generative AI tools 
with prompts designed to reveal these normative assumptions; I prompted 
the tools multiple times with each, to track the diversity of the 
outputs to the same query. I demonstrate that, as currently designed and
 trained, generative AI tools tend to reproduce normative identities and
 narratives, rarely representing less common arrangements and 
perspectives. When they do generate variety, it is often narrow, 
maintaining deeper normative assumptions in what remains absent.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2024-06-01</td>
					</tr>
					<tr>
					<th>Language</th>
						<td>en</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>SAGE Journals</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://doi.org/10.1177/20539517241252131">https://doi.org/10.1177/20539517241252131</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>12/23/2024, 4:53:19 PM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>Publisher: SAGE Publications Ltd</td>
					</tr>
					<tr>
					<th>Volume</th>
						<td>11</td>
					</tr>
					<tr>
					<th>Pages</th>
						<td>20539517241252131</td>
					</tr>
					<tr>
					<th>Publication</th>
						<td>Big Data &amp; Society</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.1177/20539517241252131">10.1177/20539517241252131</a></td>
					</tr>
					<tr>
					<th>Issue</th>
						<td>2</td>
					</tr>
					<tr>
					<th>ISSN</th>
						<td>2053-9517</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>12/23/2024, 4:53:19 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_MJBRZLV6">SAGE PDF Full Text					</li>
				</ul>
			</li>


			<li id="item_7G4HVPR8" class="item preprint">
			<h2>Generative AI has lowered the barriers to computational social sciences</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Preprint</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Yongjun Zhang</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Generative artificial intelligence (AI) has revolutionized the
 field of computational social science, unleashing new possibilities for
 analyzing multimodal data, especially for scholars who may not have 
extensive programming expertise. This breakthrough carries profound 
implications for the realm of social sciences. Firstly, generative AI 
can significantly enhance the productivity of social scientists by 
automating the generation, annotation, and debugging of code. Secondly, 
it empowers researchers to delve into sophisticated data analysis 
through the innovative use of prompt engineering. Lastly, the 
educational sphere of computational social science stands to benefit 
immensely from these tools, given their exceptional ability to annotate 
and elucidate complex codes for learners, thereby simplifying the 
learning process and making the technology more accessible.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2023-11-17</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>arXiv.org</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://arxiv.org/abs/2311.10833">http://arxiv.org/abs/2311.10833</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>1/7/2025, 5:45:14 PM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>arXiv:2311.10833 [cs]</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.48550/arXiv.2311.10833">10.48550/arXiv.2311.10833</a></td>
					</tr>
					<tr>
					<th>Repository</th>
						<td>arXiv</td>
					</tr>
					<tr>
					<th>Archive ID</th>
						<td>arXiv:2311.10833</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>1/7/2025, 5:45:14 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Computer Science - Computers and Society</li>
					<li>Computer Science - Human-Computer Interaction</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_NUJ4DVA5">Preprint PDF					</li>
					<li id="item_3MCRFHWG">Snapshot					</li>
				</ul>
			</li>


			<li id="item_2N9V7VT3" class="item preprint">
			<h2>Generative AI Meets Open-Ended Survey Responses: Participant Use of AI and Homogenization</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Preprint</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Simone Zhang</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Janet Xu</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>A. J. Alvero</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>The growing popularity of generative AI tools presents new 
challenges for data quality in online surveys and experiments. This 
study examines participants’ use of large language models to answer 
open-ended survey questions and describes empirical tendencies in human 
vs LLM-generated text responses. In an original survey of participants 
recruited from a popular online platform for sourcing social science 
research subjects, 34% reported using LLMs to help them answer 
open-ended survey questions. Simulations comparing human-written 
responses from three pre-ChatGPT studies with LLM-generated text reveal 
that LLM responses are more homogeneous and positive, particularly when 
they describe social groups in sensitive questions. These homogenization
 patterns may mask important underlying social variation in attitudes 
and beliefs among human subjects, raising concerns about data validity. 
Our findings shed light on the scope and potential consequences of 
participants’ LLM use in online research.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2024-09-20</td>
					</tr>
					<tr>
					<th>Language</th>
						<td>en-us</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>Generative AI Meets Open-Ended Survey Responses</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>OSF Preprints</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://osf.io/4esdp">https://osf.io/4esdp</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>1/6/2025, 11:44:36 AM</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.31235/osf.io/4esdp">10.31235/osf.io/4esdp</a></td>
					</tr>
					<tr>
					<th>Repository</th>
						<td>OSF</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>1/6/2025, 11:44:36 AM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>attitudes</li>
					<li>data quality</li>
					<li>generative AI</li>
					<li>human-AI interaction</li>
					<li>large language models</li>
					<li>surveys</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_535MCIK3">OSF Preprint					</li>
				</ul>
			</li>


			<li id="item_YFHUC7EI" class="item preprint">
			<h2>Generative Artificial Intelligence (GenAI) in the research process – a survey of researchers’ practices and perceptions</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Preprint</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Jens Peter Andersen</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Lise Degn</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Rachel Fishberg</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Ebbe Krogh Graversen</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Serge P. J. M. Horbach</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Evanthia K. Schmidt</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Jesper W. Schneider</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Mads P. Sørensen</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>This study explores the use of generative AI (GenAI) and 
research integrity assessments of use cases by researchers, including 
PhD students, at Danish universities. Conducted through a survey sent to
 all Danish researchers from January to February 2024, the study 
received 2,534 responses and evaluated 32 GenAI use cases across five 
research phases: idea generation, research design, data collection, data
 analysis, and writing/reporting. Respondents reported on their own and 
colleagues' GenAI usage. They also assessed whether the practices in the
 use cases were considered good research practice. Through an 
explorative factor analysis, we identified three clusters of perception:
 "GenAI as a work horse," "GenAI as a language assistant only", and 
"GenAI as a research accelerator". 
The findings further show varied opinions on GenAI's research integrity 
implications. Language editing and data analysis were generally viewed 
positively, whereas experiment design and peer review tasks faced more 
criticism. Controversial areas included image creation/modification and 
synthetic data, with comments highlighting the need for critical and 
reflexive use of GenAI. Usage differed by main research area, with 
technical and quantitative sciences reporting slightly higher usage and 
more positive assessments. Junior researchers used GenAI more than 
senior colleagues, while no significant gender differences were 
observed.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2024-09-11</td>
					</tr>
					<tr>
					<th>Language</th>
						<td>en-us</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>OSF Preprints</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://osf.io/83whe">https://osf.io/83whe</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>1/6/2025, 11:22:01 AM</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.31235/osf.io/83whe">10.31235/osf.io/83whe</a></td>
					</tr>
					<tr>
					<th>Repository</th>
						<td>OSF</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>1/6/2025, 11:22:01 AM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>GenAI</li>
					<li>Generative Artificial Intelligence</li>
					<li>research integrity</li>
					<li>research practice</li>
					<li>research process</li>
					<li>use cases</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_TZLKCJS8">OSF Preprint					</li>
				</ul>
			</li>


			<li id="item_XFRDN3YF" class="item preprint">
			<h2>Generative Multimodal Models for Social Science: An Application with Satellite and Streetscape Imagery</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Preprint</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Tina Law</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Elizabeth Roberto</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Although there is growing social science research examining 
how generative AI (genAI) models can be effectively and systematically 
applied to text-based tasks, whether and how genAI can be used to 
analyze images remain open questions. In this article, we introduce an 
adapted framework that provides guidance for analyzing images with 
generative multimodal models (MMs) and consists of three core tasks: 
curation, discovery, and measurement and inference. We demonstrate this 
framework with an empirical application that uses OpenAI’s GPT-4o model 
to analyze satellite and streetscape images (n = 1,101) to identify 
built environment features that contribute to contemporary residential 
segregation in U.S. cities. We find that model-generated labels are more
 reliable than research assistant-generated labels and comparably valid 
to expert-generated labels. We conclude with thoughts for other use 
cases and discuss how social scientists can work collaboratively to 
ensure that image analysis with generative MMs is rigorous, 
reproducible, ethical, and sustainable.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2024-08-30</td>
					</tr>
					<tr>
					<th>Language</th>
						<td>en-us</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>Generative Multimodal Models for Social Science</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>OSF Preprints</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://osf.io/6jq32">https://osf.io/6jq32</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>1/6/2025, 11:21:00 AM</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.31235/osf.io/6jq32">10.31235/osf.io/6jq32</a></td>
					</tr>
					<tr>
					<th>Repository</th>
						<td>OSF</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>1/6/2025, 11:21:00 AM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>computational social science</li>
					<li>computer vision</li>
					<li>generative artificial intelligence</li>
					<li>image as data</li>
					<li>mulitmodal models</li>
					<li>prompt engineering</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_V7K3AMPM">OSF Preprint					</li>
				</ul>
			</li>


			<li id="item_97HMBED3" class="item journalArticle">
			<h2>Glitter or gold? Deriving structured insights from sustainability reports via large language models</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Marco Bronzini</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Carlo Nicolini</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Bruno Lepri</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Andrea Passerini</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Jacopo Staiano</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Over the last decade, several regulatory bodies have started 
requiring the disclosure of non-financial information from publicly 
listed companies, in light of the investors’ increasing attention to 
Environmental, Social, and Governance (ESG) issues. Publicly released 
information on sustainability practices is often disclosed in diverse, 
unstructured, and multi-modal documentation. This poses a challenge in 
efficiently gathering and aligning the data into a unified framework to 
derive insights related to Corporate Social Responsibility (CSR). Thus, 
using Information Extraction (IE) methods becomes an intuitive choice 
for delivering insightful and actionable data to stakeholders. In this 
study, we employ Large Language Models (LLMs), In-Context Learning, and 
the Retrieval-Augmented Generation (RAG) paradigm to extract structured 
insights related to ESG aspects from companies’ sustainability reports. 
We then leverage graph-based representations to conduct statistical 
analyses concerning the extracted insights. These analyses revealed that
 ESG criteria cover a wide range of topics, exceeding 500, often beyond 
those considered in existing categorizations, and are addressed by 
companies through a variety of initiatives. Moreover, disclosure 
similarities emerged among companies from the same region or sector, 
validating ongoing hypotheses in the ESG literature. Lastly, by 
incorporating additional company attributes into our analyses, we 
investigated which factors impact the most on companies’ ESG ratings, 
showing that ESG disclosure affects the obtained ratings more than other
 financial or company data.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2024/12</td>
					</tr>
					<tr>
					<th>Language</th>
						<td>en</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>Glitter or gold?</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>epjdatascience.springeropen.com</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://epjdatascience.springeropen.com/articles/10.1140/epjds/s13688-024-00481-2">https://epjdatascience.springeropen.com/articles/10.1140/epjds/s13688-024-00481-2</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>1/3/2025, 12:32:06 PM</td>
					</tr>
					<tr>
					<th>Rights</th>
						<td>2024 The Author(s)</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>Number: 1
Publisher: SpringerOpen</td>
					</tr>
					<tr>
					<th>Volume</th>
						<td>13</td>
					</tr>
					<tr>
					<th>Pages</th>
						<td>1-41</td>
					</tr>
					<tr>
					<th>Publication</th>
						<td>EPJ Data Science</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.1140/epjds/s13688-024-00481-2">10.1140/epjds/s13688-024-00481-2</a></td>
					</tr>
					<tr>
					<th>Issue</th>
						<td>1</td>
					</tr>
					<tr>
					<th>Journal Abbr</th>
						<td>EPJ Data Sci.</td>
					</tr>
					<tr>
					<th>ISSN</th>
						<td>2193-1127</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>1/3/2025, 12:32:06 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_7RRPX3DD">Full Text PDF					</li>
				</ul>
			</li>


			<li id="item_J5VMKNH5" class="item journalArticle">
			<h2>Googling Politics? Comparing Five Computational Methods to 
Identify Political and News-related Searches from Web Browser Histories</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Marieke van Hoof</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Damian Trilling</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Corine Meppelink</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Judith Möller</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Felicia Loecherbach</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Search engines play a crucial role in today’s information 
environment. Yet, political and news-related (PNR) search engine use 
remains understudied, mainly due to the lack of suitable measurement 
methods to identify PNR searches. Existing research focuses on specific 
events, topics, or news articles, neglecting the broader scope of PNR 
search. Furthermore, self-reporting issues have led researchers to use 
browsing history data, but scalable methods for analyzing such data are 
limited. This paper addresses these gaps by comparing five computational
 methods to identify PNR searches in browsing data, including browsing 
sequences, context-enhanced dictionary, Traditional Supervised Machine 
Learning (SML), Transformer-based SML, and zero-shot classification. 
Using Dutch Google searches as a test case, we use Dutch browsing 
history data obtained via data donations in May 2022 linked to surveys 
(Nusers = 315; Nrecords = 9,868,209; Nsearches = 697,359), along with 
35.5k manually annotated search terms. The findings highlight 
substantial variation in accuracy, with some methods being more suited 
for narrower topics. We recommend a two-step approach, applying 
zero-shot classification followed by human evaluation. This methodology 
can inform future empirical research on PNR search engine use.</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>Googling Politics?</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>Taylor and Francis+NEJM</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://doi.org/10.1080/19312458.2024.2363776">https://doi.org/10.1080/19312458.2024.2363776</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>1/6/2025, 2:23:42 PM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>Publisher: Routledge
_eprint: https://doi.org/10.1080/19312458.2024.2363776</td>
					</tr>
					<tr>
					<th>Volume</th>
						<td>0</td>
					</tr>
					<tr>
					<th>Pages</th>
						<td>1-27</td>
					</tr>
					<tr>
					<th>Publication</th>
						<td>Communication Methods and Measures</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.1080/19312458.2024.2363776">10.1080/19312458.2024.2363776</a></td>
					</tr>
					<tr>
					<th>Issue</th>
						<td>0</td>
					</tr>
					<tr>
					<th>ISSN</th>
						<td>1931-2458</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>1/6/2025, 2:23:42 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_RND9BYLW">Full Text PDF					</li>
				</ul>
			</li>


			<li id="item_W5XDNPZJ" class="item journalArticle">
			<h2>Graph embedding approaches for social media sentiment analysis with model explanation</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>V. S. Anoop</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>C. Subin Krishna</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Usharani Hareesh Govindarajan</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>ChatGPT, the revolutionary chat agent launched in November 
2022, is still an active topic of discussion among technology 
enthusiasts. This open-ended chatbot allows human-like conversations 
with it on almost all topics since it was trained on millions of 
documents and developed as a large language model. Since its inception, 
there have been several discussions and deliberations, especially on 
twitter and other social media handles, on the potential of ChatGPT and 
how the power of artificial intelligence is growing leaps and bounds in 
general. These platforms also witnessed several debates on the negative 
side of ChatGPT, such as adversely affecting the integrity and ethics 
and the biased training data. This work uses graph neural network 
embeddings with machine learning algorithms for classifying the user 
sentiments on ChatGPT. We have collected a total of 8202 tweets and 
manually labeled them into multiple classes such as positive, negative, 
and neutral. We make the models explainable using SHAP (SHapley Additive
 exPlanations), which is a game theoretical technique for explaining the
 output of any machine learning models. This paper also publishes our 
labeled dataset for other researchers to use and train advanced 
classification models. When our proposed approach was compared with some
 chosen baselines, the proposed graph embedding-based machine learning 
classifiers were found to be outperforming in terms of precision, 
recall, and accuracy.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2024-04-01</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>ScienceDirect</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://www.sciencedirect.com/science/article/pii/S2667096824000107">https://www.sciencedirect.com/science/article/pii/S2667096824000107</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>1/8/2025, 3:29:43 PM</td>
					</tr>
					<tr>
					<th>Volume</th>
						<td>4</td>
					</tr>
					<tr>
					<th>Pages</th>
						<td>100221</td>
					</tr>
					<tr>
					<th>Publication</th>
						<td>International Journal of Information Management Data Insights</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.1016/j.jjimei.2024.100221">10.1016/j.jjimei.2024.100221</a></td>
					</tr>
					<tr>
					<th>Issue</th>
						<td>1</td>
					</tr>
					<tr>
					<th>Journal Abbr</th>
						<td>International Journal of Information Management Data Insights</td>
					</tr>
					<tr>
					<th>ISSN</th>
						<td>2667-0968</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>1/8/2025, 3:29:43 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Explainability</li>
					<li>Explainable artificial intelligence</li>
					<li>Graph embedding</li>
					<li>Machine learning</li>
					<li>Natural language processing</li>
					<li>Sentiment analysis</li>
					<li>Social media</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_D7XUP8X4">ScienceDirect Snapshot					</li>
				</ul>
			</li>


			<li id="item_5KLACRHG" class="item journalArticle">
			<h2>Greasing the wheels for comparative communication research: Supervised text classification for multilingual corpora</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Fabienne Lind</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Tobias Heidenreich</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Christoph Kralj</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Hajo G. Boomgaarden</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Abstract Employing supervised machine learning for text 
classification is already a resource-intensive endeavor in a monolingual
 setting. However, facing the challenge to classify a multilingual 
corpus, the cost of producing the required annotated documents quickly 
exceeds even generous time and financial constraints. We show how tools 
like automated annotation and machine translation can not only 
efficiently but also effectively be employed for the classification of a
 multilingual corpus with supervised machine learning. Our findings 
demonstrate that good results can already be achieved with the machine 
translation of about 250 to 350 documents per category class and 
language and a dictionary in just one language, which we perceive as a 
realistic scenario for many projects. The methodological strategy is 
applied to study migration frames in seven languages (news discourse in 
seven European countries) and discussed and evaluated for its usability 
in comparative communication research.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2021/10/01</td>
					</tr>
					<tr>
					<th>Language</th>
						<td>en</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>Greasing the wheels for comparative communication research</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>www.aup-online.com</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://www.aup-online.com/content/journals/10.5117/CCR2021.3.001.LIND">https://www.aup-online.com/content/journals/10.5117/CCR2021.3.001.LIND</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>1/6/2025, 1:44:48 PM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>Publisher: Amsterdam University Press</td>
					</tr>
					<tr>
					<th>Volume</th>
						<td>3</td>
					</tr>
					<tr>
					<th>Publication</th>
						<td>Computational Communication Research</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.5117/CCR2021.3.001.LIND">10.5117/CCR2021.3.001.LIND</a></td>
					</tr>
					<tr>
					<th>Issue</th>
						<td>3</td>
					</tr>
					<tr>
					<th>ISSN</th>
						<td>2665-9085</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>1/6/2025, 1:44:48 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_78GBUQU2">Full Text					</li>
				</ul>
			</li>


			<li id="item_LKRYJMFN" class="item journalArticle">
			<h2>Ground truth tracings (GTT): On the epistemic limits of machine learning</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Edward B Kang</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>There is a gap in existing critical scholarship that engages 
with the ways in which current “machine listening” or voice 
analytics/biometric systems intersect with the technical specificities 
of machine learning. This article examines the sociotechnical assemblage
 of machine learning techniques, practices, and cultures that underlie 
these technologies. After engaging with various practitioners working in
 companies that develop machine listening systems, ranging from CEOs, 
machine learning engineers, data scientists, and business analysts, 
among others, I bring attention to the centrality of “learnability” as a
 malleable conceptual framework that bends according to various 
“ground-truthing” practices in formalizing certain listening-based 
prediction tasks for machine learning. In response, I introduce a 
process I call Ground Truth Tracings to examine the various ontological 
translations that occur in training a machine to “learn to listen.” 
Ultimately, by further examining this notion of learnability through the
 aperture of power, I take insights acquired through my fieldwork in the
 machine listening industry and propose a strategically reductive 
heuristic through which the epistemological and ethical soundness of 
machine learning, writ large, can be contemplated.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2023-01-01</td>
					</tr>
					<tr>
					<th>Language</th>
						<td>en</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>Ground truth tracings (GTT)</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>SAGE Journals</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://doi.org/10.1177/20539517221146122">https://doi.org/10.1177/20539517221146122</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>12/23/2024, 5:43:29 PM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>Publisher: SAGE Publications Ltd</td>
					</tr>
					<tr>
					<th>Volume</th>
						<td>10</td>
					</tr>
					<tr>
					<th>Pages</th>
						<td>20539517221146122</td>
					</tr>
					<tr>
					<th>Publication</th>
						<td>Big Data &amp; Society</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.1177/20539517221146122">10.1177/20539517221146122</a></td>
					</tr>
					<tr>
					<th>Issue</th>
						<td>1</td>
					</tr>
					<tr>
					<th>ISSN</th>
						<td>2053-9517</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>12/23/2024, 5:43:29 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_AS6PILW7">SAGE PDF Full Text					</li>
				</ul>
			</li>


			<li id="item_G2H49RND" class="item preprint">
			<h2>Hidden Persuaders: LLMs' Political Leaning and Their Influence on Voters</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Preprint</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Yujin Potter</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Shiyang Lai</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Junsol Kim</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>James Evans</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Dawn Song</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>How could LLMs influence our democracy? We investigate LLMs' 
political leanings and the potential influence of LLMs on voters by 
conducting multiple experiments in a U.S. presidential election context.
 Through a voting simulation, we first demonstrate 18 open- and 
closed-weight LLMs' political preference for a Democratic nominee over a
 Republican nominee. We show how this leaning towards the Democratic 
nominee becomes more pronounced in instruction-tuned models compared to 
their base versions by analyzing their responses to candidate-policy 
related questions. We further explore the potential impact of LLMs on 
voter choice by conducting an experiment with 935 U.S. registered 
voters. During the experiments, participants interacted with LLMs 
(Claude-3, Llama-3, and GPT-4) over five exchanges. The experiment 
results show a shift in voter choices towards the Democratic nominee 
following LLM interaction, widening the voting margin from 0.7% to 4.6%,
 even though LLMs were not asked to persuade users to support the 
Democratic nominee during the discourse. This effect is larger than many
 previous studies on the persuasiveness of political campaigns, which 
have shown minimal effects in presidential elections. Many users also 
expressed a desire for further political interaction with LLMs. Which 
aspects of LLM interactions drove these shifts in voter choice requires 
further study. Lastly, we explore how a safety method can make LLMs more
 politically neutral, while raising the question of whether such 
neutrality is truly the path forward.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2024-11-11</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>Hidden Persuaders</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>arXiv.org</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://arxiv.org/abs/2410.24190">http://arxiv.org/abs/2410.24190</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>12/24/2024, 4:56:43 PM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>arXiv:2410.24190 [cs]</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.48550/arXiv.2410.24190">10.48550/arXiv.2410.24190</a></td>
					</tr>
					<tr>
					<th>Repository</th>
						<td>arXiv</td>
					</tr>
					<tr>
					<th>Archive ID</th>
						<td>arXiv:2410.24190</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>12/24/2024, 4:56:43 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Computer Science - Computation and Language</li>
					<li>Computer Science - Computers and Society</li>
				</ul>
				<h3 class="notes">Notes:</h3>
				<ul class="notes">
					<li id="item_9KRG3UCK">
<p class="plaintext">Comment: EMNLP 2024 Main</p>
					</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_AENPYKTW">Preprint PDF					</li>
					<li id="item_VWYFD8PS">Snapshot					</li>
				</ul>
			</li>


			<li id="item_TCDISQ5Y" class="item preprint">
			<h2>Hierarchical Narrative Analysis: Unraveling Perceptions of Generative AI</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Preprint</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Riona Matsuoka</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Hiroki Matsumoto</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Takahiro Yoshida</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Tomohiro Watanabe</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Ryoma Kondo</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Ryohei Hisano</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Written texts reflect an author's perspective, making the 
thorough analysis of literature a key research method in fields such as 
the humanities and social sciences. However, conventional text mining 
techniques like sentiment analysis and topic modeling are limited in 
their ability to capture the hierarchical narrative structures that 
reveal deeper argumentative patterns. To address this gap, we propose a 
method that leverages large language models (LLMs) to extract and 
organize these structures into a hierarchical framework. We validate 
this approach by analyzing public opinions on generative AI collected by
 Japan's Agency for Cultural Affairs, comparing the narratives of 
supporters and critics. Our analysis provides clearer visualization of 
the factors influencing divergent opinions on generative AI, offering 
deeper insights into the structures of agreement and disagreement.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2024-11-11</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>Hierarchical Narrative Analysis</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>arXiv.org</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://arxiv.org/abs/2409.11032">http://arxiv.org/abs/2409.11032</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>1/7/2025, 5:07:52 PM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>arXiv:2409.11032 [cs]</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.48550/arXiv.2409.11032">10.48550/arXiv.2409.11032</a></td>
					</tr>
					<tr>
					<th>Repository</th>
						<td>arXiv</td>
					</tr>
					<tr>
					<th>Archive ID</th>
						<td>arXiv:2409.11032</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>1/7/2025, 5:07:52 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Computer Science - Computation and Language</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_UFNIZ3VK">Preprint PDF					</li>
					<li id="item_XCUIKHG8">Snapshot					</li>
				</ul>
			</li>


			<li id="item_BYSN843Z" class="item preprint">
			<h2>How do Generative Language Models Answer Opinion Polls?</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Preprint</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Julien Boelaert</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Samuel Coavoux</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Etienne Ollion</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Ivaylo D. Petev</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Patrick Präg</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Generative AI is increasingly presented as a potential 
substitute for humans, including as human research subjects in various 
disciplines. Yet there is no scientific consensus on how closely these 
in-silico clones could represent their human counterparts. While some 
defend the use of these “synthetic users,” others point towards the 
biases in the responses provided by the LLMs. Through an experiment 
using survey questionnaires, we demonstrate that these latter critics 
are right to be wary of using generative AI to emulate respondents, but 
probably not for the right reason. Our results i) confirm that to date, 
models cannot replace research subjects for opinion or attitudinal 
research; ii) that they display a strong bias on each question (reaching
 only a small region of social space); and iii) that this bias varies 
randomly from one question to the other (reaching a different region 
every time). Besides the two existing competing theses 
(“representativity” and “social bias”), we propose a third one, which we
 call call “machine bias”. We detail this term and explore its 
consequences, for LLM research but also for studies on social biases.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2024-04-25</td>
					</tr>
					<tr>
					<th>Language</th>
						<td>en-us</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>OSF Preprints</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://osf.io/r2pnb">https://osf.io/r2pnb</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>1/3/2025, 4:02:42 PM</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.31235/osf.io/r2pnb">10.31235/osf.io/r2pnb</a></td>
					</tr>
					<tr>
					<th>Repository</th>
						<td>OSF</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>1/3/2025, 4:02:42 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_I9HJQ4I7">OSF Preprint					</li>
				</ul>
			</li>


			<li id="item_ZRRT5JWA" class="item preprint">
			<h2>How the General Benefit Reform Emerged in Finland: A Critical Analysis Using Large Language Models in Policy Research</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Preprint</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Pasi Moisio</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Merita Mesiäislehto</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Johanna Peltoniemi</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Mika Pihlajamäki</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Heikki Hiilamo</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Utilizing Large Language Models (LLM), this study investigates
 the evolution of an innovative social security policy idea, the General
 Benefit concept into a policy reform proposal in Fin-land from 2007 to 
2023. Drawing from the ideational analysis we hypothesize that political
 parties struggled over social security conditionality during the 2010s 
and that social security simplification was manipulated differently in 
relation to conditionality. Our primary data is elec-tion manifestos and
 governmental programs from 2007-2023. We employed LLMs, mainly a 
customized ChatGPT, for the text analysis of policy documents. 
Additionally, we conduct a critical human evaluation of the LLMs 
analysis and publish our model in the GPT store for the open replication
 of analyses. 

Findings indicate that the weakening of the tripartite industrial 
relations system and the break-ing of “status quo of three big parties” 
allowed new parties to influence social policy in 2010s. The General 
Benefit emerged as a response to calls for social security 
simplification and for countering (unconditional) basic income 
proposals. Adopted in 2023, the General Benefit concept aims to merge 
Finnish universal / residence-based social insurance benefits for the 
working-aged while preserving core principles like social risk 
categories and conditionality. Despite increased nativism from the 
rising True Finns party, and the adoption of universal / unconditional 
basic income by several parties, Finnish social policy trends from 2007 
to 2023 continued to emphasize employment and public finance 
sustainability.

Our study also contributes to methodological discussions on using LLMs 
in policy analysis. The “human evaluation”, performed by the authors, 
confirms that the LLM analysis accurately summarises the main features 
of the policy evolution. However, we also found that the LLM lacks 
ability to recognise the nuances of “multidimensional” political 
language and is not very helpful in cross-sectional evaluation, which 
leaves the analysis partly shallow. Thus, we con-clude that in 
qualitative policy analysis, LLMs in their current form are suitable for
 comple-menting rather than substituting human evaluation.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2024-10-10</td>
					</tr>
					<tr>
					<th>Language</th>
						<td>en-us</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>How the General Benefit Reform Emerged in Finland</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>OSF Preprints</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://osf.io/ab8mr">https://osf.io/ab8mr</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>1/3/2025, 3:54:51 PM</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.31235/osf.io/ab8mr">10.31235/osf.io/ab8mr</a></td>
					</tr>
					<tr>
					<th>Repository</th>
						<td>OSF</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>1/3/2025, 3:54:51 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Basic Income</li>
					<li>Large Language Models (LLMs)</li>
					<li>Nordic Welfare State</li>
					<li>Policy Analysis</li>
					<li>Policy Reform</li>
					<li>Social Security</li>
					<li>Universal Credit</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_N6PUS3GH">OSF Preprint					</li>
				</ul>
			</li>


			<li id="item_549KJBA7" class="item preprint">
			<h2>How to Fine-Tune BERT for Text Classification?</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Preprint</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Chi Sun</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Xipeng Qiu</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Yige Xu</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Xuanjing Huang</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Language model pre-training has proven to be useful in 
learning universal language representations. As a state-of-the-art 
language model pre-training model, BERT (Bidirectional Encoder 
Representations from Transformers) has achieved amazing results in many 
language understanding tasks. In this paper, we conduct exhaustive 
experiments to investigate different fine-tuning methods of BERT on text
 classification task and provide a general solution for BERT 
fine-tuning. Finally, the proposed solution obtains new state-of-the-art
 results on eight widely-studied text classification datasets.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2020-02-05</td>
					</tr>
					<tr>
					<th>Language</th>
						<td>en</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>arXiv.org</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://arxiv.org/abs/1905.05583">http://arxiv.org/abs/1905.05583</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>5/6/2024, 12:07:51 PM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>arXiv:1905.05583 [cs]</td>
					</tr>
					<tr>
					<th>Repository</th>
						<td>arXiv</td>
					</tr>
					<tr>
					<th>Archive ID</th>
						<td>arXiv:1905.05583</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>5/6/2024, 12:07:51 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>5/6/2024, 12:07:51 PM</td>
					</tr>
				</tbody></table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Computer Science - Computation and Language</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_56CFWL2S">Sun et al. - 2020 - How to Fine-Tune BERT for Text Classification.pdf					</li>
				</ul>
			</li>


			<li id="item_T5RFB954" class="item preprint">
			<h2>How to use LLMs for Text Analysis</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Preprint</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Petter Törnberg</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>This guide introduces Large Language Models (LLM) as a highly 
versatile text analysis method within the social sciences. As LLMs are 
easy-to-use, cheap, fast, and applicable on a broad range of text 
analysis tasks, ranging from text annotation and classification to 
sentiment analysis and critical discourse analysis, many scholars 
believe that LLMs will transform how we do text analysis. This how-to 
guide is aimed at students and researchers with limited programming 
experience, and offers a simple introduction to how LLMs can be used for
 text analysis in your own research project, as well as advice on best 
practices. We will go through each of the steps of analyzing textual 
data with LLMs using Python: installing the software, setting up the 
API, loading the data, developing an analysis prompt, analyzing the 
text, and validating the results. As an illustrative example, we will 
use the challenging task of identifying populism in political texts, and
 show how LLMs move beyond the existing state-of-the-art.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2023-07-24</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>arXiv.org</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://arxiv.org/abs/2307.13106">http://arxiv.org/abs/2307.13106</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>1/7/2025, 5:38:26 PM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>arXiv:2307.13106 [cs]</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.48550/arXiv.2307.13106">10.48550/arXiv.2307.13106</a></td>
					</tr>
					<tr>
					<th>Repository</th>
						<td>arXiv</td>
					</tr>
					<tr>
					<th>Archive ID</th>
						<td>arXiv:2307.13106</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>1/7/2025, 5:38:26 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Computer Science - Artificial Intelligence</li>
					<li>Computer Science - Computation and Language</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_6HU3TF34">Preprint PDF					</li>
					<li id="item_EIU5IHTY">Snapshot					</li>
				</ul>
			</li>


			<li id="item_QJZAYGCA" class="item bookSection">
			<h2>How We Code</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Book Section</td>
					</tr>
					<tr>
						<th class="editor">Editor</th>
						<td>Andrew R. Ruis</td>
					</tr>
					<tr>
						<th class="editor">Editor</th>
						<td>Seung B. Lee</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>David Williamson Shaffer</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>A. R. Ruis</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Coding data—deﬁning concepts and identifying where they occur 
in data—is a critical aspect of qualitative data analysis, and 
especially so in quantitative ethnography. Coding is a central process 
for creating meaning from data, and while much has been written about 
coding methods and theory, relatively little has been written about what
 constitutes best practices for fair and valid coding, what justiﬁes 
those practices, and how to implement them. In this paper, our goal is 
not to address these issues comprehensively, but to provide guidelines 
for good coding practice and to highlight some of the issues and key 
questions that quantitative ethnographers and other researchers should 
consider when coding data.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2021</td>
					</tr>
					<tr>
					<th>Language</th>
						<td>en</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>DOI.org (Crossref)</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://link.springer.com/10.1007/978-3-030-67788-6_5">https://link.springer.com/10.1007/978-3-030-67788-6_5</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>4/28/2024, 8:52:10 PM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>Series Title: Communications in Computer and Information Science
DOI: 10.1007/978-3-030-67788-6_5</td>
					</tr>
					<tr>
					<th>Volume</th>
						<td>1312</td>
					</tr>
					<tr>
					<th>Place</th>
						<td>Cham</td>
					</tr>
					<tr>
					<th>Publisher</th>
						<td>Springer International Publishing</td>
					</tr>
					<tr>
					<th>ISBN</th>
						<td>978-3-030-67787-9 978-3-030-67788-6</td>
					</tr>
					<tr>
					<th>Pages</th>
						<td>62-77</td>
					</tr>
					<tr>
					<th>Book Title</th>
						<td>Advances in Quantitative Ethnography</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>4/28/2024, 8:52:10 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>4/28/2024, 8:52:10 PM</td>
					</tr>
				</tbody></table>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_ULQKHSU4">Shaffer and Ruis - 2021 - How We Code.pdf					</li>
				</ul>
			</li>


			<li id="item_RAVTV7HG" class="item preprint">
			<h2>Human Evaluation of Interpretability: The Case of AI-Generated Music Knowledge</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Preprint</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Haizi Yu</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Heinrich Taube</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>James A. Evans</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Lav R. Varshney</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Interpretability of machine learning models has gained more 
and more attention among researchers in the artificial intelligence (AI)
 and human-computer interaction (HCI) communities. Most existing work 
focuses on decision making, whereas we consider knowledge discovery. In 
particular, we focus on evaluating AI-discovered knowledge/rules in the 
arts and humanities. From a specific scenario, we present an 
experimental procedure to collect and assess human-generated verbal 
interpretations of AI-generated music theory/rules rendered as 
sophisticated symbolic/numeric objects. Our goal is to reveal both the 
possibilities and the challenges in such a process of decoding 
expressive messages from AI sources. We treat this as a first step 
towards 1) better design of AI representations that are human 
interpretable and 2) a general methodology to evaluate interpretability 
of AI-discovered knowledge representations.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2020-04-15</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>Human Evaluation of Interpretability</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>arXiv.org</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://arxiv.org/abs/2004.06894">http://arxiv.org/abs/2004.06894</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>1/8/2025, 1:31:32 PM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>arXiv:2004.06894 [cs]</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.48550/arXiv.2004.06894">10.48550/arXiv.2004.06894</a></td>
					</tr>
					<tr>
					<th>Repository</th>
						<td>arXiv</td>
					</tr>
					<tr>
					<th>Archive ID</th>
						<td>arXiv:2004.06894</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>1/8/2025, 1:31:32 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Computer Science - Artificial Intelligence</li>
					<li>Computer Science - Human-Computer Interaction</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_IGFCU3YT">Preprint PDF					</li>
					<li id="item_Z2LFSIBV">Snapshot					</li>
				</ul>
			</li>


			<li id="item_JFAGCLHI" class="item preprint">
			<h2>Human Simulacra: Benchmarking the Personification of Large Language Models</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Preprint</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Qiuejie Xie</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Qiming Feng</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Tianqi Zhang</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Qingqiu Li</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Linyi Yang</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Yuejie Zhang</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Rui Feng</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Liang He</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Shang Gao</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Yue Zhang</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Large language models (LLMs) are recognized as systems that 
closely mimic aspects of human intelligence. This capability has 
attracted attention from the social science community, who see the 
potential in leveraging LLMs to replace human participants in 
experiments, thereby reducing research costs and complexity. In this 
paper, we introduce a framework for large language models 
personification, including a strategy for constructing virtual 
characters' life stories from the ground up, a Multi-Agent Cognitive 
Mechanism capable of simulating human cognitive processes, and a 
psychology-guided evaluation method to assess human simulations from 
both self and observational perspectives. Experimental results 
demonstrate that our constructed simulacra can produce personified 
responses that align with their target characters. Our work is a 
preliminary exploration which offers great potential in practical 
applications. All the code and datasets will be released, with the hope 
of inspiring further investigations.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2024-06-10</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>Human Simulacra</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>arXiv.org</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://arxiv.org/abs/2402.18180">http://arxiv.org/abs/2402.18180</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>1/7/2025, 5:30:29 PM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>arXiv:2402.18180 [cs]</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.48550/arXiv.2402.18180">10.48550/arXiv.2402.18180</a></td>
					</tr>
					<tr>
					<th>Repository</th>
						<td>arXiv</td>
					</tr>
					<tr>
					<th>Archive ID</th>
						<td>arXiv:2402.18180</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>1/7/2025, 5:30:29 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Computer Science - Computers and Society</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_M7Q2ZSJ8">Preprint PDF					</li>
					<li id="item_G35Q9S2A">Snapshot					</li>
				</ul>
			</li>


			<li id="item_WGQFL4VL" class="item journalArticle">
			<h2>Human-AI coevolution</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Dino Pedreschi</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Luca Pappalardo</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Emanuele Ferragina</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Ricardo Baeza-Yates</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Albert-László Barabási</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Frank Dignum</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Virginia Dignum</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Tina Eliassi-Rad</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Fosca Giannotti</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>János Kertész</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Alistair Knott</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Yannis Ioannidis</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Paul Lukowicz</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Andrea Passarella</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Alex Sandy Pentland</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>John Shawe-Taylor</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Alessandro Vespignani</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Human-AI coevolution, defined as a process in which humans and
 AI algorithms continuously influence each other, increasingly 
characterises our society, but is understudied in artificial 
intelligence and complexity science literature. Recommender systems and 
assistants play a prominent role in human-AI coevolution, as they 
permeate many facets of daily life and influence human choices through 
online platforms. The interaction between users and AI results in a 
potentially endless feedback loop, wherein users' choices generate data 
to train AI models, which, in turn, shape subsequent user preferences. 
This human-AI feedback loop has peculiar characteristics compared to 
traditional human-machine interaction and gives rise to complex and 
often “unintended” systemic outcomes. This paper introduces human-AI 
coevolution as the cornerstone for a new field of study at the 
intersection between AI and complexity science focused on the 
theoretical, empirical, and mathematical investigation of the human-AI 
feedback loop. In doing so, we: (i) outline the pros and cons of 
existing methodologies and highlight shortcomings and potential ways for
 capturing feedback loop mechanisms; (ii) propose a reflection at the 
intersection between complexity science, AI and society; (iii) provide 
real-world examples for different human-AI ecosystems; and (iv) 
illustrate challenges to the creation of such a field of study, 
conceptualising them at increasing levels of abstraction, i.e., 
scientific, legal and socio-political.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2025-02-01</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>ScienceDirect</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://www.sciencedirect.com/science/article/pii/S0004370224001802">https://www.sciencedirect.com/science/article/pii/S0004370224001802</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>1/8/2025, 3:28:00 PM</td>
					</tr>
					<tr>
					<th>Volume</th>
						<td>339</td>
					</tr>
					<tr>
					<th>Pages</th>
						<td>104244</td>
					</tr>
					<tr>
					<th>Publication</th>
						<td>Artificial Intelligence</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.1016/j.artint.2024.104244">10.1016/j.artint.2024.104244</a></td>
					</tr>
					<tr>
					<th>Journal Abbr</th>
						<td>Artificial Intelligence</td>
					</tr>
					<tr>
					<th>ISSN</th>
						<td>0004-3702</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>1/8/2025, 3:28:00 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Artificial intelligence</li>
					<li>Complex systems</li>
					<li>Computational social science</li>
					<li>Human-AI coevolution</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_KW25KNWS">ScienceDirect Snapshot					</li>
				</ul>
			</li>


			<li id="item_QXY38QT4" class="item preprint">
			<h2>Human-interpretable clustering of short-text using large language models</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Preprint</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Justin K. Miller</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Tristram J. Alexander</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Clustering short text is a difficult problem, due to the low 
word co-occurrence between short text documents. This work shows that 
large language models (LLMs) can overcome the limitations of traditional
 clustering approaches by generating embeddings that capture the 
semantic nuances of short text. In this study clusters are found in the 
embedding space using Gaussian Mixture Modelling (GMM). The resulting 
clusters are found to be more distinctive and more human-interpretable 
than clusters produced using the popular methods of doc2vec and Latent 
Dirichlet Allocation (LDA). The success of the clustering approach is 
quantified using human reviewers and through the use of a generative 
LLM. The generative LLM shows good agreement with the human reviewers, 
and is suggested as a means to bridge the `validation gap' which often 
exists between cluster production and cluster interpretation. The 
comparison between LLM-coding and human-coding reveals intrinsic biases 
in each, challenging the conventional reliance on human coding as the 
definitive standard for cluster validation.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2024-12-14</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>arXiv.org</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://arxiv.org/abs/2405.07278">http://arxiv.org/abs/2405.07278</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>1/8/2025, 3:21:58 PM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>arXiv:2405.07278 [cs]</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.48550/arXiv.2405.07278">10.48550/arXiv.2405.07278</a></td>
					</tr>
					<tr>
					<th>Repository</th>
						<td>arXiv</td>
					</tr>
					<tr>
					<th>Archive ID</th>
						<td>arXiv:2405.07278</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>1/8/2025, 3:21:58 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Computer Science - Computation and Language</li>
					<li>Computer Science - Machine Learning</li>
				</ul>
				<h3 class="notes">Notes:</h3>
				<ul class="notes">
					<li id="item_XHDRIR4T">
<p class="plaintext">Comment: Main text: 18 pages, 6 figures. Supplementary: 21 pages, 15 figures, 3 tables</p>
					</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_MKXPTXK9">Preprint PDF					</li>
					<li id="item_JWXCS4TZ">Snapshot					</li>
				</ul>
			</li>


			<li id="item_H7UV9DDL" class="item journalArticle">
			<h2>Hybrid Predictive Ensembles: Synergies Between Human and Computational Forecasts</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Lu Hong</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>P. J. Lamberson</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Scott E. Page</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>&lt;p&gt;An increasing proportion of decisions, design 
choices, and predictions are being made by hybrid groups consisting of 
humans and artificial intelligence (AI). In this paper, we provide 
analytic foundations that explain the potential benefits of hybrid 
groups on predictive tasks, the primary use of AI. Our analysis relies 
on interpretive and generative signal frameworks as well as a 
distinction between the big data used by AI and the thick, often 
narrative data used by humans. We derive several conditions on accuracy 
and correlation necessary for humans to remain in the loop. We conclude 
that human adaptability along with the potential for atypical cases that
 mislead AI will likely mean that humans always add value on predictive 
tasks.&lt;/p&gt;</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2021/06/01</td>
					</tr>
					<tr>
					<th>Language</th>
						<td>en</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>Hybrid Predictive Ensembles</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>www.sciopen.com</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://www.sciopen.com/article/10.23919/JSC.2021.0009">https://www.sciopen.com/article/10.23919/JSC.2021.0009</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>12/24/2024, 4:37:50 PM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>Publisher: 清华大学出版社</td>
					</tr>
					<tr>
					<th>Volume</th>
						<td>2</td>
					</tr>
					<tr>
					<th>Pages</th>
						<td>89-102</td>
					</tr>
					<tr>
					<th>Publication</th>
						<td>Journal of Social Computing</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.23919/JSC.2021.0009">10.23919/JSC.2021.0009</a></td>
					</tr>
					<tr>
					<th>Issue</th>
						<td>2</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>12/24/2024, 4:37:50 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_ZEF86B8V">Full Text PDF					</li>
				</ul>
			</li>


			<li id="item_FGP362KJ" class="item preprint">
			<h2>Improving and Assessing the Fidelity of Large Language Models Alignment to Online Communities</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Preprint</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Minh Duc Chu</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Zihao He</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Rebecca Dorn</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Kristina Lerman</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Large language models (LLMs) have shown promise in 
representing individuals and communities, offering new ways to study 
complex social dynamics. However, effectively aligning LLMs with 
specific human groups and systematically assessing the fidelity of the 
alignment remains a challenge. This paper presents a robust framework 
for aligning LLMs with online communities via instruction-tuning and 
comprehensively evaluating alignment across various aspects of language,
 including authenticity, emotional tone, toxicity, and harm. We 
demonstrate the utility of our approach by applying it to online 
communities centered on dieting and body image. We administer an eating 
disorder psychometric test to the aligned LLMs to reveal unhealthy 
beliefs and successfully differentiate communities with varying levels 
of eating disorder risk. Our results highlight the potential of LLMs in 
automated moderation and broader applications in public health and 
social science research.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2024-08-18</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>arXiv.org</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://arxiv.org/abs/2408.09366">http://arxiv.org/abs/2408.09366</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>1/7/2025, 5:12:26 PM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>arXiv:2408.09366 [cs]</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.48550/arXiv.2408.09366">10.48550/arXiv.2408.09366</a></td>
					</tr>
					<tr>
					<th>Repository</th>
						<td>arXiv</td>
					</tr>
					<tr>
					<th>Archive ID</th>
						<td>arXiv:2408.09366</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>1/7/2025, 5:12:26 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Computer Science - Computation and Language</li>
					<li>Computer Science - Computers and Society</li>
					<li>Computer Science - Social and Information Networks</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_QNGZAKFG">Preprint PDF					</li>
					<li id="item_DTVTRMQ6">Snapshot					</li>
				</ul>
			</li>


			<li id="item_RZWPN6KQ" class="item journalArticle">
			<h2>In generative AI we trust: can chatbots effectively verify political information?</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Elizaveta Kuznetsova</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Mykola Makhortykh</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Victoria Vziatysheva</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Martha Stolze</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Ani Baghumyan</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Aleksandra Urman</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>This article presents a comparative analysis of the potential 
of two large language model (LLM)-based chatbots—ChatGPT and Bing Chat 
(recently rebranded to Microsoft Copilot)—to detect veracity of 
political information. We use AI auditing methodology to investigate how
 chatbots evaluate true, false, and borderline statements on five 
topics: COVID-19, Russian aggression against Ukraine, the Holocaust, 
climate change, and LGBTQ + -related debates. We compare how the 
chatbots respond in high- and low-resource languages by using prompts in
 English, Russian, and Ukrainian. Furthermore, we explore chatbots’ 
ability to evaluate statements according to political communication 
concepts of disinformation, misinformation, and conspiracy theory, using
 definition-oriented prompts. We also systematically test how such 
evaluations are influenced by source attribution. The results show high 
potential of ChatGPT for the baseline veracity evaluation task, with 72%
 of the cases evaluated in accordance with the baseline on average 
across languages without pre-training. Bing Chat evaluated 67% of the 
cases in accordance with the baseline. We observe significant 
disparities in how chatbots evaluate prompts in high- and low-resource 
languages and how they adapt their evaluations to political 
communication concepts with ChatGPT providing more nuanced outputs than 
Bing Chat. These findings highlight the potential of LLM-based chatbots 
in tackling different forms of false information in online environments,
 but also point to the substantial variation in terms of how such 
potential is realized due to specific factors (e.g. language of the 
prompt or the topic).</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2024-12-17</td>
					</tr>
					<tr>
					<th>Language</th>
						<td>en</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>In generative AI we trust</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>Springer Link</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://doi.org/10.1007/s42001-024-00338-8">https://doi.org/10.1007/s42001-024-00338-8</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>1/3/2025, 2:05:21 PM</td>
					</tr>
					<tr>
					<th>Volume</th>
						<td>8</td>
					</tr>
					<tr>
					<th>Pages</th>
						<td>15</td>
					</tr>
					<tr>
					<th>Publication</th>
						<td>Journal of Computational Social Science</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.1007/s42001-024-00338-8">10.1007/s42001-024-00338-8</a></td>
					</tr>
					<tr>
					<th>Issue</th>
						<td>1</td>
					</tr>
					<tr>
					<th>Journal Abbr</th>
						<td>J Comput Soc Sc</td>
					</tr>
					<tr>
					<th>ISSN</th>
						<td>2432-2725</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>1/3/2025, 2:05:21 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>AI audit</li>
					<li>Artificial Intelligence</li>
					<li>Conspiracy theory</li>
					<li>Disinformation</li>
					<li>LLMs</li>
					<li>Misinformation</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_F8RBA48Y">Full Text PDF					</li>
				</ul>
			</li>


			<li id="item_HQ6FWWVU" class="item preprint">
			<h2>In Silico Sociology: Forecasting COVID-19 Polarization with Large Language Models</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Preprint</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Austin Kozlowski</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Hyunku Kwon</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>James Evans</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>By training deep neural networks on massive archives of 
digitized text, large language models (LLMs) learn the complex 
linguistic patterns that constitute historic and contemporary 
discourses. We argue that LLMs can serve as a valuable tool for 
sociological inquiry by enabling accurate simulation of respondents from
 specific social and cultural contexts. Applying LLMs in this capacity, 
we reconstruct the public opinion landscape of 2019 to examine the 
extent to which the future polarization over COVID-19 was prefigured in 
existing political discourse. Using an LLM trained on texts published 
through 2019, we simulate the responses of American liberals and 
conservatives to a battery of pandemic-related questions. We find that 
the simulated respondents reproduce observed partisan differences in 
COVID-19 attitudes in 84% of cases, significantly greater than chance. 
Prompting the simulated respondents to justify their responses, we find 
that much of the observed partisan gap corresponds to differing appeals 
to freedom, safety, and institutional trust. Our findings suggest that 
the politicization of COVID-19 was largely consistent with the prior 
ideological landscape, and this unprecedented event served to advance 
history along its track rather than change the rails.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2024-04-25</td>
					</tr>
					<tr>
					<th>Language</th>
						<td>en-us</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>In Silico Sociology</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>OSF Preprints</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://osf.io/7dfbc">https://osf.io/7dfbc</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>1/3/2025, 4:00:37 PM</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.31235/osf.io/7dfbc">10.31235/osf.io/7dfbc</a></td>
					</tr>
					<tr>
					<th>Repository</th>
						<td>OSF</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>1/3/2025, 4:00:37 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>AI</li>
					<li>Culture</li>
					<li>Large Language Models</li>
					<li>Polarization</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_T7U9U8KW">OSF Preprint					</li>
				</ul>
			</li>


			<li id="item_QLGHLGGX" class="item preprint">
			<h2>Instruction Tuning Vs. In-Context Learning: Revisiting Large Language Models in Few-Shot Computational Social Science</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Preprint</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Taihang Wang</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Xiaoman Xu</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Yimin Wang</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Ye Jiang</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Real-world applications of large language models (LLMs) in 
computational social science (CSS) tasks primarily depend on the 
effectiveness of instruction tuning (IT) or in-context learning (ICL). 
While IT has shown highly effective at fine-tuning LLMs for various 
tasks, ICL offers a rapid alternative for task adaptation by learning 
from examples without explicit gradient updates. In this paper, we 
evaluate the classification performance of LLMs using IT versus ICL in 
few-shot CSS tasks. The experimental results indicate that ICL 
consistently outperforms IT in most CSS tasks. Additionally, we 
investigate the relationship between the increasing number of training 
samples and LLM performance. Our findings show that simply increasing 
the number of samples without considering their quality does not 
consistently enhance the performance of LLMs with either ICL or IT and 
can sometimes even result in a performance decline. Finally, we compare 
three prompting strategies, demonstrating that ICL is more effective 
than zero-shot and Chain-of-Thought (CoT). Our research highlights the 
significant advantages of ICL in handling CSS tasks in few-shot settings
 and emphasizes the importance of optimizing sample quality and 
prompting strategies to improve LLM classification performance. The code
 will be made available.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2024-09-23</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>Instruction Tuning Vs. In-Context Learning</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>arXiv.org</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://arxiv.org/abs/2409.14673">http://arxiv.org/abs/2409.14673</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>1/7/2025, 5:06:19 PM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>arXiv:2409.14673 [cs]</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.48550/arXiv.2409.14673">10.48550/arXiv.2409.14673</a></td>
					</tr>
					<tr>
					<th>Repository</th>
						<td>arXiv</td>
					</tr>
					<tr>
					<th>Archive ID</th>
						<td>arXiv:2409.14673</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>1/7/2025, 5:06:19 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Computer Science - Artificial Intelligence</li>
					<li>Computer Science - Computation and Language</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_Y9TXJPK5">Preprint PDF					</li>
					<li id="item_7MMNSWKL">Snapshot					</li>
				</ul>
			</li>


			<li id="item_N8MZDDIF" class="item journalArticle">
			<h2>Integrating Human Insights Into Text Analysis: Semi-Supervised 
Topic Modeling of Emerging Food-Technology Businesses’ Brand 
Communication on Social Media</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Leona Yi-Fan Su</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Tianli Chen</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Yee Man Margaret Ng</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Ziyang Gong</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Yi-Cheng Wang</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Textual social media data have become indispensable to 
researchers’ understanding of message strategies and other marketing 
practices. In a new departure for the field of brand communication, this
 study adopts and extends a semi-supervised machine-learning approach, 
guided latent Dirichlet allocation (LDA), which incorporates human 
insights into the discovery and classification of topics. We used it to 
analyze tweets from businesses involved with an emerging food 
technology, cultured meat, and delineated four key message strategies 
used by these brands: providing functional, educational, corporate 
social responsibility, and relational content. We further ascertained 
the relationships between brands and the key topics embedded in their 
Twitter data. A comparison of model performance suggests that guided LDA
 can be an advantageous alternative to traditional LDA, which is 
characterized by high efficiency and immense popularity among 
researchers, but—because of its unsupervised nature—yields findings that
 can be difficult to interpret. The present study therefore has critical
 theoretical and methodological implications for communication and 
marketing scholars.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2024-04-01</td>
					</tr>
					<tr>
					<th>Language</th>
						<td>en</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>Integrating Human Insights Into Text Analysis</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>SAGE Journals</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://doi.org/10.1177/08944393231184532">https://doi.org/10.1177/08944393231184532</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>12/24/2024, 7:10:27 PM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>Publisher: SAGE Publications Inc</td>
					</tr>
					<tr>
					<th>Volume</th>
						<td>42</td>
					</tr>
					<tr>
					<th>Pages</th>
						<td>416-437</td>
					</tr>
					<tr>
					<th>Publication</th>
						<td>Social Science Computer Review</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.1177/08944393231184532">10.1177/08944393231184532</a></td>
					</tr>
					<tr>
					<th>Issue</th>
						<td>2</td>
					</tr>
					<tr>
					<th>ISSN</th>
						<td>0894-4393</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>12/24/2024, 7:10:27 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
			</li>


			<li id="item_QPUTD85C" class="item preprint">
			<h2>Intelligent Computing Social Modeling and Methodological Innovations in Political Science in the Era of Large Language Models</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Preprint</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Zhenyu Wang</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Yi Xu</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Dequan Wang</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Lingfeng Zhou</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Yiqi Zhou</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>The recent wave of artificial intelligence, epitomized by 
large language models (LLMs), has presented opportunities and challenges
 for methodological innovation in political science, sparking 
discussions on a potential paradigm shift in the social sciences. 
However, how can we understand the impact of LLMs on knowledge 
production and paradigm transformation in the social sciences from a 
comprehensive perspective that integrates technology and methodology? 
What are LLMs' specific applications and representative innovative 
methods in political science research? These questions, particularly 
from a practical methodological standpoint, remain underexplored. This 
paper proposes the "Intelligent Computing Social Modeling" (ICSM) method
 to address these issues by clarifying the critical mechanisms of LLMs. 
ICSM leverages the strengths of LLMs in idea synthesis and action 
simulation, advancing intellectual exploration in political science 
through "simulated social construction" and "simulation validation." By 
simulating the U.S. presidential election, this study empirically 
demonstrates the operational pathways and methodological advantages of 
ICSM. By integrating traditional social science paradigms, ICSM not only
 enhances the quantitative paradigm's capability to apply big data to 
assess the impact of factors but also provides qualitative paradigms 
with evidence for social mechanism discovery at the individual level, 
offering a powerful tool that balances interpretability and 
predictability in social science research. The findings suggest that 
LLMs will drive methodological innovation in political science through 
integration and improvement rather than direct substitution.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2024-10-07</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>arXiv.org</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://arxiv.org/abs/2410.16301">http://arxiv.org/abs/2410.16301</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>1/7/2025, 5:05:51 PM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>arXiv:2410.16301 [cs]</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.48550/arXiv.2410.16301">10.48550/arXiv.2410.16301</a></td>
					</tr>
					<tr>
					<th>Repository</th>
						<td>arXiv</td>
					</tr>
					<tr>
					<th>Archive ID</th>
						<td>arXiv:2410.16301</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>1/7/2025, 5:05:51 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Computer Science - Artificial Intelligence</li>
					<li>Computer Science - Computers and Society</li>
				</ul>
				<h3 class="notes">Notes:</h3>
				<ul class="notes">
					<li id="item_J7MDU8CG">
<p class="plaintext">Comment: 34 pages, 5 figures, 3 tables</p>
					</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_C8QBV6XD">Preprint PDF					</li>
					<li id="item_JS93DP82">Snapshot					</li>
				</ul>
			</li>


			<li id="item_TYDXJIYH" class="item journalArticle">
			<h2>Inter-Rater Reliability Methods in Qualitative Case Study Research</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Rosanna Cole</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>The use of inter-rater reliability (IRR) methods may provide 
an opportunity to improve the transparency and consistency of 
qualitative case study data analysis in terms of the rigor of how codes 
and constructs have been developed from the raw data. Few articles on 
qualitative research methods in the literature conduct IRR assessments 
or neglect to report them, despite some disclosure of multiple 
researcher teams and coding reconciliation in the work. The article 
argues that the in-depth discussion and reconciliation initiated by IRR 
may enhance the findings and theory that emerges from qualitative case 
study data analysis, where the main data source is often interview 
transcripts or field notes. To achieve this, the article provides a 
missing link in the literature between data gathering and analysis by 
expanding an existing process model from five to six stages. The article
 also identifies seven factors that researchers can consider to 
determine the suitability of IRR to their work and it offers an IRR 
checklist, thereby providing a contribution to the broader literature on
 qualitative research methods.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2024-11-01</td>
					</tr>
					<tr>
					<th>Language</th>
						<td>en</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>SAGE Journals</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://doi.org/10.1177/00491241231156971">https://doi.org/10.1177/00491241231156971</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>1/6/2025, 12:06:10 PM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>Publisher: SAGE Publications Inc</td>
					</tr>
					<tr>
					<th>Volume</th>
						<td>53</td>
					</tr>
					<tr>
					<th>Pages</th>
						<td>1944-1975</td>
					</tr>
					<tr>
					<th>Publication</th>
						<td>Sociological Methods &amp; Research</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.1177/00491241231156971">10.1177/00491241231156971</a></td>
					</tr>
					<tr>
					<th>Issue</th>
						<td>4</td>
					</tr>
					<tr>
					<th>ISSN</th>
						<td>0049-1241</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>1/6/2025, 12:06:10 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_EESB6CYX">SAGE PDF Full Text					</li>
				</ul>
			</li>


			<li id="item_64Q4DQGS" class="item journalArticle">
			<h2>Interactive topic modeling</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Yuening Hu</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Jordan Boyd-Graber</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Brianna Satinoff</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Alison Smith</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Topic models have been used extensively as a tool for corpus 
exploration, and a cottage industry has developed to tweak topic models 
to better encode human intuitions or to better model data. However, 
creating such extensions requires expertise in machine learning 
unavailable to potential end-users of topic modeling software. In this 
work, we develop a framework for allowing users to iteratively reﬁne the
 topics discovered by models such as latent Dirichlet allocation (LDA) 
by adding constraints that enforce that sets of words must appear 
together in the same topic. We incorporate these constraints 
interactively by selectively removing elements in the state of a Markov 
Chain used for inference; we investigate a variety of methods for 
incorporating this information and demonstrate that these interactively 
added constraints improve topic usefulness for simulated and actual user
 sessions.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>6/2014</td>
					</tr>
					<tr>
					<th>Language</th>
						<td>en</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>DOI.org (Crossref)</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://link.springer.com/10.1007/s10994-013-5413-0">http://link.springer.com/10.1007/s10994-013-5413-0</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>3/13/2024, 6:41:35 PM</td>
					</tr>
					<tr>
					<th>Volume</th>
						<td>95</td>
					</tr>
					<tr>
					<th>Pages</th>
						<td>423-469</td>
					</tr>
					<tr>
					<th>Publication</th>
						<td>Machine Learning</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.1007/s10994-013-5413-0">10.1007/s10994-013-5413-0</a></td>
					</tr>
					<tr>
					<th>Issue</th>
						<td>3</td>
					</tr>
					<tr>
					<th>Journal Abbr</th>
						<td>Mach Learn</td>
					</tr>
					<tr>
					<th>ISSN</th>
						<td>0885-6125, 1573-0565</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>3/13/2024, 6:41:35 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>3/13/2024, 6:41:35 PM</td>
					</tr>
				</tbody></table>
				<h3 class="notes">Notes:</h3>
				<ul class="notes">
					<li id="item_YLCZJAEJ">
<div><div data-schema-version="8"><p>Summary</p>
<p></p>
<p>Hu 2011 describes the ineffective for customers of topic models and 
proposes a modification to make them more accessible/useful. ITM 
(interactive topic modeling). This article notes some articles about the
 difficulties of use of Topic modeling in sec. 2</p>
</div></div>
					</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_H7XU8RSU">Hu et al. - 2014 - Interactive topic modeling.pdf						<div class="note"><div><div data-schema-version="8"><p>Consumer of topic models: ie. qual researchers. Raw topic modes aren't sufficient for them, motivating this piece.</p>
<p></p>
</div></div>
					</div>					</li>
				</ul>
			</li>


			<li id="item_AZE3SQGY" class="item preprint">
			<h2>Interpretable Word Embeddings via Informative Priors</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Preprint</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Miriam Hurtado Bodell</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Martin Arvidsson</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Måns Magnusson</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Word embeddings have demonstrated strong performance on NLP 
tasks. However, lack of interpretability and the unsupervised nature of 
word embeddings have limited their use within computational social 
science and digital humanities. We propose the use of informative priors
 to create interpretable and domain-informed dimensions for 
probabilistic word embeddings. Experimental results show that sensible 
priors can capture latent semantic concepts better than or on-par with 
the current state of the art, while retaining the simplicity and 
generalizability of using priors.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2019-09-03</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>arXiv.org</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://arxiv.org/abs/1909.01459">http://arxiv.org/abs/1909.01459</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>1/8/2025, 12:55:27 PM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>arXiv:1909.01459 [cs]</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.48550/arXiv.1909.01459">10.48550/arXiv.1909.01459</a></td>
					</tr>
					<tr>
					<th>Repository</th>
						<td>arXiv</td>
					</tr>
					<tr>
					<th>Archive ID</th>
						<td>arXiv:1909.01459</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>1/8/2025, 12:55:27 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Computer Science - Computation and Language</li>
					<li>Computer Science - Machine Learning</li>
					<li>Statistics - Machine Learning</li>
				</ul>
				<h3 class="notes">Notes:</h3>
				<ul class="notes">
					<li id="item_2GNUDMQI">
<p class="plaintext">Comment: 10 pages, 2 figures, EMNLP 2019</p>
					</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_BV6Z6SQ9">Preprint PDF					</li>
					<li id="item_JNYNYX5D">Snapshot					</li>
				</ul>
			</li>


			<li id="item_UE389GM6" class="item journalArticle">
			<h2>Introduction to Neural Transfer Learning With Transformers for Social Science Text Analysis</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Sandra Wankmüller</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Transformer-based models for transfer learning have the 
potential to achieve high prediction accuracies on text-based supervised
 learning tasks with relatively few training data instances. These 
models are thus likely to benefit social scientists that seek to have as
 accurate as possible text-based measures, but only have limited 
resources for annotating training data. To enable social scientists to 
leverage these potential benefits for their research, this article 
explains how these methods work, why they might be advantageous, and 
what their limitations are. Additionally, three Transformer-based models
 for transfer learning, BERT, RoBERTa, and the Longformer, are compared 
to conventional machine learning algorithms on three applications. 
Across all evaluated tasks, textual styles, and training data set sizes,
 the conventional models are consistently outperformed by transfer 
learning with Transformers, thereby demonstrating the benefits these 
models can bring to text-based social science research.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2022-12-20</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>SAGE Journals</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://doi.org/10.1177/00491241221134527">https://doi.org/10.1177/00491241221134527</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>5/5/2024, 8:25:45 PM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>Publisher: SAGE Publications Inc</td>
					</tr>
					<tr>
					<th>Pages</th>
						<td>00491241221134527</td>
					</tr>
					<tr>
					<th>Publication</th>
						<td>Sociological Methods &amp; Research</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.1177/00491241221134527">10.1177/00491241221134527</a></td>
					</tr>
					<tr>
					<th>ISSN</th>
						<td>0049-1241</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>5/5/2024, 8:25:45 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>5/5/2024, 8:25:45 PM</td>
					</tr>
				</tbody></table>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_8XSEJKX9">Full Text PDF					</li>
				</ul>
			</li>


			<li id="item_J46NKETN" class="item preprint">
			<h2>Investigating Cultural Alignment of Large Language Models</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Preprint</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Badr AlKhamissi</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Muhammad ElNokrashy</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Mai AlKhamissi</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Mona Diab</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>The intricate relationship between language and culture has 
long been a subject of exploration within the realm of linguistic 
anthropology. Large Language Models (LLMs), promoted as repositories of 
collective human knowledge, raise a pivotal question: do these models 
genuinely encapsulate the diverse knowledge adopted by different 
cultures? Our study reveals that these models demonstrate greater 
cultural alignment along two dimensions -- firstly, when prompted with 
the dominant language of a specific culture, and secondly, when 
pretrained with a refined mixture of languages employed by that culture.
 We quantify cultural alignment by simulating sociological surveys, 
comparing model responses to those of actual survey participants as 
references. Specifically, we replicate a survey conducted in various 
regions of Egypt and the United States through prompting LLMs with 
different pretraining data mixtures in both Arabic and English with the 
personas of the real respondents and the survey questions. Further 
analysis reveals that misalignment becomes more pronounced for 
underrepresented personas and for culturally sensitive topics, such as 
those probing social values. Finally, we introduce Anthropological 
Prompting, a novel method leveraging anthropological reasoning to 
enhance cultural alignment. Our study emphasizes the necessity for a 
more balanced multilingual pretraining dataset to better represent the 
diversity of human experience and the plurality of different cultures 
with many implications on the topic of cross-lingual transfer.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2024-07-06</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>arXiv.org</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://arxiv.org/abs/2402.13231">http://arxiv.org/abs/2402.13231</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>1/7/2025, 4:50:03 PM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>arXiv:2402.13231 [cs]</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.48550/arXiv.2402.13231">10.48550/arXiv.2402.13231</a></td>
					</tr>
					<tr>
					<th>Repository</th>
						<td>arXiv</td>
					</tr>
					<tr>
					<th>Archive ID</th>
						<td>arXiv:2402.13231</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>1/7/2025, 4:50:03 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Computer Science - Computation and Language</li>
					<li>Computer Science - Computers and Society</li>
				</ul>
				<h3 class="notes">Notes:</h3>
				<ul class="notes">
					<li id="item_T5UTBBT8">
<p class="plaintext">Comment: ACL 2024 (Main)</p>
					</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_M6N89XYB">Preprint PDF					</li>
					<li id="item_AGTZRMFB">Snapshot					</li>
				</ul>
			</li>


			<li id="item_9MJQU3FR" class="item journalArticle">
			<h2>Investigating hybridity in artificial intelligence research</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Kate Williams</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Glen Berman</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Sandra Michalska</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Research in the global field of artificial intelligence is 
increasingly hybrid in orientation. Researchers are beholden to the 
requirements of multiple intersecting spheres, such as scholarly, 
public, and commercial, each with their own language and logic. 
Relatedly, collaboration across disciplinary, sector and national 
borders is increasingly expected, or required. Using a dataset of 93,482
 artificial intelligence publications, this article operationalises 
scholarly, public, and commercial spheres through citations, news 
mentions, and patent mentions, respectively. High performing 
publications (99th percentile) for each metric were separated into eight
 categories of influence. These comprised four blended categories of 
influence (news, patents and citations; news and patents; news and 
citations; patents and citations) and three single categories of 
influence (citations; news; patents), in addition to the ‘Other’ 
category of non-high performing publications. The article develops and 
applies two components of a new hybridity lens: evaluative hybridity and
 generative hybridity. Using multinomial logistic regression, selected 
aspects of knowledge production – research context, focus, artefacts, 
and collaborative configurations – were examined. The results elucidate 
key characteristics of knowledge production in the artificial 
intelligence field and demonstrate the utility of the proposed lens.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2023-07-01</td>
					</tr>
					<tr>
					<th>Language</th>
						<td>en</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>SAGE Journals</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://doi.org/10.1177/20539517231180577">https://doi.org/10.1177/20539517231180577</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>12/23/2024, 5:32:48 PM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>Publisher: SAGE Publications Ltd</td>
					</tr>
					<tr>
					<th>Volume</th>
						<td>10</td>
					</tr>
					<tr>
					<th>Pages</th>
						<td>20539517231180577</td>
					</tr>
					<tr>
					<th>Publication</th>
						<td>Big Data &amp; Society</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.1177/20539517231180577">10.1177/20539517231180577</a></td>
					</tr>
					<tr>
					<th>Issue</th>
						<td>2</td>
					</tr>
					<tr>
					<th>ISSN</th>
						<td>2053-9517</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>12/23/2024, 5:32:48 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_P5M4GNN5">SAGE PDF Full Text					</li>
				</ul>
			</li>


			<li id="item_DXKZFXTC" class="item journalArticle">
			<h2>Investigating Opinions on Public Policies in Digital Media: 
Setting up a Supervised Machine Learning Tool for Stance Classification</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Christina Viehmann</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Tilman Beck</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Marcus Maurer</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Oliver Quiring</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Iryna Gurevych</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Supervised machine learning (SML) provides us with tools to 
efficiently scrutinize large corpora of communication texts. Yet, 
setting up such a tool involves plenty of decisions starting with the 
data needed for training, the selection of an algorithm, and the details
 of model training. We aim at establishing a firm link between 
communication research tasks and the corresponding state-of-the-art in 
natural language processing research by systematically comparing the 
performance of different automatic text analysis approaches. We do this 
for a challenging task – stance detection of opinions on policy measures
 to tackle the COVID-19 pandemic in Germany voiced on Twitter. Our 
results add evidence that pre-trained language models such as BERT 
outperform feature-based and other neural network approaches. Yet, the 
gains one can achieve differ greatly depending on the specific merits of
 pre-training (i.e., use of different language models). Adding to the 
robustness of our conclusions, we run a generalizability check with a 
different use case in terms of language and topic. Additionally, we 
illustrate how the amount and quality of training data affect model 
performance pointing to potential compensation effects. Based on our 
results, we derive important practical recommendations for setting up 
such SML tools to study communication texts.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2023-04-03</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>Investigating Opinions on Public Policies in Digital Media</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>Taylor and Francis+NEJM</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://doi.org/10.1080/19312458.2022.2151579">https://doi.org/10.1080/19312458.2022.2151579</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>1/6/2025, 2:27:52 PM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>Publisher: Routledge
_eprint: https://doi.org/10.1080/19312458.2022.2151579</td>
					</tr>
					<tr>
					<th>Volume</th>
						<td>17</td>
					</tr>
					<tr>
					<th>Pages</th>
						<td>150-184</td>
					</tr>
					<tr>
					<th>Publication</th>
						<td>Communication Methods and Measures</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.1080/19312458.2022.2151579">10.1080/19312458.2022.2151579</a></td>
					</tr>
					<tr>
					<th>Issue</th>
						<td>2</td>
					</tr>
					<tr>
					<th>ISSN</th>
						<td>1931-2458</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>1/6/2025, 2:27:52 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
			</li>


			<li id="item_RY82RSCG" class="item conferencePaper">
			<h2>Is ChatGPT better than Human Annotators? Potential and Limitations of ChatGPT in Explaining Implicit Hate Speech</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Conference Paper</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Fan Huang</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Haewoon Kwak</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Jisun An</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Recent studies have alarmed that many online hate speeches are
 implicit. With its subtle nature, the explainability of the detection 
of such hateful speech has been a challenging problem. In this work, we 
examine whether ChatGPT can be used for providing natural language 
explanations (NLEs) for implicit hateful speech detection. We design our
 prompt to elicit concise ChatGPT-generated NLEs and conduct user 
studies to evaluate their qualities by comparison with human-written 
NLEs. We discuss the potential and limitations of ChatGPT in the context
 of implicit hateful speech research.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>April 30, 2023</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>Is ChatGPT better than Human Annotators?</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>ACM Digital Library</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://doi.org/10.1145/3543873.3587368">https://doi.org/10.1145/3543873.3587368</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>1/7/2025, 7:00:00 PM</td>
					</tr>
					<tr>
					<th>Place</th>
						<td>New York, NY, USA</td>
					</tr>
					<tr>
					<th>Publisher</th>
						<td>Association for Computing Machinery</td>
					</tr>
					<tr>
					<th>ISBN</th>
						<td>978-1-4503-9419-2</td>
					</tr>
					<tr>
					<th>Pages</th>
						<td>294–297</td>
					</tr>
					<tr>
					<th>Series</th>
						<td>WWW '23 Companion</td>
					</tr>
					<tr>
					<th>Proceedings Title</th>
						<td>Companion Proceedings of the ACM Web Conference 2023</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.1145/3543873.3587368">10.1145/3543873.3587368</a></td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>1/8/2025, 1:11:54 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_WABV7B9D">Submitted Version					</li>
				</ul>
			</li>


			<li id="item_JVG9LXFN" class="item journalArticle">
			<h2>JST and rJST: joint estimation of sentiment and topics in textual data using a semi-supervised approach</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Christian Pipal</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Martijn Schoonvelde</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Gijs Schumacher</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Max Boiten</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>This paper demonstrates the performance of the Joint Sentiment
 Topic model (JST) and the reversed Joint Sentiment Topic model (rJST) 
in measuring sentiment in political speeches, comparing them against a 
set of popular methods for sentiment analysis: widely used off-the-shelf
 sentiment dictionaries; an embeddings-enhanced dictionary approach; 
Latent Semantic Scaling, a semi-supervised approach; and a zero-shot 
transformer-based approach using a large language model (GPT-4). The 
findings reveal JST’s superiority over all non-transformer-based 
approaches in predicting human-coded sentiment in multiple languages and
 its ability to replicate known sentiment trends in legislative speech. 
rJST, meanwhile, provides valuable topic-specific sentiment estimates, 
responsive to political dynamics and significant events. Both models 
are, however, outperformed by transformer-based models like GPT-4. 
Additionally, the paper introduces the ’sentitopics’ R-package, designed
 to facilitate the use of JST and rJST in computational text analysis 
workflows. This package is compatible with popular text analysis tools, 
making the models accessible for applied researchers in communication 
science.</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>JST and rJST</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>Taylor and Francis+NEJM</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://doi.org/10.1080/19312458.2024.2383453">https://doi.org/10.1080/19312458.2024.2383453</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>1/6/2025, 1:55:23 PM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>Publisher: Routledge
_eprint: https://doi.org/10.1080/19312458.2024.2383453</td>
					</tr>
					<tr>
					<th>Volume</th>
						<td>0</td>
					</tr>
					<tr>
					<th>Pages</th>
						<td>1-19</td>
					</tr>
					<tr>
					<th>Publication</th>
						<td>Communication Methods and Measures</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.1080/19312458.2024.2383453">10.1080/19312458.2024.2383453</a></td>
					</tr>
					<tr>
					<th>Issue</th>
						<td>0</td>
					</tr>
					<tr>
					<th>ISSN</th>
						<td>1931-2458</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>1/6/2025, 1:55:23 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_5PPKID22">Full Text PDF					</li>
				</ul>
			</li>


			<li id="item_A7GBQGZ9" class="item preprint">
			<h2>Keeping Humans in the Loop: Human-Centered Automated Annotation with Generative AI</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Preprint</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Nicholas Pangakis</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Samuel Wolken</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Automated text annotation is a compelling use case for 
generative large language models (LLMs) in social media research. Recent
 work suggests that LLMs can achieve strong performance on annotation 
tasks; however, these studies evaluate LLMs on a small number of tasks 
and likely suffer from contamination due to a reliance on public 
benchmark datasets. Here, we test a human-centered framework for 
responsibly evaluating artificial intelligence tools used in automated 
annotation. We use GPT-4 to replicate 27 annotation tasks across 11 
password-protected datasets from recently published computational social
 science articles in high-impact journals. For each task, we compare 
GPT-4 annotations against human-annotated ground-truth labels and 
against annotations from separate supervised classification models 
fine-tuned on human-generated labels. Although the quality of LLM labels
 is generally high, we find significant variation in LLM performance 
across tasks, even within datasets. Our findings underscore the 
importance of a human-centered workflow and careful evaluation 
standards: Automated annotations significantly diverge from human 
judgment in numerous scenarios, despite various optimization strategies 
such as prompt tuning. Grounding automated annotation in validation 
labels generated by humans is essential for responsible evaluation.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2024-09-21</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>Keeping Humans in the Loop</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>arXiv.org</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://arxiv.org/abs/2409.09467">http://arxiv.org/abs/2409.09467</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>1/7/2025, 5:08:26 PM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>arXiv:2409.09467 [cs]</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.48550/arXiv.2409.09467">10.48550/arXiv.2409.09467</a></td>
					</tr>
					<tr>
					<th>Repository</th>
						<td>arXiv</td>
					</tr>
					<tr>
					<th>Archive ID</th>
						<td>arXiv:2409.09467</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>1/7/2025, 5:08:26 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Computer Science - Computation and Language</li>
				</ul>
				<h3 class="notes">Notes:</h3>
				<ul class="notes">
					<li id="item_SC79Z8M6">
<p class="plaintext">Comment: Accepted at Proceedings of the International AAAI Conference on Web and Social Media. Vol. 19. 2025</p>
					</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_YDNY7D69">Preprint PDF					</li>
					<li id="item_3X8RY9QP">Snapshot					</li>
				</ul>
			</li>


			<li id="item_IPHXBKQ9" class="item preprint">
			<h2>"Kelly is a Warm Person, Joseph is a Role Model": Gender Biases in LLM-Generated Reference Letters</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Preprint</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Yixin Wan</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>George Pu</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Jiao Sun</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Aparna Garimella</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Kai-Wei Chang</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Nanyun Peng</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Large Language Models (LLMs) have recently emerged as an 
effective tool to assist individuals in writing various types of 
content, including professional documents such as recommendation 
letters. Though bringing convenience, this application also introduces 
unprecedented fairness concerns. Model-generated reference letters might
 be directly used by users in professional scenarios. If underlying 
biases exist in these model-constructed letters, using them without 
scrutinization could lead to direct societal harms, such as sabotaging 
application success rates for female applicants. In light of this 
pressing issue, it is imminent and necessary to comprehensively study 
fairness issues and associated harms in this real-world use case. In 
this paper, we critically examine gender biases in LLM-generated 
reference letters. Drawing inspiration from social science findings, we 
design evaluation methods to manifest biases through 2 dimensions: (1) 
biases in language style and (2) biases in lexical content. We further 
investigate the extent of bias propagation by analyzing the 
hallucination bias of models, a term that we define to be bias 
exacerbation in model-hallucinated contents. Through benchmarking 
evaluation on 2 popular LLMs- ChatGPT and Alpaca, we reveal significant 
gender biases in LLM-generated recommendation letters. Our findings not 
only warn against using LLMs for this application without 
scrutinization, but also illuminate the importance of thoroughly 
studying hidden biases and harms in LLM-generated professional 
documents.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2023-12-01</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>"Kelly is a Warm Person, Joseph is a Role Model"</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>arXiv.org</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://arxiv.org/abs/2310.09219">http://arxiv.org/abs/2310.09219</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>1/7/2025, 5:41:52 PM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>arXiv:2310.09219 [cs]</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.48550/arXiv.2310.09219">10.48550/arXiv.2310.09219</a></td>
					</tr>
					<tr>
					<th>Repository</th>
						<td>arXiv</td>
					</tr>
					<tr>
					<th>Archive ID</th>
						<td>arXiv:2310.09219</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>1/7/2025, 5:41:52 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Computer Science - Artificial Intelligence</li>
					<li>Computer Science - Computation and Language</li>
				</ul>
				<h3 class="notes">Notes:</h3>
				<ul class="notes">
					<li id="item_G32YC8TA">
<p class="plaintext">Comment: Accepted to EMNLP 2023 Findings</p>
					</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_ZWIYCRG3">Preprint PDF					</li>
					<li id="item_ADKWW4KE">Snapshot					</li>
				</ul>
			</li>


			<li id="item_FVIYKQQK" class="item journalArticle">
			<h2>Keyword expansion techniques for mining social movement data on social media</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Lia Bozarth</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Ceren Budak</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Political and social scientists have been relying extensively 
on keywords such as hashtags to mine social movement data from social 
media sites, particularly Twitter. Yet, prior work demonstrates that 
unrepresentative keyword sets can lead to flawed research conclusions. 
Numerous keyword expansion methods have been proposed to increase the 
comprehensiveness of keywords, but systematic evaluations of these 
methods have been lacking. Our paper fills this gap. We evaluate five 
diverse keyword expansion techniques (or pipelines) on five 
representative social movements across two distinct activity levels. Our
 results guide researchers who aim to use social media keyword searches 
to mine data. For instance, we show that word embedding-based methods 
significantly outperform other even more complex and newer approaches 
when movements are in normal activity periods. These methods are also 
less computationally intensive. More importantly, we also observe that 
no single pipeline can identify little more than half of all 
movement-related tweets when these movements are at their peak 
mobilization period offline. However, coverage can increase 
significantly when more than one pipeline is used. This is true even 
when the pipelines are selected at random.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2022/12</td>
					</tr>
					<tr>
					<th>Language</th>
						<td>en</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>epjdatascience.springeropen.com</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://epjdatascience.springeropen.com/articles/10.1140/epjds/s13688-022-00343-9">https://epjdatascience.springeropen.com/articles/10.1140/epjds/s13688-022-00343-9</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>1/3/2025, 12:45:52 PM</td>
					</tr>
					<tr>
					<th>Rights</th>
						<td>2022 The Author(s)</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>Number: 1
Publisher: SpringerOpen</td>
					</tr>
					<tr>
					<th>Volume</th>
						<td>11</td>
					</tr>
					<tr>
					<th>Pages</th>
						<td>1-24</td>
					</tr>
					<tr>
					<th>Publication</th>
						<td>EPJ Data Science</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.1140/epjds/s13688-022-00343-9">10.1140/epjds/s13688-022-00343-9</a></td>
					</tr>
					<tr>
					<th>Issue</th>
						<td>1</td>
					</tr>
					<tr>
					<th>Journal Abbr</th>
						<td>EPJ Data Sci.</td>
					</tr>
					<tr>
					<th>ISSN</th>
						<td>2193-1127</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>1/3/2025, 12:45:52 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_IHA6F6SW">Full Text PDF					</li>
				</ul>
			</li>


			<li id="item_VVTHL79M" class="item journalArticle">
			<h2>Knowledge Discovery: Methods from data mining and machine learning</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Xiaoling Shu</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Yiwan Ye</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>The interdisciplinary field of knowledge discovery and data 
mining emerged from a necessity of big data requiring new analytical 
methods beyond the traditional statistical approaches to discover new 
knowledge from the data mine. This emergent approach is a dialectic 
research process that is both deductive and inductive. The data mining 
approach automatically or semi-automatically considers a larger number 
of joint, interactive, and independent predictors to address causal 
heterogeneity and improve prediction. Instead of challenging the 
conventional model-building approach, it plays an important 
complementary role in improving model goodness of fit, revealing valid 
and significant hidden patterns in data, identifying nonlinear and 
non-additive effects, providing insights into data developments, 
methods, and theory, and enriching scientific discovery. Machine 
learning builds models and algorithms by learning and improving from 
data when the explicit model structure is unclear and algorithms with 
good performance are difficult to attain. The most recent development is
 to incorporate this new paradigm of predictive modeling with the 
classical approach of parameter estimation regressions to produce 
improved models that combine explanation and prediction.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2023-02-01</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>Knowledge Discovery</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>ScienceDirect</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://www.sciencedirect.com/science/article/pii/S0049089X22001284">https://www.sciencedirect.com/science/article/pii/S0049089X22001284</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>1/7/2025, 4:13:00 PM</td>
					</tr>
					<tr>
					<th>Volume</th>
						<td>110</td>
					</tr>
					<tr>
					<th>Pages</th>
						<td>102817</td>
					</tr>
					<tr>
					<th>Publication</th>
						<td>Social Science Research</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.1016/j.ssresearch.2022.102817">10.1016/j.ssresearch.2022.102817</a></td>
					</tr>
					<tr>
					<th>Journal Abbr</th>
						<td>Social Science Research</td>
					</tr>
					<tr>
					<th>ISSN</th>
						<td>0049-089X</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>1/7/2025, 4:13:00 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>big data</li>
					<li>causal discovery</li>
					<li>data mining</li>
					<li>Knowledge discovery</li>
					<li>machine learning</li>
					<li>predition</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_SXQ85C8T">ScienceDirect Snapshot					</li>
				</ul>
			</li>


			<li id="item_293LVUK4" class="item preprint">
			<h2>Knowledge Distillation in Automated Annotation: Supervised Text Classification with LLM-Generated Training Labels</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Preprint</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Nicholas Pangakis</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Samuel Wolken</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Computational social science (CSS) practitioners often rely on
 human-labeled data to fine-tune supervised text classifiers. We assess 
the potential for researchers to augment or replace human-generated 
training data with surrogate training labels from generative large 
language models (LLMs). We introduce a recommended workflow and test 
this LLM application by replicating 14 classification tasks and 
measuring performance. We employ a novel corpus of English-language text
 classification data sets from recent CSS articles in high-impact 
journals. Because these data sets are stored in password-protected 
archives, our analyses are less prone to issues of contamination. For 
each task, we compare supervised classifiers fine-tuned using GPT-4 
labels against classifiers fine-tuned with human annotations and against
 labels from GPT-4 and Mistral-7B with few-shot in-context learning. Our
 findings indicate that supervised classification models fine-tuned on 
LLM-generated labels perform comparably to models fine-tuned with labels
 from human annotators. Fine-tuning models using LLM-generated labels 
can be a fast, efficient and cost-effective method of building 
supervised text classifiers.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2024-06-25</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>Knowledge Distillation in Automated Annotation</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>arXiv.org</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://arxiv.org/abs/2406.17633">http://arxiv.org/abs/2406.17633</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>1/7/2025, 5:26:16 PM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>arXiv:2406.17633 [cs]</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.48550/arXiv.2406.17633">10.48550/arXiv.2406.17633</a></td>
					</tr>
					<tr>
					<th>Repository</th>
						<td>arXiv</td>
					</tr>
					<tr>
					<th>Archive ID</th>
						<td>arXiv:2406.17633</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>1/7/2025, 5:26:16 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Computer Science - Computation and Language</li>
					<li>Computer Science - Machine Learning</li>
				</ul>
				<h3 class="notes">Notes:</h3>
				<ul class="notes">
					<li id="item_6PKRLXVF">
<p class="plaintext">Comment: In Proceedings of the Sixth Workshop on Natural Language Processing and Computational Social Science</p>
					</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_8E4J667X">Preprint PDF					</li>
					<li id="item_WQ3KKIEK">Snapshot					</li>
				</ul>
			</li>


			<li id="item_FKBYLGJW" class="item preprint">
			<h2>Large Language Model for Qualitative Research -- A Systematic Mapping Study</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Preprint</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Cauã Ferreira Barros</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Bruna Borges Azevedo</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Valdemar Vicente Graciano Neto</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Mohamad Kassab</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Marcos Kalinowski</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Hugo Alexandre D. do Nascimento</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Michelle C. G. S. P. Bandeira</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>The exponential growth of text-based data in domains such as 
healthcare, education, and social sciences has outpaced the capacity of 
traditional qualitative analysis methods, which are time-intensive and 
prone to subjectivity. Large Language Models (LLMs), powered by advanced
 generative AI, have emerged as transformative tools capable of 
automating and enhancing qualitative analysis. This study systematically
 maps the literature on the use of LLMs for qualitative research, 
exploring their application contexts, configurations, methodologies, and
 evaluation metrics. Findings reveal that LLMs are utilized across 
diverse fields, demonstrating the potential to automate processes 
traditionally requiring extensive human input. However, challenges such 
as reliance on prompt engineering, occasional inaccuracies, and 
contextual limitations remain significant barriers. This research 
highlights opportunities for integrating LLMs with human expertise, 
improving model robustness, and refining evaluation methodologies. By 
synthesizing trends and identifying research gaps, this study aims to 
guide future innovations in the application of LLMs for qualitative 
analysis.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2024-11-18</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>arXiv.org</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://arxiv.org/abs/2411.14473">http://arxiv.org/abs/2411.14473</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>1/7/2025, 5:01:32 PM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>arXiv:2411.14473 [cs]</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.48550/arXiv.2411.14473">10.48550/arXiv.2411.14473</a></td>
					</tr>
					<tr>
					<th>Repository</th>
						<td>arXiv</td>
					</tr>
					<tr>
					<th>Archive ID</th>
						<td>arXiv:2411.14473</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>1/7/2025, 5:01:32 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Computer Science - Artificial Intelligence</li>
					<li>Computer Science - Computation and Language</li>
				</ul>
				<h3 class="notes">Notes:</h3>
				<ul class="notes">
					<li id="item_4DE5XDZD">
<p class="plaintext">Comment: 8 pages, includes 1 figures and 3 tables. Submitted to the WSESE 2025 ICSE Workshop</p>
					</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_PQJYX6VG">Preprint PDF					</li>
					<li id="item_MZ5WQQAB">Snapshot					</li>
				</ul>
			</li>


			<li id="item_7DFV6UP7" class="item preprint">
			<h2>Large Language Models and Thematic Analysis: Human-AI Synergy in Researching Hate Speech on Social Media</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Preprint</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Petre Breazu</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Miriam Schirmer</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Songbo Hu</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Napoleon Katsos</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>In the dynamic field of artificial intelligence (AI), the 
development and application of Large Language Models (LLMs) for text 
analysis are of significant academic interest. Despite the promising 
capabilities of various LLMs in conducting qualitative analysis, their 
use in the humanities and social sciences has not been thoroughly 
examined. This article contributes to the emerging literature on LLMs in
 qualitative analysis by documenting an experimental study involving 
GPT-4. The study focuses on performing thematic analysis (TA) using a 
YouTube dataset derived from an EU-funded project, which was previously 
analyzed by other researchers. This dataset is about the representation 
of Roma migrants in Sweden during 2016, a period marked by the aftermath
 of the 2015 refugee crisis and preceding the Swedish national elections
 in 2017. Our study seeks to understand the potential of combining human
 intelligence with AI's scalability and efficiency, examining the 
advantages and limitations of employing LLMs in qualitative research 
within the humanities and social sciences. Additionally, we discuss 
future directions for applying LLMs in these fields.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2024-08-09</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>Large Language Models and Thematic Analysis</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>arXiv.org</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://arxiv.org/abs/2408.05126">http://arxiv.org/abs/2408.05126</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>1/7/2025, 5:12:56 PM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>arXiv:2408.05126 [cs]</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.48550/arXiv.2408.05126">10.48550/arXiv.2408.05126</a></td>
					</tr>
					<tr>
					<th>Repository</th>
						<td>arXiv</td>
					</tr>
					<tr>
					<th>Archive ID</th>
						<td>arXiv:2408.05126</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>1/7/2025, 5:12:56 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Computer Science - Computation and Language</li>
					<li>Computer Science - Human-Computer Interaction</li>
					<li>Computer Science - Social and Information Networks</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_NF6SMX2K">Preprint PDF					</li>
					<li id="item_3M5UH9LT">Snapshot					</li>
				</ul>
			</li>


			<li id="item_VDRTTRGF" class="item preprint">
			<h2>Large language models as linguistic simulators and cognitive models in human research</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Preprint</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Zhicheng Lin</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>The rise of large language models (LLMs) that generate 
human-like text has sparked debates over their potential to replace 
human participants in behavioral and cognitive research. We critically 
evaluate this replacement perspective to appraise the fundamental 
utility of language models in psychology and social science. Through a 
five-dimension framework, characterization, representation, 
interpretation, implication, and utility, we identify six fallacies that
 undermine the replacement perspective: (1) equating token prediction 
with human intelligence, (2) assuming LLMs represent the average human, 
(3) interpreting alignment as explanation, (4) anthropomorphizing AI, 
(5) essentializing identities, and (6) purporting LLMs as primary tools 
that directly reveal the human mind. Rather than replacement, the 
evidence and arguments are consistent with a simulation perspective, 
where LLMs offer a new paradigm to simulate roles and model cognitive 
processes. We highlight limitations and considerations about internal, 
external, construct, and statistical validity, providing methodological 
guidelines for effective integration of LLMs into psychological 
research, with a focus on model selection, prompt design, 
interpretation, and ethical considerations. This perspective reframes 
the role of language models in behavioral and cognitive science, serving
 as linguistic simulators and cognitive models that shed light on the 
similarities and differences between machine intelligence and human 
cognition and thoughts.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2024-10-20</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>arXiv.org</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://arxiv.org/abs/2402.04470">http://arxiv.org/abs/2402.04470</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>1/7/2025, 5:31:27 PM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>arXiv:2402.04470 [cs]</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.48550/arXiv.2402.04470">10.48550/arXiv.2402.04470</a></td>
					</tr>
					<tr>
					<th>Repository</th>
						<td>arXiv</td>
					</tr>
					<tr>
					<th>Archive ID</th>
						<td>arXiv:2402.04470</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>1/7/2025, 5:31:28 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Computer Science - Computers and Society</li>
				</ul>
				<h3 class="notes">Notes:</h3>
				<ul class="notes">
					<li id="item_XET5WPPB">
<p class="plaintext">Comment: 30 pages, 1 figure, 3 tables, 2 boxes</p>
					</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_VBYK7A9L">Preprint PDF					</li>
					<li id="item_V8TCG5XD">Snapshot					</li>
				</ul>
			</li>


			<li id="item_RJJGYLDK" class="item preprint">
			<h2>Large Language Models can Achieve Social Balance</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Preprint</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Pedro Cisneros-Velarde</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Social balance is a concept in sociology which states that if 
every three individuals in a population achieve certain structures of 
positive or negative interactions, then the whole population ends up in 
one faction of positive interactions or divided between two or more 
antagonistic factions. In this paper, we consider a group of interacting
 large language models (LLMs) and study how, after continuous 
interactions, they can achieve social balance. Across three different 
LLM models, we found that social balance depends on (i) whether 
interactions are updated based on "relationships", "appraisals", or 
"opinions"; (ii) whether agents update their interactions based on 
homophily or influence from their peers; and (iii) the number of 
simultaneous interactions the LLMs consider. When social balance is 
achieved, its particular structure of positive or negative interactions 
depends on these three conditions and are different across LLM models 
and sizes. The stability of interactions and the justification for their
 update also vary across models. Thus, social balance is driven by the 
pre-training and alignment particular to each LLM model.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2024-10-05</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>arXiv.org</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://arxiv.org/abs/2410.04054">http://arxiv.org/abs/2410.04054</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>1/7/2025, 4:46:10 PM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>arXiv:2410.04054 [cs]</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.48550/arXiv.2410.04054">10.48550/arXiv.2410.04054</a></td>
					</tr>
					<tr>
					<th>Repository</th>
						<td>arXiv</td>
					</tr>
					<tr>
					<th>Archive ID</th>
						<td>arXiv:2410.04054</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>1/7/2025, 4:46:10 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Computer Science - Artificial Intelligence</li>
					<li>Computer Science - Computation and Language</li>
					<li>Computer Science - Multiagent Systems</li>
					<li>Computer Science - Social and Information Networks</li>
					<li>Physics - Physics and Society</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_QCZTU7GC">Preprint PDF					</li>
					<li id="item_G2VI8YS3">Snapshot					</li>
				</ul>
			</li>


			<li id="item_24NAXZ9B" class="item preprint">
			<h2>Large Language Models Can Be Used to Estimate the Latent Positions of Politicians</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Preprint</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Patrick Y. Wu</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Jonathan Nagler</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Joshua A. Tucker</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Solomon Messing</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Existing approaches to estimating politicians' latent 
positions along specific dimensions often fail when relevant data is 
limited. We leverage the embedded knowledge in generative large language
 models (LLMs) to address this challenge and measure lawmakers' 
positions along specific political or policy dimensions. We prompt an 
instruction/dialogue-tuned LLM to pairwise compare lawmakers and then 
scale the resulting graph using the Bradley-Terry model. We estimate 
novel measures of U.S. senators' positions on liberal-conservative 
ideology, gun control, and abortion. Our liberal-conservative scale, 
used to validate LLM-driven scaling, strongly correlates with existing 
measures and offsets interpretive gaps, suggesting LLMs synthesize 
relevant data from internet and digitized media rather than memorizing 
existing measures. Our gun control and abortion measures -- the first of
 their kind -- differ from the liberal-conservative scale in face-valid 
ways and predict interest group ratings and legislator votes better than
 ideology alone. Our findings suggest LLMs hold promise for solving 
complex social science measurement problems.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2023-09-26</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>arXiv.org</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://arxiv.org/abs/2303.12057">http://arxiv.org/abs/2303.12057</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>1/7/2025, 5:40:45 PM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>arXiv:2303.12057 [cs]</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.48550/arXiv.2303.12057">10.48550/arXiv.2303.12057</a></td>
					</tr>
					<tr>
					<th>Repository</th>
						<td>arXiv</td>
					</tr>
					<tr>
					<th>Archive ID</th>
						<td>arXiv:2303.12057</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>1/7/2025, 5:40:45 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Computer Science - Computation and Language</li>
					<li>Computer Science - Computers and Society</li>
				</ul>
				<h3 class="notes">Notes:</h3>
				<ul class="notes">
					<li id="item_U2VTQE3E">
<p class="plaintext">Comment: 18 pages, 4 figures; V2: fixed graphical error on Figure 2; V3: reorganized sections, updated prose; V4: added additional scales and analysis</p>
					</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_MQ7NRUMJ">Preprint PDF					</li>
					<li id="item_U48S6BTK">Snapshot					</li>
				</ul>
			</li>


			<li id="item_TMAM2MR2" class="item preprint">
			<h2>Large Language Models for Social Networks: Applications, Challenges, and Solutions</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Preprint</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Jingying Zeng</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Richard Huang</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Waleed Malik</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Langxuan Yin</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Bojan Babic</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Danny Shacham</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Xiao Yan</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Jaewon Yang</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Qi He</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Large Language Models (LLMs) are transforming the way people 
generate, explore, and engage with content. We study how we can develop 
LLM applications for online social networks. Despite LLMs' successes in 
other domains, it is challenging to develop LLM-based products for 
social networks for numerous reasons, and it has been relatively 
under-reported in the research community. We categorize LLM applications
 for social networks into three categories. First is knowledge tasks 
where users want to find new knowledge and information, such as search 
and question-answering. Second is entertainment tasks where users want 
to consume interesting content, such as getting entertaining 
notification content. Third is foundational tasks that need to be done 
to moderate and operate the social networks, such as content annotation 
and LLM monitoring. For each task, we share the challenges we found, 
solutions we developed, and lessons we learned. To the best of our 
knowledge, this is the first comprehensive paper about developing LLM 
applications for social networks.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2024-01-04</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>Large Language Models for Social Networks</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>arXiv.org</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://arxiv.org/abs/2401.02575">http://arxiv.org/abs/2401.02575</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>1/8/2025, 2:54:10 PM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>arXiv:2401.02575 [cs]</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.48550/arXiv.2401.02575">10.48550/arXiv.2401.02575</a></td>
					</tr>
					<tr>
					<th>Repository</th>
						<td>arXiv</td>
					</tr>
					<tr>
					<th>Archive ID</th>
						<td>arXiv:2401.02575</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>1/8/2025, 2:54:10 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Computer Science - Artificial Intelligence</li>
					<li>Computer Science - Machine Learning</li>
					<li>Computer Science - Social and Information Networks</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_GLY9X4NE">Preprint PDF					</li>
					<li id="item_7V4EVCDL">Snapshot					</li>
				</ul>
			</li>


			<li id="item_9SUFFGXA" class="item preprint">
			<h2>Large Language Models for Text Classification: From Zero-Shot Learning to Instruction-Tuning</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Preprint</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Youngjin (YJ) Chae</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Thomas Davidson</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Advances in large language models (LLMs) have transformed the 
field of natural language processing and have enormous potential for 
social scientific analysis. We explore the application of LLMs to 
supervised text classification. As a case study, we consider stance 
detection and examine variation in predictive accuracy across different 
architectures, training regimes, and task specifications. We compare ten
 models ranging in size from 86 million to 1.7 trillion parameters and 
four distinct training regimes: prompt-based zero-shot learning; 
few-shot learning; fine-tuning; and instruction-tuning. The largest 
models generally offer the best predictive performance, but fine-tuning 
smaller models is a competitive solution due to their relatively high 
accuracy and low cost. For complex prediction tasks, instruction-tuned 
open-weights models can perform well, rivaling state-of-the-art 
commercial models. We provide recommendations for the use of LLMs for 
text classification in sociological research and discuss the limitations
 and challenges related to the use of these technologies.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2023-08-24</td>
					</tr>
					<tr>
					<th>Language</th>
						<td>en-us</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>Large Language Models for Text Classification</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>OSF Preprints</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://osf.io/sthwk">https://osf.io/sthwk</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>1/3/2025, 4:17:53 PM</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.31235/osf.io/sthwk">10.31235/osf.io/sthwk</a></td>
					</tr>
					<tr>
					<th>Repository</th>
						<td>OSF</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>1/3/2025, 4:17:53 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>computational sociology</li>
					<li>large language models</li>
					<li>natural language processing</li>
					<li>political sociology</li>
					<li>social media</li>
					<li>stance detection</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_7SDJRUJC">OSF Preprint					</li>
				</ul>
			</li>


			<li id="item_FWGAP27P" class="item webpage">
			<h2>Large Language Models for Wearable Sensor-Based Human Activity 
Recognition, Health Monitoring, and Behavioral Modeling: A Survey of 
Early Trends, Datasets, and Challenges</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Web Page</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>The proliferation of wearable technology enables the 
generation of vast amounts of sensor data, offering significant 
opportunities for advancements in health monitoring, activity 
recognition, and personalized medicine. However, the complexity and 
volume of these data present substantial challenges in data modeling and
 analysis, which have been addressed with approaches spanning time 
series modeling to deep learning techniques. The latest frontier in this
 domain is the adoption of large language models (LLMs), such as GPT-4 
and Llama, for data analysis, modeling, understanding, and human 
behavior monitoring through the lens of wearable sensor data. This 
survey explores the current trends and challenges in applying LLMs for 
sensor-based human activity recognition and behavior modeling. We 
discuss the nature of wearable sensor data, the capabilities and 
limitations of LLMs in modeling them, and their integration with 
traditional machine learning techniques. We also identify key 
challenges, including data quality, computational requirements, 
interpretability, and privacy concerns. By examining case studies and 
successful applications, we highlight the potential of LLMs in enhancing
 the analysis and interpretation of wearable sensor data. Finally, we 
propose future directions for research, emphasizing the need for 
improved preprocessing techniques, more efficient and scalable models, 
and interdisciplinary collaboration. This survey aims to provide a 
comprehensive overview of the intersection between wearable sensor data 
and LLMs, offering insights into the current state and future prospects 
of this emerging field.</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://www.mdpi.com/1424-8220/24/15/5045">https://www.mdpi.com/1424-8220/24/15/5045</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>1/8/2025, 1:48:51 PM</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>1/8/2025, 1:48:51 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_32BYR57L">Large Language Models for Wearable 
Sensor-Based Human Activity Recognition, Health Monitoring, and 
Behavioral Modeling: A Survey of Early Trends, Datasets, and Challenges	
				</li>
				</ul>
			</li>


			<li id="item_SNG4HAKU" class="item journalArticle">
			<h2>Large Language Models Outperform Expert Coders and Supervised Classifiers at Annotating Political Social Media Messages</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Petter Törnberg</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Instruction-tuned Large Language Models (LLMs) have recently 
emerged as a powerful new tool for text analysis. As these models are 
capable of zero-shot annotation based on instructions written in natural
 language, they obviate the need of large sets of training data—and thus
 bring potential paradigm-shifting implications for using text as data. 
While the models show substantial promise, their relative performance 
compared to human coders and supervised models remains poorly understood
 and subject to significant academic debate. This paper assesses the 
strengths and weaknesses of popular fine-tuned AI models compared to 
both conventional supervised classifiers and manual annotation by 
experts and crowd workers. The task used is to identify the political 
affiliation of politicians based on a single X/Twitter message, focusing
 on data from 11 different countries. The paper finds that GPT-4 
achieves higher accuracy than both supervised models and human coders 
across all languages and country contexts. In the US context, it 
achieves an accuracy of 0.934 and an inter-coder reliability of 0.982. 
Examining the cases where the models fail, the paper finds that the 
LLM—unlike the supervised models—correctly annotates messages that 
require interpretation of implicit or unspoken references, or reasoning 
on the basis of contextual knowledge—capacities that have traditionally 
been understood to be distinctly human. The paper thus contributes to 
our understanding of the revolutionary implications of LLMs for text 
analysis within the social sciences.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2024-09-22</td>
					</tr>
					<tr>
					<th>Language</th>
						<td>en</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>SAGE Journals</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://doi.org/10.1177/08944393241286471">https://doi.org/10.1177/08944393241286471</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>10/1/2024, 10:11:39 AM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>Publisher: SAGE Publications Inc</td>
					</tr>
					<tr>
					<th>Pages</th>
						<td>08944393241286471</td>
					</tr>
					<tr>
					<th>Publication</th>
						<td>Social Science Computer Review</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.1177/08944393241286471">10.1177/08944393241286471</a></td>
					</tr>
					<tr>
					<th>ISSN</th>
						<td>0894-4393</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>10/1/2024, 10:11:39 AM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>10/1/2024, 10:11:39 AM</td>
					</tr>
				</tbody></table>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_9FXFTPA2">SAGE PDF Full Text					</li>
				</ul>
			</li>


			<li id="item_HUMPFLWW" class="item preprint">
			<h2>Large Language Models Reflect the Ideology of their Creators</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Preprint</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Maarten Buyl</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Alexander Rogiers</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Sander Noels</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Iris Dominguez-Catena</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Edith Heiter</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Raphael Romero</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Iman Johary</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Alexandru-Cristian Mara</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Jefrey Lijffijt</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Tijl De Bie</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Large language models (LLMs) are trained on vast amounts of 
data to generate natural language, enabling them to perform tasks like 
text summarization and question answering. These models have become 
popular in artificial intelligence (AI) assistants like ChatGPT and 
already play an influential role in how humans access information. 
However, the behavior of LLMs varies depending on their design, 
training, and use. In this paper, we uncover notable diversity in the 
ideological stance exhibited across different LLMs and languages in 
which they are accessed. We do this by prompting a diverse panel of 
popular LLMs to describe a large number of prominent and controversial 
personalities from recent world history, both in English and in Chinese.
 By identifying and analyzing moral assessments reflected in the 
generated descriptions, we find consistent normative differences between
 how the same LLM responds in Chinese compared to English. Similarly, we
 identify normative disagreements between Western and non-Western LLMs 
about prominent actors in geopolitical conflicts. Furthermore, popularly
 hypothesized disparities in political goals among Western models are 
reflected in significant normative differences related to inclusion, 
social inequality, and political scandals. Our results show that the 
ideological stance of an LLM often reflects the worldview of its 
creators. This raises important concerns around technological and 
regulatory efforts with the stated aim of making LLMs ideologically 
`unbiased', and it poses risks for political instrumentalization.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2024-10-24</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>arXiv.org</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://arxiv.org/abs/2410.18417">http://arxiv.org/abs/2410.18417</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>1/8/2025, 3:15:07 PM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>arXiv:2410.18417 [cs]</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.48550/arXiv.2410.18417">10.48550/arXiv.2410.18417</a></td>
					</tr>
					<tr>
					<th>Repository</th>
						<td>arXiv</td>
					</tr>
					<tr>
					<th>Archive ID</th>
						<td>arXiv:2410.18417</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>1/8/2025, 3:15:07 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Computer Science - Computation and Language</li>
					<li>Computer Science - Machine Learning</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_CUSARSZ7">Preprint PDF					</li>
					<li id="item_5N35VK2H">Snapshot					</li>
				</ul>
			</li>


			<li id="item_SMJ64UWT" class="item preprint">
			<h2>Large language models should not replace human participants because they can misportray and flatten identity groups</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Preprint</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Angelina Wang</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Jamie Morgenstern</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>John P. Dickerson</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Large language models (LLMs) are increasing in capability and 
popularity, propelling their application in new domains -- including as 
replacements for human participants in computational social science, 
user testing, annotation tasks, and more. In many settings, researchers 
seek to distribute their surveys to a sample of participants that are 
representative of the underlying human population of interest. This 
means in order to be a suitable replacement, LLMs will need to be able 
to capture the influence of positionality (i.e., relevance of social 
identities like gender and race). However, we show that there are two 
inherent limitations in the way current LLMs are trained that prevent 
this. We argue analytically for why LLMs are likely to both misportray 
and flatten the representations of demographic groups, then empirically 
show this on 4 LLMs through a series of human studies with 3200 
participants across 16 demographic identities. We also discuss a third 
limitation about how identity prompts can essentialize identities. 
Throughout, we connect each limitation to a pernicious history that 
explains why it is harmful for marginalized demographic groups. Overall,
 we urge caution in use cases where LLMs are intended to replace human 
participants whose identities are relevant to the task at hand. At the 
same time, in cases where the goal is to supplement rather than replace 
(e.g., pilot studies), we provide inference-time techniques that we 
empirically demonstrate do reduce, but do not remove, these harms.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2024-10-01</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>arXiv.org</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://arxiv.org/abs/2402.01908">http://arxiv.org/abs/2402.01908</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>1/7/2025, 5:33:21 PM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>arXiv:2402.01908 [cs]</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.48550/arXiv.2402.01908">10.48550/arXiv.2402.01908</a></td>
					</tr>
					<tr>
					<th>Repository</th>
						<td>arXiv</td>
					</tr>
					<tr>
					<th>Archive ID</th>
						<td>arXiv:2402.01908</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>1/7/2025, 5:33:21 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Computer Science - Computers and Society</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_9YHQCYUM">Preprint PDF					</li>
					<li id="item_ZPP56ED5">Snapshot					</li>
				</ul>
			</li>


			<li id="item_HQP7QPTB" class="item journalArticle">
			<h2>Large scale analysis of gender bias and sexism in song lyrics</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Lorenzo Betti</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Carlo Abrate</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Andreas Kaltenbrunner</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>We employ Natural Language Processing techniques to analyse 
377,808 English song lyrics from the “Two Million Song Database” corpus,
 focusing on the expression of sexism across five decades (1960–2010) 
and the measurement of gender biases. Using a sexism classifier, we 
identify sexist lyrics at a larger scale than previous studies using 
small samples of manually annotated popular songs. Furthermore, we 
reveal gender biases by measuring associations in word embeddings 
learned on song lyrics. We find sexist content to increase across time, 
especially from male artists and for popular songs appearing in 
Billboard charts. Songs are also shown to contain different language 
biases depending on the gender of the performer, with male solo artist 
songs containing more and stronger biases. This is the first large scale
 analysis of this type, giving insights into language usage in such an 
influential part of popular culture.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2023/12</td>
					</tr>
					<tr>
					<th>Language</th>
						<td>en</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>epjdatascience.springeropen.com</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://epjdatascience.springeropen.com/articles/10.1140/epjds/s13688-023-00384-8">https://epjdatascience.springeropen.com/articles/10.1140/epjds/s13688-023-00384-8</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>1/3/2025, 12:45:08 PM</td>
					</tr>
					<tr>
					<th>Rights</th>
						<td>2023 The Author(s)</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>Number: 1
Publisher: SpringerOpen</td>
					</tr>
					<tr>
					<th>Volume</th>
						<td>12</td>
					</tr>
					<tr>
					<th>Pages</th>
						<td>1-22</td>
					</tr>
					<tr>
					<th>Publication</th>
						<td>EPJ Data Science</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.1140/epjds/s13688-023-00384-8">10.1140/epjds/s13688-023-00384-8</a></td>
					</tr>
					<tr>
					<th>Issue</th>
						<td>1</td>
					</tr>
					<tr>
					<th>Journal Abbr</th>
						<td>EPJ Data Sci.</td>
					</tr>
					<tr>
					<th>ISSN</th>
						<td>2193-1127</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>1/3/2025, 12:45:08 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_XHXXIY77">Full Text PDF					</li>
				</ul>
			</li>


			<li id="item_YPW87EAG" class="item journalArticle">
			<h2>Latent Code Identification (LACOID): A Machine Learning-Based 
Integrative Framework [and Open-Source Software] to Classify Big Textual
 Data, Rebuild Contextualized/Unaltered Meanings, and Avoid Aggregation 
Bias</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Manuel S. González Canché</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Labeling or classifying textual data and qualitative evidence 
is an expensive and consequential challenge. The rigor and consistency 
behind the construction of these labels ultimately shape research 
ﬁndings and conclusions. A multifaceted methodological conundrum to 
address this challenge is the need for human reasoning for classiﬁcation
 that leads to deeper and more nuanced understandings; however, this 
same manual human classiﬁcation comes with the well-documented increase 
in classiﬁcation inconsistencies and errors, particularly when dealing 
with vast amounts of documents and teams of coders. An alternative to 
human coding consists of machine learning-assisted techniques. These 
data science and visualization techniques offer tools for data 
classiﬁcation that are cost-effective and consistent but are prone to 
losing participants’ meanings or voices for two main reasons: (a) these 
classiﬁcations typically aggregate all texts conﬁguring each input ﬁle 
(i.e., each interview transcript) into a single topic or code and (b) 
these words conﬁguring texts are analyzed outside of their original 
contexts. To address this challenge and analytic conundrum, we present 
an analytic framework and software tool, that addresses the following 
question: How to classify vast amounts of qualitative evidence 
effectively and efﬁciently without losing context or the original voices
 of our research participants and while leveraging the nuances that 
human reasoning bring to the qualitative and mixed methods analytic 
tables? This framework mirrors the line-by-line coding employed in 
human/manual code identiﬁcation but relying on machine learning to 
classify texts in minutes rather than months. The resulting outputs 
provide complete transparency of the classiﬁcation process and aid to 
recreate the contextualized, original, and unaltered meanings embedded 
in the input documents, as provided by our participants. We offer access
 to the database (Gonza´lez Canche´, 2022e) and software required 
(Gonza´lez Canche´, 2022a, Mac https://cutt.ly/jc7n3OT, and Windows 
https://cutt.ly/wc7nNKF) to replicate the analyses. We hope this 
opportunity to become familiar with the analytic framework and software,
 may result in expanded access of data science tools to analyze 
qualitative evidence (see also Gonza´lez Canche´ 2022b, 2022c, 2022d, 
for related no-code data science applications to classify and analyze 
qualitative and textual data dynamically).</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>12/2023</td>
					</tr>
					<tr>
					<th>Language</th>
						<td>en</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>Latent Code Identification (LACOID)</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>DOI.org (Crossref)</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://journals.sagepub.com/doi/10.1177/16094069221144940">http://journals.sagepub.com/doi/10.1177/16094069221144940</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>4/28/2024, 3:27:40 PM</td>
					</tr>
					<tr>
					<th>Volume</th>
						<td>22</td>
					</tr>
					<tr>
					<th>Pages</th>
						<td>160940692211449</td>
					</tr>
					<tr>
					<th>Publication</th>
						<td>International Journal of Qualitative Methods</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.1177/16094069221144940">10.1177/16094069221144940</a></td>
					</tr>
					<tr>
					<th>Journal Abbr</th>
						<td>International Journal of Qualitative Methods</td>
					</tr>
					<tr>
					<th>ISSN</th>
						<td>1609-4069, 1609-4069</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>4/28/2024, 3:27:40 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>4/28/2024, 3:27:40 PM</td>
					</tr>
				</tbody></table>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_RNIPBZ8B">González Canché - 2023 - Latent Code Identification (LACOID) A Machine Lea.pdf					</li>
				</ul>
			</li>


			<li id="item_YZMBULR8" class="item journalArticle">
			<h2>Latent Semantic Scaling: A Semisupervised Text Analysis Technique for New Domains and Languages</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Kohei Watanabe</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Many social scientists recognize that quantitative text 
analysis is a useful research methodology, but its application is still 
concentrated in documents written in European languages, especially 
English, and few sub-fields of political science, such as comparative 
politics and legislative studies. This seems to be due to the absence of
 flexible and cost-efficient methods that can be used to analyze 
documents in different domains and languages. Aiming to solve this 
problem, this paper proposes a semisupervised document scaling 
technique, called Latent Semantic Scaling (LSS), which can locate 
documents on various pre-defined dimensions. LSS achieves this by 
combining user-provided seed words and latent semantic analysis (word 
embedding). The article demonstrates its flexibility and efficiency in 
large-scale sentiment analysis of New York Times articles on the economy
 and Asahi Shimbun articles on politics. These examples show that LSS 
can produce results comparable to that of the Lexicoder Sentiment 
Dictionary (LSD) in both English and Japanese with only small sets of 
sentiment seed words. A new heuristic method that assists LSS users to 
choose a near-optimal number of singular values to obtain word vectors 
that best capture differences between documents on target dimensions is 
also presented.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2021-04-03</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>Latent Semantic Scaling</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>Taylor and Francis+NEJM</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://doi.org/10.1080/19312458.2020.1832976">https://doi.org/10.1080/19312458.2020.1832976</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>1/6/2025, 2:11:48 PM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>Publisher: Routledge
_eprint: https://doi.org/10.1080/19312458.2020.1832976</td>
					</tr>
					<tr>
					<th>Volume</th>
						<td>15</td>
					</tr>
					<tr>
					<th>Pages</th>
						<td>81-102</td>
					</tr>
					<tr>
					<th>Publication</th>
						<td>Communication Methods and Measures</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.1080/19312458.2020.1832976">10.1080/19312458.2020.1832976</a></td>
					</tr>
					<tr>
					<th>Issue</th>
						<td>2</td>
					</tr>
					<tr>
					<th>ISSN</th>
						<td>1931-2458</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>1/6/2025, 2:11:48 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_M68QX3LW">Full Text PDF					</li>
				</ul>
			</li>


			<li id="item_6LK6AX6B" class="item preprint">
			<h2>Learning from One and Only One Shot</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Preprint</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Haizi Yu</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Igor Mineyev</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Lav R. Varshney</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>James A. Evans</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Humans can generalize from only a few examples and from little
 pretraining on similar tasks. Yet, machine learning (ML) typically 
requires large data to learn or pre-learn to transfer. Motivated by 
nativism and artificial general intelligence, we directly model 
human-innate priors in abstract visual tasks such as character and 
doodle recognition. This yields a white-box model that learns 
general-appearance similarity by mimicking how humans naturally 
``distort'' an object at first sight. Using just nearest-neighbor 
classification on this cognitively-inspired similarity space, we achieve
 human-level recognition with only $1$--$10$ examples per class and no 
pretraining. This differs from few-shot learning that uses massive 
pretraining. In the tiny-data regime of MNIST, EMNIST, Omniglot, and 
QuickDraw benchmarks, we outperform both modern neural networks and 
classical ML. For unsupervised learning, by learning the non-Euclidean, 
general-appearance similarity space in a $k$-means style, we achieve 
multifarious visual realizations of abstract concepts by generating 
human-intuitive archetypes as cluster centroids.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2024-05-21</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>arXiv.org</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://arxiv.org/abs/2201.08815">http://arxiv.org/abs/2201.08815</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>12/24/2024, 5:18:01 PM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>arXiv:2201.08815 [cs]</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.48550/arXiv.2201.08815">10.48550/arXiv.2201.08815</a></td>
					</tr>
					<tr>
					<th>Repository</th>
						<td>arXiv</td>
					</tr>
					<tr>
					<th>Archive ID</th>
						<td>arXiv:2201.08815</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>12/24/2024, 5:18:01 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Computer Science - Artificial Intelligence</li>
					<li>Computer Science - Computer Vision and Pattern Recognition</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_N3DFL4MD">Preprint PDF					</li>
					<li id="item_PFVP4G7A">Snapshot					</li>
				</ul>
			</li>


			<li id="item_NTNSQDWY" class="item preprint">
			<h2>Leveraging Large Language Models to Detect Influence Campaigns in Social Media</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Preprint</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Luca Luceri</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Eric Boniardi</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Emilio Ferrara</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Social media influence campaigns pose significant challenges 
to public discourse and democracy. Traditional detection methods fall 
short due to the complexity and dynamic nature of social media. 
Addressing this, we propose a novel detection method using Large 
Language Models (LLMs) that incorporates both user metadata and network 
structures. By converting these elements into a text format, our 
approach effectively processes multilingual content and adapts to the 
shifting tactics of malicious campaign actors. We validate our model 
through rigorous testing on multiple datasets, showcasing its superior 
performance in identifying influence efforts. This research not only 
offers a powerful tool for detecting campaigns, but also sets the stage 
for future enhancements to keep up with the fast-paced evolution of 
social media-based influence tactics.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2023-11-14</td>
					</tr>
					<tr>
					<th>Language</th>
						<td>en</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>arXiv.org</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://arxiv.org/abs/2311.07816">http://arxiv.org/abs/2311.07816</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>1/8/2025, 2:14:07 PM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>arXiv:2311.07816 [cs]</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.48550/arXiv.2311.07816">10.48550/arXiv.2311.07816</a></td>
					</tr>
					<tr>
					<th>Repository</th>
						<td>arXiv</td>
					</tr>
					<tr>
					<th>Archive ID</th>
						<td>arXiv:2311.07816</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>1/8/2025, 2:14:07 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Computer Science - Artificial Intelligence</li>
					<li>Computer Science - Social and Information Networks</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_HE3AV4VF">Luceri et al. - 2023 - Leveraging Large Language Models to Detect Influen.pdf					</li>
				</ul>
			</li>


			<li id="item_RJ3Z4QV7" class="item journalArticle">
			<h2>Leveraging Researcher Domain Expertise to Annotate Concepts Within Imbalanced Data</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Dror K. Markus</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Guy Mor-Lan</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Tamir Sheafer</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Shaul R. Shenhav</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>As more computational communication researchers turn to 
supervised machine learning methods for text classification, we note the
 challenge in implementing such techniques within an imbalanced dataset.
 Such issues are critical in our domain, where, in many cases, 
researchers attempt to identify and study theoretically interesting 
categories that can be rare in a target corpus. Specifically, imbalanced
 distributions, with a skewed distribution of texts among the 
categories, can lead to a lengthy and expensive annotation stage, 
forcing practitioners to sample and label large numbers of texts to 
train a classification model. In this paper, we provide an overview of 
the issue, and describe existing strategies for mitigating such 
challenges. Noting the pitfalls of previous solutions, we then provide a
 semi-supervised method – Expert Initiated Latent Space Sampling – that 
complements researcher domain expertise with a systematic, unsupervised 
exploration of the latent semantic space to overcome such limitations. 
Utilizing simulations to systematically evaluate our method and compare 
it to existing approaches, we show that our procedure offers significant
 advantages in terms of efficiency and accuracy in many classification 
tasks.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2023-07-03</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>Taylor and Francis+NEJM</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://doi.org/10.1080/19312458.2023.2182278">https://doi.org/10.1080/19312458.2023.2182278</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>1/6/2025, 2:28:03 PM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>Publisher: Routledge
_eprint: https://doi.org/10.1080/19312458.2023.2182278</td>
					</tr>
					<tr>
					<th>Volume</th>
						<td>17</td>
					</tr>
					<tr>
					<th>Pages</th>
						<td>250-271</td>
					</tr>
					<tr>
					<th>Publication</th>
						<td>Communication Methods and Measures</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.1080/19312458.2023.2182278">10.1080/19312458.2023.2182278</a></td>
					</tr>
					<tr>
					<th>Issue</th>
						<td>3</td>
					</tr>
					<tr>
					<th>ISSN</th>
						<td>1931-2458</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>1/6/2025, 2:28:03 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_3GZ2UW6F">Full Text PDF					</li>
				</ul>
			</li>


			<li id="item_GWLWFQNF" class="item journalArticle">
			<h2>Like, Comment, and Share on TikTok: Exploring the Effect of 
Sentiment and Second-Person View on the User Engagement with TikTok News
 Videos</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Zicheng Cheng</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Yanlin Li</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>TikTok—the world’s most downloaded app since 2020, has become a
 place for more than silly dancing and lip-syncing. TikTok users are 
increasingly turning to TikTok for news content. Meanwhile, news 
publishers are embracing TikTok to reach a younger audience. We aim to 
examine the content strategy adopted by the most-followed news 
publishers on TikTok and how effective their TikTok strategy is in 
spurring audience engagement in terms of liking, commenting, and 
sharing. This study retrieved 101,292 TikTok news videos as of November 
22, 2022. With the help of computer vision, natural language processing,
 and sentiment analysis, we found that TikTok news videos containing 
negative sentiment and more second-person view shots are associated with
 significantly higher audience engagement. In addition, this study 
demonstrated that the TikTok video features and engagement levels differ
 between the news publishers and other TikTok creators. Moderator 
analysis shows that both the effect of negative sentiment on engagement 
and the effect of the second-person view on engagement are moderated by 
the TikTok account type. The impact of negative sentiment and 
second-person view on engagement behaviors becomes smaller or even 
insignificant for news publisher TikTok videos. Theoretical and 
practical implications are discussed in this study.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2024-02-01</td>
					</tr>
					<tr>
					<th>Language</th>
						<td>en</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>Like, Comment, and Share on TikTok</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>SAGE Journals</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://doi.org/10.1177/08944393231178603">https://doi.org/10.1177/08944393231178603</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>12/24/2024, 7:20:17 PM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>Publisher: SAGE Publications Inc</td>
					</tr>
					<tr>
					<th>Volume</th>
						<td>42</td>
					</tr>
					<tr>
					<th>Pages</th>
						<td>201-223</td>
					</tr>
					<tr>
					<th>Publication</th>
						<td>Social Science Computer Review</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.1177/08944393231178603">10.1177/08944393231178603</a></td>
					</tr>
					<tr>
					<th>Issue</th>
						<td>1</td>
					</tr>
					<tr>
					<th>ISSN</th>
						<td>0894-4393</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>12/24/2024, 7:20:17 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_Z2RNIWNY">SAGE PDF Full Text					</li>
				</ul>
			</li>


			<li id="item_BBVZ3SSJ" class="item preprint">
			<h2>Limits of Large Language Models in Debating Humans</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Preprint</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>James Flamino</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Mohammed Shahid Modi</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Boleslaw K. Szymanski</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Brendan Cross</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Colton Mikolajczyk</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Large Language Models (LLMs) have shown remarkable promise in 
their ability to interact proficiently with humans. Subsequently, their 
potential use as artificial confederates and surrogates in sociological 
experiments involving conversation is an exciting prospect. But how 
viable is this idea? This paper endeavors to test the limits of 
current-day LLMs with a pre-registered study integrating real people 
with LLM agents acting as people. The study focuses on debate-based 
opinion consensus formation in three environments: humans only, agents 
and humans, and agents only. Our goal is to understand how LLM agents 
influence humans, and how capable they are in debating like humans. We 
find that LLMs can blend in and facilitate human productivity but are 
less convincing in debate, with their behavior ultimately deviating from
 human's. We elucidate these primary failings and anticipate that LLMs 
must evolve further before being viable debaters.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2024-02-06</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>arXiv.org</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://arxiv.org/abs/2402.06049">http://arxiv.org/abs/2402.06049</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>1/7/2025, 4:51:59 PM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>arXiv:2402.06049 [cs]</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.48550/arXiv.2402.06049">10.48550/arXiv.2402.06049</a></td>
					</tr>
					<tr>
					<th>Repository</th>
						<td>arXiv</td>
					</tr>
					<tr>
					<th>Archive ID</th>
						<td>arXiv:2402.06049</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>1/7/2025, 4:51:59 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Computer Science - Artificial Intelligence</li>
					<li>Computer Science - Computation and Language</li>
					<li>Computer Science - Human-Computer Interaction</li>
					<li>Statistics - Applications</li>
				</ul>
				<h3 class="notes">Notes:</h3>
				<ul class="notes">
					<li id="item_PMSIHGE8">
<p class="plaintext">Comment: 23 pages, 6 figures, 3 tables, 21 pages of supplemental materials, 8 supplemental figures, 6 supplemental tables</p>
					</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_B3EHZ27Y">Preprint PDF					</li>
					<li id="item_3IKQL7MH">Snapshot					</li>
				</ul>
			</li>


			<li id="item_2X2MY6MP" class="item journalArticle">
			<h2>Linguistic justice as a framework for designing, developing, and managing natural language processing tools</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Julia Nee</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Genevieve Macfarlane Smith</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Alicia Sheares</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Ishita Rustagi</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>As natural language processing tools powered by big data 
become increasingly ubiquitous, questions of how to design, develop, and
 manage these tools and their impacts on diverse populations are 
pressing. We propose utilizing the concept of linguistic justice—the 
realization of equitable access to social and political life regardless 
of language—to provide a framework for examining natural language 
processing tools that learn from and use human language data. To support
 linguistic justice, we argue that natural language processing tools 
(along with the datasets that are used to train and evaluate them) must 
be examined not only from the perspective of a privileged, majority 
language user, but also from the perspectives of minoritized language 
users. Considering such perspectives can help to surface areas in which 
the data used within natural language processing tools may be (often 
inadvertently) working against linguistic justice by failing to provide 
access to information, services, or opportunities in users’ language of 
choice, underperforming for certain linguistic groups, or advancing 
harmful stereotypes that can lead to negative life outcomes for members 
of marginalized groups. At the same time, this framework can help to 
illuminate ways that these shortcomings can be addressed and allow us to
 use inclusive language data and approaches to leverage natural language
 processing technologies that advance linguistic justice.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2022-01-01</td>
					</tr>
					<tr>
					<th>Language</th>
						<td>en</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>SAGE Journals</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://doi.org/10.1177/20539517221090930">https://doi.org/10.1177/20539517221090930</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>12/23/2024, 5:58:59 PM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>Publisher: SAGE Publications Ltd</td>
					</tr>
					<tr>
					<th>Volume</th>
						<td>9</td>
					</tr>
					<tr>
					<th>Pages</th>
						<td>20539517221090930</td>
					</tr>
					<tr>
					<th>Publication</th>
						<td>Big Data &amp; Society</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.1177/20539517221090930">10.1177/20539517221090930</a></td>
					</tr>
					<tr>
					<th>Issue</th>
						<td>1</td>
					</tr>
					<tr>
					<th>ISSN</th>
						<td>2053-9517</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>12/23/2024, 5:58:59 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_EQ9XURQ8">SAGE PDF Full Text					</li>
				</ul>
			</li>


			<li id="item_UGQ9MKMK" class="item preprint">
			<h2>LLM-Assisted Content Analysis: Using Large Language Models to Support Deductive Coding</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Preprint</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Robert Chew</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>John Bollenbacher</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Michael Wenger</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Jessica Speer</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Annice Kim</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Deductive coding is a widely used qualitative research method 
for determining the prevalence of themes across documents. While useful,
 deductive coding is often burdensome and time consuming since it 
requires researchers to read, interpret, and reliably categorize a large
 body of unstructured text documents. Large language models (LLMs), like
 ChatGPT, are a class of quickly evolving AI tools that can perform a 
range of natural language processing and reasoning tasks. In this study,
 we explore the use of LLMs to reduce the time it takes for deductive 
coding while retaining the flexibility of a traditional content 
analysis. We outline the proposed approach, called LLM-assisted content 
analysis (LACA), along with an in-depth case study using GPT-3.5 for 
LACA on a publicly available deductive coding data set. Additionally, we
 conduct an empirical benchmark using LACA on 4 publicly available data 
sets to assess the broader question of how well GPT-3.5 performs across a
 range of deductive coding tasks. Overall, we find that GPT-3.5 can 
often perform deductive coding at levels of agreement comparable to 
human coders. Additionally, we demonstrate that LACA can help refine 
prompts for deductive coding, identify codes for which an LLM is 
randomly guessing, and help assess when to use LLMs vs. human coders for
 deductive coding. We conclude with several implications for future 
practice of deductive coding and related research methods.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2023-06-23</td>
					</tr>
					<tr>
					<th>Language</th>
						<td>en</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>LLM-Assisted Content Analysis</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>arXiv.org</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://arxiv.org/abs/2306.14924">http://arxiv.org/abs/2306.14924</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>1/8/2025, 3:18:33 PM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>arXiv:2306.14924 [cs]</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.48550/arXiv.2306.14924">10.48550/arXiv.2306.14924</a></td>
					</tr>
					<tr>
					<th>Repository</th>
						<td>arXiv</td>
					</tr>
					<tr>
					<th>Archive ID</th>
						<td>arXiv:2306.14924</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>1/8/2025, 3:18:33 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Computer Science - Artificial Intelligence</li>
					<li>Computer Science - Computation and Language</li>
					<li>Computer Science - Machine Learning</li>
					<li>Statistics - Applications</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_JLWFK7F2">Chew et al. - 2023 - LLM-Assisted Content Analysis Using Large Languag.pdf					</li>
				</ul>
			</li>


			<li id="item_EW2E7UXX" class="item preprint">
			<h2>LLM-Measure: Generating Valid, Consistent, and Reproducible Text-Based Measures for Social Science Research</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Preprint</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Yi Yang</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Hanyu Duan</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Jiaxin Liu</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Kar Yan Tam</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>The increasing use of text as data in social science research 
necessitates the development of valid, consistent, reproducible, and 
efficient methods for generating text-based concept measures. This paper
 presents a novel method that leverages the internal hidden states of 
large language models (LLMs) to generate these concept measures. 
Specifically, the proposed method learns a concept vector that captures 
how the LLM internally represents the target concept, then estimates the
 concept value for text data by projecting the text's LLM hidden states 
onto the concept vector. Three replication studies demonstrate the 
method's effectiveness in producing highly valid, consistent, and 
reproducible text-based measures across various social science research 
contexts, highlighting its potential as a valuable tool for the research
 community.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2024-09-19</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>LLM-Measure</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>arXiv.org</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://arxiv.org/abs/2409.12722">http://arxiv.org/abs/2409.12722</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>1/7/2025, 5:06:47 PM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>arXiv:2409.12722 [cs]</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.48550/arXiv.2409.12722">10.48550/arXiv.2409.12722</a></td>
					</tr>
					<tr>
					<th>Repository</th>
						<td>arXiv</td>
					</tr>
					<tr>
					<th>Archive ID</th>
						<td>arXiv:2409.12722</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>1/7/2025, 5:06:47 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Computer Science - Computation and Language</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_TXMGE839">Preprint PDF					</li>
					<li id="item_IYG4H6IW">Snapshot					</li>
				</ul>
			</li>


			<li id="item_KLE7D33Z" class="item preprint">
			<h2>LLM-Mirror: A Generated-Persona Approach for Survey Pre-Testing</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Preprint</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Sunwoong Kim</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Jongho Jeong</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Jin Soo Han</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Donghyuk Shin</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Surveys are widely used in social sciences to understand human
 behavior, but their implementation often involves iterative adjustments
 that demand significant effort and resources. To this end, researchers 
have increasingly turned to large language models (LLMs) to simulate 
human behavior. While existing studies have focused on distributional 
similarities, individual-level comparisons remain underexplored. 
Building upon prior work, we investigate whether providing LLMs with 
respondents' prior information can replicate both statistical 
distributions and individual decision-making patterns using Partial 
Least Squares Structural Equation Modeling (PLS-SEM), a well-established
 causal analysis method. We also introduce the concept of the 
LLM-Mirror, user personas generated by supplying respondent-specific 
information to the LLM. By comparing responses generated by the 
LLM-Mirror with actual individual survey responses, we assess its 
effectiveness in replicating individual-level outcomes. Our findings 
show that: (1) PLS-SEM analysis shows LLM-generated responses align with
 human responses, (2) LLMs, when provided with respondent-specific 
information, are capable of reproducing individual human responses, and 
(3) LLM-Mirror responses closely follow human responses at the 
individual level. These findings highlight the potential of LLMs as a 
complementary tool for pre-testing surveys and optimizing research 
design.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2024-12-05</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>LLM-Mirror</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>arXiv.org</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://arxiv.org/abs/2412.03162">http://arxiv.org/abs/2412.03162</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>1/7/2025, 5:00:59 PM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>arXiv:2412.03162 [cs]</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.48550/arXiv.2412.03162">10.48550/arXiv.2412.03162</a></td>
					</tr>
					<tr>
					<th>Repository</th>
						<td>arXiv</td>
					</tr>
					<tr>
					<th>Archive ID</th>
						<td>arXiv:2412.03162</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>1/7/2025, 5:00:59 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Computer Science - Computers and Society</li>
				</ul>
				<h3 class="notes">Notes:</h3>
				<ul class="notes">
					<li id="item_I7SXWMGM">
<p class="plaintext">Comment: 11 pages, 5 figures</p>
					</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_2B7SNUA6">Preprint PDF					</li>
					<li id="item_SWWDXPJV">Snapshot					</li>
				</ul>
			</li>


			<li id="item_8U4D5MDE" class="item journalArticle">
			<h2>Machine Learning as a Model for Cultural Learning: Teaching an Algorithm What it Means to be Fat</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Alina Arseniev-Koehler</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Jacob G. Foster</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Public culture is a powerful source of cognitive 
socialization; for example, media language is full of meanings about 
body weight. Yet it remains unclear how individuals process meanings in 
public culture. We suggest that schema learning is a core mechanism by 
which public culture becomes personal culture. We propose that a 
burgeoning approach in computational text analysis – neural word 
embeddings – can be interpreted as a formal model for cultural learning.
 Embeddings allow us to empirically model schema learning and activation
 from natural language data. We illustrate our approach by extracting 
four lower-order schemas from news articles: the gender, moral, health, 
and class meanings of body weight. Using these lower-order schemas we 
quantify how words about body weight “fill in the blanks” about gender, 
morality, health, and class. Our findings reinforce ongoing concerns 
that machine-learning models (e.g., of natural language) can encode and 
reproduce harmful human biases.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2022-11-01</td>
					</tr>
					<tr>
					<th>Language</th>
						<td>en</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>Machine Learning as a Model for Cultural Learning</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>SAGE Journals</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://doi.org/10.1177/00491241221122603">https://doi.org/10.1177/00491241221122603</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>1/6/2025, 1:15:58 PM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>Publisher: SAGE Publications Inc</td>
					</tr>
					<tr>
					<th>Volume</th>
						<td>51</td>
					</tr>
					<tr>
					<th>Pages</th>
						<td>1484-1539</td>
					</tr>
					<tr>
					<th>Publication</th>
						<td>Sociological Methods &amp; Research</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.1177/00491241221122603">10.1177/00491241221122603</a></td>
					</tr>
					<tr>
					<th>Issue</th>
						<td>4</td>
					</tr>
					<tr>
					<th>ISSN</th>
						<td>0049-1241</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>1/6/2025, 1:15:58 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_PZD4H7AZ">SAGE PDF Full Text					</li>
				</ul>
			</li>


			<li id="item_FGMVBRKW" class="item journalArticle">
			<h2>Machine Learning for Social Science: An Agnostic Approach</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Justin Grimmer</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Margaret E Roberts</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Brandon M Stewart</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Social scientists are now in an era of data abundance, and 
machine learning tools are increasingly used to extract meaning from 
data sets both massive and small. We explain how the inclusion of 
machine learning in the social sciences requires us to rethink not only 
applications of machine learning methods but also best practices in the 
social sciences. In contrast to the traditional tasks for machine 
learning in computer science and statistics, when machine learning is 
applied to social scientific data, it is used to discover new concepts, 
measure the prevalence of those concepts, assess causal effects, and 
make predictions. The abundance of data and resources facilitates the 
move away from a deductive social science to a more sequential, 
interactive, and ultimately inductive approach to inference. We explain 
how an agnostic approach to machine learning methods focused on the 
social science tasks facilitates progress across a wide range of 
questions.</td>
					</tr>
					<tr>
					<th>Language</th>
						<td>en</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>Zotero</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>3/14/2024, 5:04:31 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>3/14/2024, 5:04:31 PM</td>
					</tr>
				</tbody></table>
				<h3 class="notes">Notes:</h3>
				<ul class="notes">
					<li id="item_QBR4KEUP">
<div><div data-citation-items="%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F3957624%2Fitems%2FFGMVBRKW%22%5D%2C%22itemData%22%3A%7B%22id%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F3957624%2Fitems%2FFGMVBRKW%22%2C%22type%22%3A%22article-journal%22%2C%22abstract%22%3A%22Social%20scientists%20are%20now%20in%20an%20era%20of%20data%20abundance%2C%20and%20machine%20learning%20tools%20are%20increasingly%20used%20to%20extract%20meaning%20from%20data%20sets%20both%20massive%20and%20small.%20We%20explain%20how%20the%20inclusion%20of%20machine%20learning%20in%20the%20social%20sciences%20requires%20us%20to%20rethink%20not%20only%20applications%20of%20machine%20learning%20methods%20but%20also%20best%20practices%20in%20the%20social%20sciences.%20In%20contrast%20to%20the%20traditional%20tasks%20for%20machine%20learning%20in%20computer%20science%20and%20statistics%2C%20when%20machine%20learning%20is%20applied%20to%20social%20scientific%20data%2C%20it%20is%20used%20to%20discover%20new%20concepts%2C%20measure%20the%20prevalence%20of%20those%20concepts%2C%20assess%20causal%20effects%2C%20and%20make%20predictions.%20The%20abundance%20of%20data%20and%20resources%20facilitates%20the%20move%20away%20from%20a%20deductive%20social%20science%20to%20a%20more%20sequential%2C%20interactive%2C%20and%20ultimately%20inductive%20approach%20to%20inference.%20We%20explain%20how%20an%20agnostic%20approach%20to%20machine%20learning%20methods%20focused%20on%20the%20social%20science%20tasks%20facilitates%20progress%20across%20a%20wide%20range%20of%20questions.%22%2C%22language%22%3A%22en%22%2C%22source%22%3A%22Zotero%22%2C%22title%22%3A%22Machine%20Learning%20for%20Social%20Science%3A%20An%20Agnostic%20Approach%22%2C%22author%22%3A%5B%7B%22family%22%3A%22Grimmer%22%2C%22given%22%3A%22Justin%22%7D%2C%7B%22family%22%3A%22Roberts%22%2C%22given%22%3A%22Margaret%20E%22%7D%2C%7B%22family%22%3A%22Stewart%22%2C%22given%22%3A%22Brandon%20M%22%7D%5D%7D%7D%5D" data-schema-version="8"><p>Note</p>
<p></p>
<p>Advocating alterign soc’s “best practices” in light of ML techniques.
 I dont know how to feel about this, it could be profitable or it could 
lose something importnat too. Modern data overcomes a historicalproblem 
with data scarcity</p>
<p></p>
<p>“doubt the model but trust the validation”</p>
<p></p>
<p> techniques from ML for social research, not necessarily the full 
model. Techniques like sample splitting could describe a paradigm for 
qual sociologists to split their interview data, performing grounded 
theory to discover categories on the training data, and attempting to 
verify it on the test data. This gives the key value of verification to 
qual. Work, that grounded theory conclusion withstands this 
verification.<br><br>Discusses transfer learning, which is especially relevant given modern growth of chatGPT/LLMs</p>
<p>“While there are many dimensionality reduction methods, they are all 
based on the bottleneck principle. This is the idea that by forcing a 
high-dimensional observation to be reconstructed from a very 
low-dimensional latent variable (i.e., the bottleneck), we can distill 
the essence of the data. From a statistical perspective, this can be 
seen as removing noise or smoothing. From a social science perspective, 
we often think of the low-dimensional space as representing an 
underlying property of the observation” <span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F3957624%2Fitems%2FFGMVBRKW%22%5D%2C%22locator%22%3A%22402%22%2C%22itemData%22%3A%7B%22id%22%3A89%2C%22type%22%3A%22article-journal%22%2C%22abstract%22%3A%22Social%20scientists%20are%20now%20in%20an%20era%20of%20data%20abundance%2C%20and%20machine%20learning%20tools%20are%20increasingly%20used%20to%20extract%20meaning%20from%20data%20sets%20both%20massive%20and%20small.%20We%20explain%20how%20the%20inclusion%20of%20machine%20learning%20in%20the%20social%20sciences%20requires%20us%20to%20rethink%20not%20only%20applications%20of%20machine%20learning%20methods%20but%20also%20best%20practices%20in%20the%20social%20sciences.%20In%20contrast%20to%20the%20traditional%20tasks%20for%20machine%20learning%20in%20computer%20science%20and%20statistics%2C%20when%20machine%20learning%20is%20applied%20to%20social%20scientific%20data%2C%20it%20is%20used%20to%20discover%20new%20concepts%2C%20measure%20the%20prevalence%20of%20those%20concepts%2C%20assess%20causal%20effects%2C%20and%20make%20predictions.%20The%20abundance%20of%20data%20and%20resources%20facilitates%20the%20move%20away%20from%20a%20deductive%20social%20science%20to%20a%20more%20sequential%2C%20interactive%2C%20and%20ultimately%20inductive%20approach%20to%20inference.%20We%20explain%20how%20an%20agnostic%20approach%20to%20machine%20learning%20methods%20focused%20on%20the%20social%20science%20tasks%20facilitates%20progress%20across%20a%20wide%20range%20of%20questions.%22%2C%22language%22%3A%22en%22%2C%22source%22%3A%22Zotero%22%2C%22title%22%3A%22Machine%20Learning%20for%20Social%20Science%3A%20An%20Agnostic%20Approach%22%2C%22author%22%3A%5B%7B%22family%22%3A%22Grimmer%22%2C%22given%22%3A%22Justin%22%7D%2C%7B%22family%22%3A%22Roberts%22%2C%22given%22%3A%22Margaret%20E%22%7D%2C%7B%22family%22%3A%22Stewart%22%2C%22given%22%3A%22Brandon%20M%22%7D%5D%7D%7D%5D%2C%22properties%22%3A%7B%7D%7D">(<span class="citation-item">Grimmer et al., p. 402</span>)</span> -I’m worried about this line, because does this truly distill and reveal underlying truth?</p>
<p></p>
<p>Authors note a lack of benchmarks, for comparison across tasks, 
somethign sociologists dont do because tasks are always so different.</p>
</div></div>
					</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_IUZUWULR">Grimmer et al. - Machine Learning for Social Science An Agnostic A.pdf					</li>
				</ul>
			</li>


			<li id="item_9IUK64I2" class="item journalArticle">
			<h2>Machine learning, meaning making: On reading computer science texts</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Louise Amoore</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Alexander Campolo</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Benjamin Jacobsen</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Ludovico Rella</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Computer science tends to foreclose the reading of its texts 
by social science and humanities scholars – via code and scale, 
mathematics, black box opacities, secret or proprietary models. Yet, 
when computer science papers are read in order to better understand what
 machine learning means for societies, a form of reading is brought to 
bear that is not primarily about excavating the hidden meaning of a text
 or exposing underlying truths about science. Not strictly reading to 
make sense or to discern definitive meaning of computer science texts, 
reading is an engagement with the sense-making and meaning-making that 
takes place. We propose a strategy for reading computer science that is 
attentive to the act of reading itself, that stays close to the 
difficulty involved in all forms of reading, and that works with the 
text as already properly belonging to the ethico-politics that this 
difficulty engenders. Addressing a series of three “reading problems” – 
genre, readability, and meaning – we discuss machine learning textbooks 
and papers as sites where today's algorithmic models are actively giving
 accounts of their paradigmatic worldview. Much more than matters of 
technical definition or proof of concept, texts are sites where concepts
 are forged and contested. In our times, when the political application 
of AI and machine learning is so commonly geared to settle or predict 
difficult societal problems in advance, a reading strategy must open the
 gaps and difficulties of that which cannot be settled or resolved.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2023-01-01</td>
					</tr>
					<tr>
					<th>Language</th>
						<td>en</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>Machine learning, meaning making</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>SAGE Journals</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://doi.org/10.1177/20539517231166887">https://doi.org/10.1177/20539517231166887</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>12/23/2024, 5:42:32 PM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>Publisher: SAGE Publications Ltd</td>
					</tr>
					<tr>
					<th>Volume</th>
						<td>10</td>
					</tr>
					<tr>
					<th>Pages</th>
						<td>20539517231166887</td>
					</tr>
					<tr>
					<th>Publication</th>
						<td>Big Data &amp; Society</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.1177/20539517231166887">10.1177/20539517231166887</a></td>
					</tr>
					<tr>
					<th>Issue</th>
						<td>1</td>
					</tr>
					<tr>
					<th>ISSN</th>
						<td>2053-9517</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>12/23/2024, 5:43:07 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_TUV7W9TS">SAGE PDF Full Text					</li>
				</ul>
			</li>


			<li id="item_AWDCUKQV" class="item journalArticle">
			<h2>Machine Translation: Mining Text for Social Theory</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>James A. Evans</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Pedro Aceves</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>More of the social world lives within electronic text than 
ever before, from collective activity on the web, social media, and 
instant messaging to online transactions, government intelligence, and 
digitized libraries. This supply of text has elicited demand for natural
 language processing and machine learning tools to filter, search, and 
translate text into valuable data. We survey some of the most exciting 
computational approaches to text analysis, highlighting both supervised 
methods that extend old theories to new data and unsupervised techniques
 that discover hidden regularities worth theorizing. We then review 
recent research that uses these tools to develop social insight by 
exploring (a) collective attention and reasoning through the content of 
communication; (b) social relationships through the process of 
communication; and (c) social states, roles, and moves identified 
through heterogeneous signals within communication. We highlight social 
questions for which these advances could offer powerful new insight.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2016/07/30</td>
					</tr>
					<tr>
					<th>Language</th>
						<td>en</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>Machine Translation</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>www.annualreviews.org</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://www.annualreviews.org/content/journals/10.1146/annurev-soc-081715-074206">https://www.annualreviews.org/content/journals/10.1146/annurev-soc-081715-074206</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>9/24/2024, 10:02:46 AM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>Publisher: Annual Reviews</td>
					</tr>
					<tr>
					<th>Volume</th>
						<td>42</td>
					</tr>
					<tr>
					<th>Pages</th>
						<td>21-50</td>
					</tr>
					<tr>
					<th>Publication</th>
						<td>Annual Review of Sociology</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.1146/annurev-soc-081715-074206">10.1146/annurev-soc-081715-074206</a></td>
					</tr>
					<tr>
					<th>Issue</th>
						<td>Volume 42, 2016</td>
					</tr>
					<tr>
					<th>ISSN</th>
						<td>0360-0572, 1545-2115</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>9/24/2024, 10:02:46 AM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>9/24/2024, 10:02:46 AM</td>
					</tr>
				</tbody></table>
			</li>


			<li id="item_C4NEHIRC" class="item journalArticle">
			<h2>Machine Translation: Mining Text for Social Theory: Annual Review of Sociology</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>James A. Evans</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Pedro Aceves</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>More of the social world lives within electronic text than 
ever before, from collective activity on the web, social media, and 
instant messaging to online transactions, government intelligence, and 
digitized libraries. This supply of text has elicited demand for natural
 language processing and machine learning tools to filter, search, and 
translate text into valuable data. We survey some of the most exciting 
computational approaches to text analysis, highlighting both supervised 
methods that extend old theories to new data and unsupervised techniques
 that discover hidden regularities worth theorizing. We then review 
recent research that uses these tools to develop social insight by 
exploring (a) collective attention and reasoning through the content of 
communication; (b) social relationships through the process of 
communication; and (c) social states, roles, and moves identified 
through heterogeneous signals within communication. We highlight social 
questions for which these advances could offer powerful new insight.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2016-01-01</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>Machine Translation</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>EBSCOhost</td>
					</tr>
					<tr>
					<th>Volume</th>
						<td>42</td>
					</tr>
					<tr>
					<th>Pages</th>
						<td>21-50</td>
					</tr>
					<tr>
					<th>Publication</th>
						<td>Annual Review of Sociology</td>
					</tr>
					<tr>
					<th>ISSN</th>
						<td>03600572</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>3/14/2024, 3:14:43 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="notes">Notes:</h3>
				<ul class="notes">
					<li id="item_7X297K5N">
<div><div data-citation-items="%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F3957624%2Fitems%2FC4NEHIRC%22%5D%2C%22itemData%22%3A%7B%22id%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F3957624%2Fitems%2FC4NEHIRC%22%2C%22type%22%3A%22article-journal%22%2C%22abstract%22%3A%22More%20of%20the%20social%20world%20lives%20within%20electronic%20text%20than%20ever%20before%2C%20from%20collective%20activity%20on%20the%20web%2C%20social%20media%2C%20and%20instant%20messaging%20to%20online%20transactions%2C%20government%20intelligence%2C%20and%20digitized%20libraries.%20This%20supply%20of%20text%20has%20elicited%20demand%20for%20natural%20language%20processing%20and%20machine%20learning%20tools%20to%20filter%2C%20search%2C%20and%20translate%20text%20into%20valuable%20data.%20We%20survey%20some%20of%20the%20most%20exciting%20computational%20approaches%20to%20text%20analysis%2C%20highlighting%20both%20supervised%20methods%20that%20extend%20old%20theories%20to%20new%20data%20and%20unsupervised%20techniques%20that%20discover%20hidden%20regularities%20worth%20theorizing.%20We%20then%20review%20recent%20research%20that%20uses%20these%20tools%20to%20develop%20social%20insight%20by%20exploring%20(a)%20collective%20attention%20and%20reasoning%20through%20the%20content%20of%20communication%3B%20(b)%20social%20relationships%20through%20the%20process%20of%20communication%3B%20and%20(c)%20social%20states%2C%20roles%2C%20and%20moves%20identified%20through%20heterogeneous%20signals%20within%20communication.%20We%20highlight%20social%20questions%20for%20which%20these%20advances%20could%20offer%20powerful%20new%20insight.%22%2C%22container-title%22%3A%22Annual%20Review%20of%20Sociology%22%2C%22ISSN%22%3A%2203600572%22%2C%22page%22%3A%2221-50%22%2C%22source%22%3A%22EBSCOhost%22%2C%22title%22%3A%22Machine%20Translation%3A%20Mining%20Text%20for%20Social%20Theory%3A%20Annual%20Review%20of%20Sociology%22%2C%22title-short%22%3A%22Machine%20Translation%22%2C%22volume%22%3A%2242%22%2C%22author%22%3A%5B%7B%22family%22%3A%22Evans%22%2C%22given%22%3A%22James%20A.%22%7D%2C%7B%22family%22%3A%22Aceves%22%2C%22given%22%3A%22Pedro%22%7D%5D%2C%22issued%22%3A%7B%22date-parts%22%3A%5B%5B%222016%22%2C1%2C1%5D%5D%7D%7D%7D%5D" data-schema-version="8"><p>Note</p>
<p>with the riseof massive online social behavior, it becomes a demand that NLP/ML techniques be adopted for research<br><br>NLP
 can 1) learn and apply meaningful textual classification that is too 
resource intensive to manually code, and 2) ‘discover’ unnoticed, 
surprising pattern, though often at the cost of subtlety, depth, and 
context</p>
<p></p>
<p><span class="highlight" data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F3957624%2Fitems%2FWVCLSL4C%22%2C%22pageLabel%22%3A%2225%22%2C%22position%22%3A%7B%22pageIndex%22%3A4%2C%22rects%22%3A%5B%5B365.589%2C568.09%2C451.873%2C577.245%5D%2C%5B87.246%2C556.138%2C200.973%2C565.292%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F3957624%2Fitems%2FC4NEHIRC%22%5D%2C%22locator%22%3A%2225%22%7D%7D">“Machine memory augments human analytical limits”</span> <span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F3957624%2Fitems%2FC4NEHIRC%22%5D%2C%22locator%22%3A%2225%22%2C%22itemData%22%3A%7B%22id%22%3A76%2C%22type%22%3A%22article-journal%22%2C%22abstract%22%3A%22More%20of%20the%20social%20world%20lives%20within%20electronic%20text%20than%20ever%20before%2C%20from%20collective%20activity%20on%20the%20web%2C%20social%20media%2C%20and%20instant%20messaging%20to%20online%20transactions%2C%20government%20intelligence%2C%20and%20digitized%20libraries.%20This%20supply%20of%20text%20has%20elicited%20demand%20for%20natural%20language%20processing%20and%20machine%20learning%20tools%20to%20filter%2C%20search%2C%20and%20translate%20text%20into%20valuable%20data.%20We%20survey%20some%20of%20the%20most%20exciting%20computational%20approaches%20to%20text%20analysis%2C%20highlighting%20both%20supervised%20methods%20that%20extend%20old%20theories%20to%20new%20data%20and%20unsupervised%20techniques%20that%20discover%20hidden%20regularities%20worth%20theorizing.%20We%20then%20review%20recent%20research%20that%20uses%20these%20tools%20to%20develop%20social%20insight%20by%20exploring%20(a)%20collective%20attention%20and%20reasoning%20through%20the%20content%20of%20communication%3B%20(b)%20social%20relationships%20through%20the%20process%20of%20communication%3B%20and%20(c)%20social%20states%2C%20roles%2C%20and%20moves%20identified%20through%20heterogeneous%20signals%20within%20communication.%20We%20highlight%20social%20questions%20for%20which%20these%20advances%20could%20offer%20powerful%20new%20insight.%22%2C%22container-title%22%3A%22Annual%20Review%20of%20Sociology%22%2C%22ISSN%22%3A%2203600572%22%2C%22page%22%3A%2221-50%22%2C%22source%22%3A%22EBSCOhost%22%2C%22title%22%3A%22Machine%20Translation%3A%20Mining%20Text%20for%20Social%20Theory%3A%20Annual%20Review%20of%20Sociology%22%2C%22title-short%22%3A%22Machine%20Translation%22%2C%22volume%22%3A%2242%22%2C%22author%22%3A%5B%7B%22family%22%3A%22Evans%22%2C%22given%22%3A%22James%20A.%22%7D%2C%7B%22family%22%3A%22Aceves%22%2C%22given%22%3A%22Pedro%22%7D%5D%2C%22issued%22%3A%7B%22date-parts%22%3A%5B%5B%222016%22%2C1%2C1%5D%5D%7D%7D%7D%5D%2C%22properties%22%3A%7B%7D%7D">(<span class="citation-item">Evans and Aceves, 2016, p. 25</span>)</span></p>
<p>Includes a cool diagram about different ways to tag thesame statement</p>
</div></div>
					</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_VL3QKR9V">EBSCO Record					</li>
					<li id="item_WVCLSL4C">Evans and Aceves - 2016 - Machine Translation Mining Text for Social Theory.pdf					</li>
				</ul>
			</li>


			<li id="item_A5ZWWKHI" class="item preprint">
			<h2>Machine-assisted quantitizing designs: augmenting humanities and social sciences with artificial intelligence</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Preprint</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Andres Karjus</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>The increasing capacities of large language models (LLMs) have
 been shown to present an unprecedented opportunity to scale up data 
analytics in the humanities and social sciences, by automating complex 
qualitative tasks otherwise typically carried out by human researchers. 
While numerous benchmarking studies have assessed the analytic prowess 
of LLMs, there is less focus on operationalizing this capacity for 
inference and hypothesis testing. Addressing this challenge, a 
systematic framework is argued for here, building on mixed methods 
quantitizing and converting design principles, and feature analysis from
 linguistics, to transparently integrate human expertise and machine 
scalability. Replicability and statistical robustness are discussed, 
including how to incorporate machine annotator error rates in subsequent
 inference. The approach is discussed and demonstrated in over a dozen 
LLM-assisted case studies, covering 9 diverse languages, multiple 
disciplines and tasks, including analysis of themes, stances, ideas, and
 genre compositions; linguistic and semantic annotation, interviews, 
text mining and event cause inference in noisy historical data, literary
 social network construction, metadata imputation, and multimodal visual
 cultural analytics. Using hypothesis-driven topic classification 
instead of "distant reading" is discussed. The replications among the 
experiments also illustrate how tasks previously requiring protracted 
team effort or complex computational pipelines can now be accomplished 
by an LLM-assisted scholar in a fraction of the time. Importantly, the 
approach is not intended to replace, but to augment and scale researcher
 expertise and analytic practices. With these opportunities in sight, 
qualitative skills and the ability to pose insightful questions have 
arguably never been more critical.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2024-10-20</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>Machine-assisted quantitizing designs</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>arXiv.org</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://arxiv.org/abs/2309.14379">http://arxiv.org/abs/2309.14379</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>1/7/2025, 5:41:15 PM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>arXiv:2309.14379 [cs]</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.48550/arXiv.2309.14379">10.48550/arXiv.2309.14379</a></td>
					</tr>
					<tr>
					<th>Repository</th>
						<td>arXiv</td>
					</tr>
					<tr>
					<th>Archive ID</th>
						<td>arXiv:2309.14379</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>1/7/2025, 5:41:15 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Computer Science - Artificial Intelligence</li>
					<li>Computer Science - Computation and Language</li>
					<li>Computer Science - Computers and Society</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_2XC789PH">Preprint PDF					</li>
					<li id="item_4EKGG23X">Snapshot					</li>
				</ul>
			</li>


			<li id="item_JK5EVGCV" class="item journalArticle">
			<h2>Mapping the multidimensional trend of generative AI: A bibliometric analysis and qualitative thematic review</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Dragoș M. Obreja</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Răzvan Rughiniș</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Daniel Rosner</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Generative artificial intelligence (AI) represents an 
increasingly popular topic that is visible even in most research areas 
within the social sciences and humanities fields. However, little 
attention has been paid to the knowledge dimensions reflecting the 
potential macro-social implications of generative technologies. This 
study utilizes a two-fold methodology, consisting of a bibliometric 
analysis of articles published in the last decade (N&nbsp;=&nbsp;484) 
and a subsequent qualitative thematic review of the most influential 
articles in each research area (N&nbsp;=&nbsp;246). The objective is to 
investigate the main conceptual dimensions associated with generative AI
 in the social sciences. Applying a thematic analysis framework, we 
notice that the most popular dimensions are technological, ethical, and 
social. These dimensions primarily focus on investigating the 
implications of the generative use of AI on employees in professional 
sectors as well as on students and teachers in the educational 
environment. Moreover, the political dimension reflects macro-social 
consequences on governance and legal components related to ensuring 
social protection for professions that risk becoming obsolete due to the
 widespread adoption of ChatGPT-type technologies. Overall, our research
 emphasizes concrete scholarly tensions through which generative 
AI-based technologies are predominantly encouraged in the educational 
and organizational sectors, but the potential risks associated with 
copyright infringement and job loss might constitute important drivers 
of social change. We also notice that a Foucauldian power/knowledge 
framework would prove useful in understanding the underdiscussed effects
 of generative AI on the societal/macro level.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2025-03-01</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>Mapping the multidimensional trend of generative AI</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>ScienceDirect</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://www.sciencedirect.com/science/article/pii/S2451958824002094">https://www.sciencedirect.com/science/article/pii/S2451958824002094</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>1/8/2025, 3:30:08 PM</td>
					</tr>
					<tr>
					<th>Volume</th>
						<td>17</td>
					</tr>
					<tr>
					<th>Pages</th>
						<td>100576</td>
					</tr>
					<tr>
					<th>Publication</th>
						<td>Computers in Human Behavior Reports</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.1016/j.chbr.2024.100576">10.1016/j.chbr.2024.100576</a></td>
					</tr>
					<tr>
					<th>Journal Abbr</th>
						<td>Computers in Human Behavior Reports</td>
					</tr>
					<tr>
					<th>ISSN</th>
						<td>2451-9588</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>1/8/2025, 3:30:08 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Bibliometrics</li>
					<li>ChatGPT</li>
					<li>Copyright</li>
					<li>Ethical implications</li>
					<li>Generative AI</li>
					<li>Knowledge dimensions</li>
					<li>Thematic review</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_G6EEMH5B">ScienceDirect Snapshot					</li>
				</ul>
			</li>


			<li id="item_AGJEDBAX" class="item preprint">
			<h2>MarxistLLM: Fine-tuning a language model with a Marxist worldview</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Preprint</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Matti Nelimarkka</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Theoretical perspectives, such as rational choice theory, 
Marxist theory, and feminist theory, provide a defined worldview 
(Weltanschauung) that guides researchers' analytical and interpretive 
processes.
Recently, there has been an increasing interest in using language models
 to examine social aspects through social science lenses.
However, critical computing scholars have shown that models embody 
specific societal values and perspectives.
This inherent encoding of viewpoints means that when these models are 
employed in research, scholars might not reflect the worldview or use 
theoretical perspectives.
To address this issue, our study explores the possibility of fine-tuning
 large language models with a specific Weltanschauung. 
Specifically, we incorporate the writings of Marx and Engels to 
fine-tune these models, aiming to infuse them with Marxist ideological 
terminology and worldview.
We evaluate how these fine-tuned models differ in empirical analysis.
This investigation underscores the importance of aligning theoretical 
perspectives with computational tools to enhance the social sciences.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2024-08-22</td>
					</tr>
					<tr>
					<th>Language</th>
						<td>en-us</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>MarxistLLM</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>OSF Preprints</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://osf.io/fkj63">https://osf.io/fkj63</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>1/6/2025, 10:59:20 AM</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.31235/osf.io/fkj63">10.31235/osf.io/fkj63</a></td>
					</tr>
					<tr>
					<th>Repository</th>
						<td>OSF</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>1/6/2025, 10:59:20 AM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_VCLPB6X3">OSF Preprint					</li>
				</ul>
			</li>


			<li id="item_GCURR6V5" class="item preprint">
			<h2>Measuring Implicit Bias in Explicitly Unbiased Large Language Models</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Preprint</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Xuechunzi Bai</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Angelina Wang</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Ilia Sucholutsky</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Thomas L. Griffiths</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Large language models (LLMs) can pass explicit social bias 
tests but still harbor implicit biases, similar to humans who endorse 
egalitarian beliefs yet exhibit subtle biases. Measuring such implicit 
biases can be a challenge: as LLMs become increasingly proprietary, it 
may not be possible to access their embeddings and apply existing bias 
measures; furthermore, implicit biases are primarily a concern if they 
affect the actual decisions that these systems make. We address both 
challenges by introducing two new measures of bias: LLM Implicit Bias, a
 prompt-based method for revealing implicit bias; and LLM Decision Bias,
 a strategy to detect subtle discrimination in decision-making tasks. 
Both measures are based on psychological research: LLM Implicit Bias 
adapts the Implicit Association Test, widely used to study the automatic
 associations between concepts held in human minds; and LLM Decision 
Bias operationalizes psychological results indicating that relative 
evaluations between two candidates, not absolute evaluations assessing 
each independently, are more diagnostic of implicit biases. Using these 
measures, we found pervasive stereotype biases mirroring those in 
society in 8 value-aligned models across 4 social categories (race, 
gender, religion, health) in 21 stereotypes (such as race and 
criminality, race and weapons, gender and science, age and negativity). 
Our prompt-based LLM Implicit Bias measure correlates with existing 
language model embedding-based bias methods, but better predicts 
downstream behaviors measured by LLM Decision Bias. These new 
prompt-based measures draw from psychology's long history of research 
into measuring stereotype biases based on purely observable behavior; 
they expose nuanced biases in proprietary value-aligned LLMs that appear
 unbiased according to standard benchmarks.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2024-05-23</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>arXiv.org</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://arxiv.org/abs/2402.04105">http://arxiv.org/abs/2402.04105</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>1/8/2025, 2:52:42 PM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>arXiv:2402.04105 [cs]</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.48550/arXiv.2402.04105">10.48550/arXiv.2402.04105</a></td>
					</tr>
					<tr>
					<th>Repository</th>
						<td>arXiv</td>
					</tr>
					<tr>
					<th>Archive ID</th>
						<td>arXiv:2402.04105</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>1/8/2025, 2:52:42 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Computer Science - Computation and Language</li>
					<li>Computer Science - Computers and Society</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_Y3SUI6KT">Preprint PDF					</li>
					<li id="item_XIVD94RW">Snapshot					</li>
				</ul>
			</li>


			<li id="item_8BCFVJQ8" class="item journalArticle">
			<h2>Mental disorder and suicidal ideation detection from social media using deep neural networks</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Özay Ezerceli</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Rahim Dehkharghani</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Depression and suicidal ideation are global reasons for 
life-threatening injury and death. Mental disorders have increased 
especially among young people in recent years, and early detection of 
those cases can prevent suicide attempts. Social media platforms provide
 users with an anonymous space to interact with others, making them a 
secure environment to discuss their mental disorders. This paper 
proposes a solution to detect depression/suicidal ideation using natural
 language processing and deep learning techniques. We used Transformers 
and a unique model to train the proposed model and applied it to three 
different datasets: SuicideDetection, CEASEv2.0, and SWMH. The proposed 
model is evaluated using the accuracy, precision, recall, and ROC curve.
 The proposed model outperforms the state-of-the-art in the 
SuicideDetection and CEASEv2.0 datasets, achieving F1 scores of 0.97 and
 0.75, respectively. However, in the SWMH data set, the proposed model 
is 4% points behind the state-of-the-art precision providing the F1 
score of 0.68. In the real world, this project could help psychologists 
in the early detection of depression and suicidal ideation for a more 
efficient treatment. The proposed model achieves state-of-the-art 
performance in two of the three datasets, so they could be used to 
develop a screening tool that could be used by mental health 
professionals or individuals to assess their own risk of suicide. This 
could lead to early intervention and treatment, which could save lives.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2024-12-01</td>
					</tr>
					<tr>
					<th>Language</th>
						<td>en</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>Springer Link</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://doi.org/10.1007/s42001-024-00307-1">https://doi.org/10.1007/s42001-024-00307-1</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>1/3/2025, 2:09:25 PM</td>
					</tr>
					<tr>
					<th>Volume</th>
						<td>7</td>
					</tr>
					<tr>
					<th>Pages</th>
						<td>2277-2307</td>
					</tr>
					<tr>
					<th>Publication</th>
						<td>Journal of Computational Social Science</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.1007/s42001-024-00307-1">10.1007/s42001-024-00307-1</a></td>
					</tr>
					<tr>
					<th>Issue</th>
						<td>3</td>
					</tr>
					<tr>
					<th>Journal Abbr</th>
						<td>J Comput Soc Sc</td>
					</tr>
					<tr>
					<th>ISSN</th>
						<td>2432-2725</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>1/3/2025, 2:09:25 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Artificial Intelligence</li>
					<li>BERT transformers</li>
					<li>Deep neural network</li>
					<li>Social media content</li>
					<li>Suicidal ideation detection</li>
					<li>Word embedding</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_WUQXLWSW">Full Text PDF					</li>
				</ul>
			</li>


			<li id="item_8VACJKDU" class="item journalArticle">
			<h2>Methodological advances in quantitative social science: In 
celebration of the &lt;i&gt;Social Science Research&lt;/i&gt; 50th 
anniversary</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Weihua An</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Shawn Bauldry</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>The past 50 years have observed massive growth in more 
specialized methods that are tailored to analyze data with specific 
structures or in a particular domain. These include methods for 
assessing measurement, categorical data analysis, sequence analysis, 
social network analysis, demographic methods, methods for studying 
social mobility, etc.
Measurement invariance, sometimes also referred to as measurement 
equivalence or measurement comparability, indicates whether a concept is
 measured</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2023-02-01</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>Methodological advances in quantitative social science</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>ScienceDirect</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://www.sciencedirect.com/science/article/pii/S0049089X22001582">https://www.sciencedirect.com/science/article/pii/S0049089X22001582</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>1/7/2025, 4:09:05 PM</td>
					</tr>
					<tr>
					<th>Volume</th>
						<td>110</td>
					</tr>
					<tr>
					<th>Pages</th>
						<td>102843</td>
					</tr>
					<tr>
					<th>Publication</th>
						<td>Social Science Research</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.1016/j.ssresearch.2022.102843">10.1016/j.ssresearch.2022.102843</a></td>
					</tr>
					<tr>
					<th>Journal Abbr</th>
						<td>Social Science Research</td>
					</tr>
					<tr>
					<th>ISSN</th>
						<td>0049-089X</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>1/7/2025, 4:09:06 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 8:09:08 PM</td>
					</tr>
				</tbody></table>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_3VGUKVH9">An and Bauldry - 2023 - Methodological advances in quantitative social sci.pdf					</li>
					<li id="item_3X6BE6D6">ScienceDirect Snapshot					</li>
				</ul>
			</li>


			<li id="item_D9V3M952" class="item preprint">
			<h2>Mixed Membership Word Embeddings for Computational Social Science</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Preprint</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>James Foulds</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Word embeddings improve the performance of NLP systems by 
revealing the hidden structural relationships between words. Despite 
their success in many applications, word embeddings have seen very 
little use in computational social science NLP tasks, presumably due to 
their reliance on big data, and to a lack of interpretability. I propose
 a probabilistic model-based word embedding method which can recover 
interpretable embeddings, without big data. The key insight is to 
leverage mixed membership modeling, in which global representations are 
shared, but individual entities (i.e. dictionary words) are free to use 
these representations to uniquely differing degrees. I show how to train
 the model using a combination of state-of-the-art training techniques 
for word embeddings and topic models. The experimental results show an 
improvement in predictive language modeling of up to 63% in MRR over the
 skip-gram, and demonstrate that the representations are beneficial for 
supervised learning. I illustrate the interpretability of the models 
with computational social science case studies on State of the Union 
addresses and NIPS articles.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2018-02-20</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>arXiv.org</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://arxiv.org/abs/1705.07368">http://arxiv.org/abs/1705.07368</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>1/8/2025, 12:56:43 PM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>arXiv:1705.07368 [cs]</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.48550/arXiv.1705.07368">10.48550/arXiv.1705.07368</a></td>
					</tr>
					<tr>
					<th>Repository</th>
						<td>arXiv</td>
					</tr>
					<tr>
					<th>Archive ID</th>
						<td>arXiv:1705.07368</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>1/8/2025, 12:56:43 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Computer Science - Artificial Intelligence</li>
					<li>Computer Science - Computation and Language</li>
					<li>Computer Science - Machine Learning</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_PGPY68RC">Preprint PDF					</li>
					<li id="item_BB7KYQG2">Snapshot					</li>
				</ul>
			</li>


			<li id="item_VNUEIU7V" class="item preprint">
			<h2>Modeling Human Subjectivity in LLMs Using Explicit and Implicit Human Factors in Personas</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Preprint</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Salvatore Giorgi</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Tingting Liu</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Ankit Aich</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Kelsey Isman</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Garrick Sherman</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Zachary Fried</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>João Sedoc</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Lyle H. Ungar</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Brenda Curtis</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Large language models (LLMs) are increasingly being used in 
human-centered social scientific tasks, such as data annotation, 
synthetic data creation, and engaging in dialog. However, these tasks 
are highly subjective and dependent on human factors, such as one's 
environment, attitudes, beliefs, and lived experiences. Thus, it may be 
the case that employing LLMs (which do not have such human factors) in 
these tasks results in a lack of variation in data, failing to reflect 
the diversity of human experiences. In this paper, we examine the role 
of prompting LLMs with human-like personas and asking the models to 
answer as if they were a specific human. This is done explicitly, with 
exact demographics, political beliefs, and lived experiences, or 
implicitly via names prevalent in specific populations. The LLM personas
 are then evaluated via (1) subjective annotation task (e.g., detecting 
toxicity) and (2) a belief generation task, where both tasks are known 
to vary across human factors. We examine the impact of explicit vs. 
implicit personas and investigate which human factors LLMs recognize and
 respond to. Results show that explicit LLM personas show mixed results 
when reproducing known human biases, but generally fail to demonstrate 
implicit biases. We conclude that LLMs may capture the statistical 
patterns of how people speak, but are generally unable to model the 
complex interactions and subtleties of human perceptions, potentially 
limiting their effectiveness in social science applications.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2024-10-17</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>arXiv.org</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://arxiv.org/abs/2406.14462">http://arxiv.org/abs/2406.14462</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>1/7/2025, 5:27:00 PM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>arXiv:2406.14462 [cs]</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.48550/arXiv.2406.14462">10.48550/arXiv.2406.14462</a></td>
					</tr>
					<tr>
					<th>Repository</th>
						<td>arXiv</td>
					</tr>
					<tr>
					<th>Archive ID</th>
						<td>arXiv:2406.14462</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>1/7/2025, 5:27:00 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Computer Science - Computation and Language</li>
				</ul>
				<h3 class="notes">Notes:</h3>
				<ul class="notes">
					<li id="item_U72D523U">
<p class="plaintext">Comment: Accepted at Findings of EMNLP 2024</p>
					</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_MHULIK7F">Preprint PDF					</li>
					<li id="item_36X5H7TU">Snapshot					</li>
				</ul>
			</li>


			<li id="item_MF5FKI4C" class="item journalArticle">
			<h2>Monitoring Looting at Cultural Heritage Sites: Applying Deep Learning on Optical Unmanned Aerial Vehicles Data as a Solution</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Mark Altaweel</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Adel Khelifi</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Mohammad Maher Shana’ah</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>The looting of cultural heritage sites has been a growing 
problem and threatens national economies, social identity, destroys 
research potential, and traumatizes communities. For many countries, the
 challenge in protecting heritage is that there are often too few 
resources, particularly paid site guards, while sites can also be in 
remote locations. Here, we develop a new approach that applies deep 
learning methods to detect the presence of looting at heritage sites 
using optical imagery from unmanned aerial vehicles (UAVs). We present 
results that demonstrate the accuracy, precision, and recall of our 
approach. Results show that optical UAV data can be an easy way for 
authorities to monitor heritage sites, demonstrating the utility of deep
 learning in aiding the protection of heritage sites by automating the 
detection of any new damage to sites. We discuss the impact and 
potential for deep learning to be used as a tool for the protection of 
heritage sites. How the approach could be improved with new data is also
 discussed. Additionally, the code and data used are provided as part of
 the outputs.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2024-04-01</td>
					</tr>
					<tr>
					<th>Language</th>
						<td>en</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>Monitoring Looting at Cultural Heritage Sites</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>SAGE Journals</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://doi.org/10.1177/08944393231188471">https://doi.org/10.1177/08944393231188471</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>12/24/2024, 7:10:55 PM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>Publisher: SAGE Publications Inc</td>
					</tr>
					<tr>
					<th>Volume</th>
						<td>42</td>
					</tr>
					<tr>
					<th>Pages</th>
						<td>480-495</td>
					</tr>
					<tr>
					<th>Publication</th>
						<td>Social Science Computer Review</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.1177/08944393231188471">10.1177/08944393231188471</a></td>
					</tr>
					<tr>
					<th>Issue</th>
						<td>2</td>
					</tr>
					<tr>
					<th>ISSN</th>
						<td>0894-4393</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>12/24/2024, 7:10:55 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_7EFW9VH5">SAGE PDF Full Text					</li>
				</ul>
			</li>


			<li id="item_X5AX5YB9" class="item preprint">
			<h2>Multiclass Classification of Policy Documents with Large Language Models</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Preprint</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Erkan Gunes</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Christoffer Koch Florczak</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Classifying policy documents into policy issue topics has been
 a long-time effort in political science and communication disciplines. 
Efforts to automate text classification processes for social science 
research purposes have so far achieved remarkable results, but there is 
still a large room for progress. In this work, we test the prediction 
performance of an alternative strategy, which requires human involvement
 much less than full manual coding. We use the GPT 3.5 and GPT 4 models 
of the OpenAI, which are pre-trained instruction-tuned Large Language 
Models (LLM), to classify congressional bills and congressional hearings
 into Comparative Agendas Project's 21 major policy issue topics. We 
propose three use-case scenarios and estimate overall accuracies ranging
 from %58-83 depending on scenario and GPT model employed. The three 
scenarios aims at minimal, moderate, and major human interference, 
respectively. Overall, our results point towards the insufficiency of 
complete reliance on GPT with minimal human intervention, an increasing 
accuracy along with the human effort exerted, and a surprisingly high 
accuracy achieved in the most humanly demanding use-case. However, the 
superior use-case achieved the %83 accuracy on the %65 of the data in 
which the two models agreed, suggesting that a similar approach to ours 
can be relatively easily implemented and allow for mostly automated 
coding of a majority of a given dataset. This could free up resources 
allowing manual human coding of the remaining %35 of the data to achieve
 an overall higher level of accuracy while reducing costs significantly.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2023-10-12</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>arXiv.org</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://arxiv.org/abs/2310.08167">http://arxiv.org/abs/2310.08167</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>1/7/2025, 5:41:39 PM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>arXiv:2310.08167 [cs]</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.48550/arXiv.2310.08167">10.48550/arXiv.2310.08167</a></td>
					</tr>
					<tr>
					<th>Repository</th>
						<td>arXiv</td>
					</tr>
					<tr>
					<th>Archive ID</th>
						<td>arXiv:2310.08167</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>1/7/2025, 5:41:40 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Computer Science - Computation and Language</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_5IA5MB9I">Preprint PDF					</li>
					<li id="item_UI6A6PQA">Snapshot					</li>
				</ul>
			</li>


			<li id="item_B9VMLDN7" class="item preprint">
			<h2>Navigating AI in Social Work and Beyond: A Multidisciplinary Review</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Preprint</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Matt Victor Dalziel</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Krystal Schaffer</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Neil Martin</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>This review began with the modest goal of drafting a brief 
commentary on how the social work profession engages with and is 
impacted by artificial intelligence (AI). However, it quickly became 
apparent that a deeper exploration was required to adequately capture 
the profound influence of AI, one of the most transformative and debated
 innovations in modern history. As a result, this review evolved into an
 interdisciplinary endeavour, gathering seminal texts, critical 
articles, and influential voices from across industries and academia. 
This review aims to provide a comprehensive yet accessible overview, 
situating AI within broader societal and academic conversations as 2025 
dawns. We explore perspectives from leading tech entrepreneurs, cultural
 icons, CEOs, and politicians alongside the pioneering contributions of 
AI engineers, innovators, and academics from fields as diverse as 
mathematics, sociology, philosophy, economics, and more. This review 
also briefly analyses AI's real-world impacts, ethical challenges, and 
implications for social work. It presents a vision for AI-facilitated 
simulations that could transform social work education through Advanced 
Personalised Simulation Training (APST). This tool uses AI to tailor 
high-fidelity simulations to individual student needs, providing 
real-time feedback and preparing them for the complexities of their 
future practice environments. We maintain a critical tone throughout, 
balancing our awe of AI's remarkable advancements with necessary 
caution. As AI continues to permeate every professional realm, 
understanding its subtleties, challenges, and opportunities becomes 
essential. Those who fully grasp the intricacies of this technology will
 be best positioned to navigate the impending AI Era.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2024-10-25</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>Navigating AI in Social Work and Beyond</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>arXiv.org</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://arxiv.org/abs/2411.07245">http://arxiv.org/abs/2411.07245</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>1/7/2025, 4:34:05 PM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>arXiv:2411.07245 [cs]</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.48550/arXiv.2411.07245">10.48550/arXiv.2411.07245</a></td>
					</tr>
					<tr>
					<th>Repository</th>
						<td>arXiv</td>
					</tr>
					<tr>
					<th>Archive ID</th>
						<td>arXiv:2411.07245</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>1/7/2025, 4:34:05 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Computer Science - Artificial Intelligence</li>
					<li>Computer Science - Computers and Society</li>
				</ul>
				<h3 class="notes">Notes:</h3>
				<ul class="notes">
					<li id="item_TGMATWRT">
<p class="plaintext">Comment: 30 pages</p>
					</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_EFYI2CWY">Preprint PDF					</li>
					<li id="item_DAQ5ZNKQ">Snapshot					</li>
				</ul>
			</li>


			<li id="item_A6MEX4W8" class="item preprint">
			<h2>Navigating Prompt Complexity for Zero-Shot Classification: A Study of Large Language Models in Computational Social Science</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Preprint</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Yida Mu</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Ben P. Wu</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>William Thorne</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Ambrose Robinson</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Nikolaos Aletras</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Carolina Scarton</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Kalina Bontcheva</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Xingyi Song</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Instruction-tuned Large Language Models (LLMs) have exhibited 
impressive language understanding and the capacity to generate responses
 that follow specific prompts. However, due to the computational demands
 associated with training these models, their applications often adopt a
 zero-shot setting. In this paper, we evaluate the zero-shot performance
 of two publicly accessible LLMs, ChatGPT and OpenAssistant, in the 
context of six Computational Social Science classification tasks, while 
also investigating the effects of various prompting strategies. Our 
experiments investigate the impact of prompt complexity, including the 
effect of incorporating label definitions into the prompt; use of 
synonyms for label names; and the influence of integrating past memories
 during foundation model training. The findings indicate that in a 
zero-shot setting, current LLMs are unable to match the performance of 
smaller, fine-tuned baseline transformer models (such as BERT-large). 
Additionally, we find that different prompting strategies can 
significantly affect classification accuracy, with variations in 
accuracy and F1 scores exceeding 10\%.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2024-03-24</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>Navigating Prompt Complexity for Zero-Shot Classification</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>arXiv.org</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://arxiv.org/abs/2305.14310">http://arxiv.org/abs/2305.14310</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>1/7/2025, 5:39:04 PM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>arXiv:2305.14310 [cs]</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.48550/arXiv.2305.14310">10.48550/arXiv.2305.14310</a></td>
					</tr>
					<tr>
					<th>Repository</th>
						<td>arXiv</td>
					</tr>
					<tr>
					<th>Archive ID</th>
						<td>arXiv:2305.14310</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>1/7/2025, 5:39:04 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Computer Science - Computation and Language</li>
				</ul>
				<h3 class="notes">Notes:</h3>
				<ul class="notes">
					<li id="item_VT96T9BR">
<p class="plaintext">Comment: Accepted at LREC-COLING 2024</p>
					</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_4CKTTSK7">Preprint PDF					</li>
					<li id="item_MS55VN2C">Snapshot					</li>
				</ul>
			</li>


			<li id="item_CDMCA9EG" class="item bookSection">
			<h2>nCoder+: A Semantic Tool for Improving Recall of nCoder Coding</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Book Section</td>
					</tr>
					<tr>
						<th class="editor">Editor</th>
						<td>Brendan Eagan</td>
					</tr>
					<tr>
						<th class="editor">Editor</th>
						<td>Morten Misfeldt</td>
					</tr>
					<tr>
						<th class="editor">Editor</th>
						<td>Amanda Siebert-Evenstone</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Zhiqiang Cai</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Amanda Siebert-Evenstone</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Brendan Eagan</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>David Williamson Shaffer</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Xiangen Hu</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Arthur C. Graesser</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Coding is a process of assigning meaning to a given piece of 
evidence. Evidence may be found in a variety of data types, including 
documents, research interviews, posts from social media, conversations 
from learning platforms, or any source of data that may provide insights
 for the questions under qualitative study. In this study, we focus on 
text data and consider coding as a process of identifying words or 
phrases and categorizing them into codes to facilitate data analysis. 
There are a number of different approaches to generating qualitative 
codes, such as grounded coding, a priori coding, or using both in an 
iterative process. However, both qualitative and quantitative analysts 
face the same coding problem: when the data size is large, manually 
coding becomes impractical. nCoder is a tool that helps researchers to 
discover and code key concepts in text data with minimum human 
judgements. Once reliability and validity are established, nCoder 
automatically applies the coding scheme to the dataset. However, for 
concepts that occur infrequently, even with an acceptable reliability, 
the classiﬁer may still result in too many false negatives. This paper 
explores these problems within the current nCoder and proposes adding a 
semantic component to the nCoder. A tool called “nCoder+” is presented 
with real data to demonstrate the usefulness of the semantic component. 
The possible ways of integrating this component and other natural 
language processing techniques into nCoder are discussed.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2019</td>
					</tr>
					<tr>
					<th>Language</th>
						<td>en</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>nCoder+</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>DOI.org (Crossref)</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://link.springer.com/10.1007/978-3-030-33232-7_4">https://link.springer.com/10.1007/978-3-030-33232-7_4</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>4/28/2024, 8:16:48 PM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>Series Title: Communications in Computer and Information Science
DOI: 10.1007/978-3-030-33232-7_4</td>
					</tr>
					<tr>
					<th>Volume</th>
						<td>1112</td>
					</tr>
					<tr>
					<th>Place</th>
						<td>Cham</td>
					</tr>
					<tr>
					<th>Publisher</th>
						<td>Springer International Publishing</td>
					</tr>
					<tr>
					<th>ISBN</th>
						<td>978-3-030-33231-0 978-3-030-33232-7</td>
					</tr>
					<tr>
					<th>Pages</th>
						<td>41-54</td>
					</tr>
					<tr>
					<th>Book Title</th>
						<td>Advances in Quantitative Ethnography</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>4/28/2024, 8:16:48 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>4/28/2024, 8:16:48 PM</td>
					</tr>
				</tbody></table>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_5JMCWB8X">Cai et al. - 2019 - nCoder+ A Semantic Tool for Improving Recall of n.pdf					</li>
				</ul>
			</li>


			<li id="item_QCV5H4VG" class="item journalArticle">
			<h2>Nested Analysis as a Mixed-Method Strategy for Comparative Research: The American Political Science Review</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Evan S. Lieberman</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Despite repeated calls for the use of ''mixed methods" in 
comparative analysis, political scientists have few systematic guides 
for carrying out such work. This paper details a unified approach which 
joins intensive case-study analysis with statistical analysis. Not only 
are the advantages of each approach combined, but also there is a 
synergistic value to the nested research design: for example, 
statistical analyses can guide case selection for in-depth research, 
provide direction for more focused case studies and comparisons, and be 
used to provide additional tests of hypotheses generated from small-N 
research. Small-N analyses can be used to assess the plausibility of 
observed statistical relationships between variables, to generate 
theoretical insights from outlier and other cases, and to develop better
 measurement strategies. This integrated strategy improves the prospects
 of making valid causal inferences in cross-national and other forms of 
comparative research by drawing on the distinct strengths of two 
important approaches.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2005-08-01</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>Nested Analysis as a Mixed-Method Strategy for Comparative Research</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>EBSCOhost</td>
					</tr>
					<tr>
					<th>Volume</th>
						<td>99</td>
					</tr>
					<tr>
					<th>Pages</th>
						<td>435-452</td>
					</tr>
					<tr>
					<th>Publication</th>
						<td>The American Political Science Review</td>
					</tr>
					<tr>
					<th>Issue</th>
						<td>3</td>
					</tr>
					<tr>
					<th>ISSN</th>
						<td>00030554</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>3/14/2024, 7:32:50 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>3/14/2024, 7:32:50 PM</td>
					</tr>
				</tbody></table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Analytics</li>
					<li>Comparative politics</li>
					<li>Datasets</li>
					<li>Inference</li>
					<li>Modeling</li>
					<li>Qualitative comparative analysis</li>
					<li>Regression analysis</li>
					<li>Research design</li>
					<li>Research methods</li>
					<li>Statistical analysis</li>
				</ul>
				<h3 class="notes">Notes:</h3>
				<ul class="notes">
					<li id="item_5LB5NVQP">
<div><div data-citation-items="%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F3957624%2Fitems%2FQCV5H4VG%22%5D%2C%22itemData%22%3A%7B%22id%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F3957624%2Fitems%2FQCV5H4VG%22%2C%22type%22%3A%22article-journal%22%2C%22abstract%22%3A%22Despite%20repeated%20calls%20for%20the%20use%20of%20''mixed%20methods%5C%22%20in%20comparative%20analysis%2C%20political%20scientists%20have%20few%20systematic%20guides%20for%20carrying%20out%20such%20work.%20This%20paper%20details%20a%20unified%20approach%20which%20joins%20intensive%20case-study%20analysis%20with%20statistical%20analysis.%20Not%20only%20are%20the%20advantages%20of%20each%20approach%20combined%2C%20but%20also%20there%20is%20a%20synergistic%20value%20to%20the%20nested%20research%20design%3A%20for%20example%2C%20statistical%20analyses%20can%20guide%20case%20selection%20for%20in-depth%20research%2C%20provide%20direction%20for%20more%20focused%20case%20studies%20and%20comparisons%2C%20and%20be%20used%20to%20provide%20additional%20tests%20of%20hypotheses%20generated%20from%20small-N%20research.%20Small-N%20analyses%20can%20be%20used%20to%20assess%20the%20plausibility%20of%20observed%20statistical%20relationships%20between%20variables%2C%20to%20generate%20theoretical%20insights%20from%20outlier%20and%20other%20cases%2C%20and%20to%20develop%20better%20measurement%20strategies.%20This%20integrated%20strategy%20improves%20the%20prospects%20of%20making%20valid%20causal%20inferences%20in%20cross-national%20and%20other%20forms%20of%20comparative%20research%20by%20drawing%20on%20the%20distinct%20strengths%20of%20two%20important%20approaches.%22%2C%22container-title%22%3A%22The%20American%20Political%20Science%20Review%22%2C%22ISSN%22%3A%2200030554%22%2C%22issue%22%3A%223%22%2C%22page%22%3A%22435-452%22%2C%22source%22%3A%22EBSCOhost%22%2C%22title%22%3A%22Nested%20Analysis%20as%20a%20Mixed-Method%20Strategy%20for%20Comparative%20Research%3A%20The%20American%20Political%20Science%20Review%22%2C%22title-short%22%3A%22Nested%20Analysis%20as%20a%20Mixed-Method%20Strategy%20for%20Comparative%20Research%22%2C%22volume%22%3A%2299%22%2C%22author%22%3A%5B%7B%22family%22%3A%22Lieberman%22%2C%22given%22%3A%22Evan%20S.%22%7D%5D%2C%22issued%22%3A%7B%22date-parts%22%3A%5B%5B%222005%22%2C8%2C1%5D%5D%7D%7D%7D%5D" data-schema-version="8"><p>Note</p>
<p></p>
<p><span class="highlight" data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F3957624%2Fitems%2FC6J2FWXM%22%2C%22pageLabel%22%3A%22435%22%2C%22position%22%3A%7B%22pageIndex%22%3A0%2C%22rects%22%3A%5B%5B358.665%2C213.676%2C543.51%2C223.13%5D%2C%5B310.381%2C202.723%2C543.497%2C212.177%5D%2C%5B310.381%2C191.761%2C543.516%2C201.215%5D%2C%5B310.381%2C180.799%2C398.969%2C190.254%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F3957624%2Fitems%2FQCV5H4VG%22%5D%2C%22locator%22%3A%22435%22%7D%7D">“Clearly,
 not all forms of mixed strategies will provide greater insights into 
particular research problems. In fact, some may simply generate more 
confusion than clarit”</span> <span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F3957624%2Fitems%2FQCV5H4VG%22%5D%2C%22locator%22%3A%22435%22%2C%22itemData%22%3A%7B%22id%22%3A103%2C%22type%22%3A%22article-journal%22%2C%22abstract%22%3A%22Despite%20repeated%20calls%20for%20the%20use%20of%20''mixed%20methods%5C%22%20in%20comparative%20analysis%2C%20political%20scientists%20have%20few%20systematic%20guides%20for%20carrying%20out%20such%20work.%20This%20paper%20details%20a%20unified%20approach%20which%20joins%20intensive%20case-study%20analysis%20with%20statistical%20analysis.%20Not%20only%20are%20the%20advantages%20of%20each%20approach%20combined%2C%20but%20also%20there%20is%20a%20synergistic%20value%20to%20the%20nested%20research%20design%3A%20for%20example%2C%20statistical%20analyses%20can%20guide%20case%20selection%20for%20in-depth%20research%2C%20provide%20direction%20for%20more%20focused%20case%20studies%20and%20comparisons%2C%20and%20be%20used%20to%20provide%20additional%20tests%20of%20hypotheses%20generated%20from%20small-N%20research.%20Small-N%20analyses%20can%20be%20used%20to%20assess%20the%20plausibility%20of%20observed%20statistical%20relationships%20between%20variables%2C%20to%20generate%20theoretical%20insights%20from%20outlier%20and%20other%20cases%2C%20and%20to%20develop%20better%20measurement%20strategies.%20This%20integrated%20strategy%20improves%20the%20prospects%20of%20making%20valid%20causal%20inferences%20in%20cross-national%20and%20other%20forms%20of%20comparative%20research%20by%20drawing%20on%20the%20distinct%20strengths%20of%20two%20important%20approaches.%22%2C%22container-title%22%3A%22The%20American%20Political%20Science%20Review%22%2C%22ISSN%22%3A%2200030554%22%2C%22issue%22%3A%223%22%2C%22page%22%3A%22435-452%22%2C%22source%22%3A%22EBSCOhost%22%2C%22title%22%3A%22Nested%20Analysis%20as%20a%20Mixed-Method%20Strategy%20for%20Comparative%20Research%3A%20The%20American%20Political%20Science%20Review%22%2C%22title-short%22%3A%22Nested%20Analysis%20as%20a%20Mixed-Method%20Strategy%20for%20Comparative%20Research%22%2C%22volume%22%3A%2299%22%2C%22author%22%3A%5B%7B%22family%22%3A%22Lieberman%22%2C%22given%22%3A%22Evan%20S.%22%7D%5D%2C%22issued%22%3A%7B%22date-parts%22%3A%5B%5B%222005%22%2C8%2C1%5D%5D%7D%7D%7D%5D%2C%22properties%22%3A%7B%7D%7D">(<span class="citation-item">Lieberman, 2005, p. 435</span>)</span></p>
<p></p>
<p>lieberman: "nested analysis" as a way to jointly use methods (though 
here its statistical and qualitative, not data science). Author 
describes calls for mixed methods in thefield, but few resources on how 
to do this. They suggest this arrangement is synergistic, not merely 
additive.</p>
<p><span class="highlight" data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F3957624%2Fitems%2FC6J2FWXM%22%2C%22pageLabel%22%3A%22436%22%2C%22position%22%3A%7B%22pageIndex%22%3A1%2C%22rects%22%3A%5B%5B318.368%2C290.248%2C533.798%2C299.703%5D%2C%5B300.654%2C279.286%2C533.779%2C288.741%5D%2C%5B300.654%2C268.324%2C533.742%2C277.779%5D%2C%5B300.654%2C257.371%2C533.748%2C266.826%5D%2C%5B300.654%2C246.41%2C367.888%2C255.864%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F3957624%2Fitems%2FQCV5H4VG%22%5D%2C%22locator%22%3A%22436%22%7D%7D"><strong>“Nested
 analysis is resolutely “catholic” in its assumptions and objectives. It
 assumes an interest in both the exploration of general relationships 
and explanations and the specific explanations of individual cases and 
groups of cases.”</strong></span><strong> <span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F3957624%2Fitems%2FQCV5H4VG%22%5D%2C%22locator%22%3A%22436%22%2C%22itemData%22%3A%7B%22id%22%3A103%2C%22type%22%3A%22article-journal%22%2C%22abstract%22%3A%22Despite%20repeated%20calls%20for%20the%20use%20of%20''mixed%20methods%5C%22%20in%20comparative%20analysis%2C%20political%20scientists%20have%20few%20systematic%20guides%20for%20carrying%20out%20such%20work.%20This%20paper%20details%20a%20unified%20approach%20which%20joins%20intensive%20case-study%20analysis%20with%20statistical%20analysis.%20Not%20only%20are%20the%20advantages%20of%20each%20approach%20combined%2C%20but%20also%20there%20is%20a%20synergistic%20value%20to%20the%20nested%20research%20design%3A%20for%20example%2C%20statistical%20analyses%20can%20guide%20case%20selection%20for%20in-depth%20research%2C%20provide%20direction%20for%20more%20focused%20case%20studies%20and%20comparisons%2C%20and%20be%20used%20to%20provide%20additional%20tests%20of%20hypotheses%20generated%20from%20small-N%20research.%20Small-N%20analyses%20can%20be%20used%20to%20assess%20the%20plausibility%20of%20observed%20statistical%20relationships%20between%20variables%2C%20to%20generate%20theoretical%20insights%20from%20outlier%20and%20other%20cases%2C%20and%20to%20develop%20better%20measurement%20strategies.%20This%20integrated%20strategy%20improves%20the%20prospects%20of%20making%20valid%20causal%20inferences%20in%20cross-national%20and%20other%20forms%20of%20comparative%20research%20by%20drawing%20on%20the%20distinct%20strengths%20of%20two%20important%20approaches.%22%2C%22container-title%22%3A%22The%20American%20Political%20Science%20Review%22%2C%22ISSN%22%3A%2200030554%22%2C%22issue%22%3A%223%22%2C%22page%22%3A%22435-452%22%2C%22source%22%3A%22EBSCOhost%22%2C%22title%22%3A%22Nested%20Analysis%20as%20a%20Mixed-Method%20Strategy%20for%20Comparative%20Research%3A%20The%20American%20Political%20Science%20Review%22%2C%22title-short%22%3A%22Nested%20Analysis%20as%20a%20Mixed-Method%20Strategy%20for%20Comparative%20Research%22%2C%22volume%22%3A%2299%22%2C%22author%22%3A%5B%7B%22family%22%3A%22Lieberman%22%2C%22given%22%3A%22Evan%20S.%22%7D%5D%2C%22issued%22%3A%7B%22date-parts%22%3A%5B%5B%222005%22%2C8%2C1%5D%5D%7D%7D%7D%5D%2C%22properties%22%3A%7B%7D%7D">(<span class="citation-item">Lieberman, 2005, p. 436</span>)</span></strong></p>
</div></div>
					</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_TZJ39JQN">EBSCO Record					</li>
					<li id="item_C6J2FWXM">Lieberman - 2005 - Nested Analysis as a Mixed-Method Strategy for Com.pdf					</li>
				</ul>
			</li>


			<li id="item_H47FPBVK" class="item preprint">
			<h2>Neural embedding of beliefs reveals the role of relative dissonance in human decision-making</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Preprint</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Byunghwee Lee</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Rachith Aiyappa</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Yong-Yeol Ahn</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Haewoon Kwak</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Jisun An</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Beliefs serve as the foundation for human cognition and 
decision-making. They guide individuals in deriving meaning from their 
lives, shaping their behaviors, and forming social connections. 
Therefore, a model that encapsulates beliefs and their 
interrelationships is crucial for quantitatively studying the influence 
of beliefs on our actions. Despite its importance, research on the 
interplay between human beliefs has often been limited to a small set of
 beliefs pertaining to specific issues, with a heavy reliance on surveys
 or experiments. Here, we propose a method for extracting nuanced 
relations between thousands of beliefs by leveraging large-scale user 
participation data from an online debate platform and mapping these 
beliefs to an embedding space using a fine-tuned large language model 
(LLM). This belief embedding space effectively encapsulates the 
interconnectedness of diverse beliefs as well as polarization across 
various social issues. We discover that the positions within this belief
 space predict new beliefs of individuals. Furthermore, we find that the
 relative distance between one's existing beliefs and new beliefs can 
serve as a quantitative estimate of cognitive dissonance, allowing us to
 predict new beliefs. Our study highlights how modern LLMs, when 
combined with collective online records of human beliefs, can offer 
insights into the fundamental principles that govern human belief 
formation and decision-making processes.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2024-08-13</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>arXiv.org</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://arxiv.org/abs/2408.07237">http://arxiv.org/abs/2408.07237</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>1/8/2025, 2:53:37 PM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>arXiv:2408.07237 [cs]</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.48550/arXiv.2408.07237">10.48550/arXiv.2408.07237</a></td>
					</tr>
					<tr>
					<th>Repository</th>
						<td>arXiv</td>
					</tr>
					<tr>
					<th>Archive ID</th>
						<td>arXiv:2408.07237</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>1/8/2025, 2:53:37 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Computer Science - Computation and Language</li>
					<li>Computer Science - Computers and Society</li>
					<li>Physics - Physics and Society</li>
				</ul>
				<h3 class="notes">Notes:</h3>
				<ul class="notes">
					<li id="item_4VXTT9BU">
<p class="plaintext">Comment: 26 pages, 6 figures, SI</p>
					</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_WUJZXILR">Preprint PDF					</li>
					<li id="item_BVAIR69T">Snapshot					</li>
				</ul>
			</li>


			<li id="item_XK5TA2ML" class="item journalArticle">
			<h2>Neural Recall Network: A Neural Network Solution to Low Recall Problem in Regex-based Qualitative Coding</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Zhiqiang Cai</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Cody Marquart</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>David Shaffer</td>
					</tr>
					<tr>
						<th class="contributor">Contributor</th>
						<td>Antonija Mitrovic</td>
					</tr>
					<tr>
						<th class="contributor">Contributor</th>
						<td>Nigel Bosch</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Regular expression (regex) coding has advantages for text 
analysis. Humans are often able to quickly construct intelligible coding
 rules with high precision. That is, researchers can identify words and 
word patterns that correctly classify examples of a particular concept. 
And, it is often easy to identify false positives and improve the regex 
classifier so that the positive items are accurately captured. However, 
ensuring that a regex list is complete is a bigger challenge, because 
the concepts to be identified in data are often sparsely distributed, 
which makes it difficult to identify examples of false negatives. For 
this reason, regex-based classifiers suffer by having low recall. That 
is, it often misses items that should be classified as positive. In this
 paper, we provide a neural network solution to this problem by 
identifying a negative reversion set, in which false negative items 
occur much more frequently than in the data set as a whole. Thus, the 
regex classifier can be more quickly improved by adding missing regexes 
based on the false negatives found from the negative reversion set. This
 study used an existing data set collected from a simulation-based 
learning environment for which researchers had previously defined six 
codes and developed classifiers with validated regex lists. We randomly 
constructed incomplete (partial) regex lists and used neural network 
models to identify negative reversion sets in which the frequency of 
false negatives increased from a range of 3%-8% in the full data set to a
 range of 12%-52% in the negative reversion set. Based on this finding, 
we propose an interactive coding mechanism in which human-developed 
regex classifiers provide input for training machine learning algorithms
 and machine learning algorithms “smartly” select highly suspected false
 negative items for human to more quickly develop regex classifiers.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2022-07-18</td>
					</tr>
					<tr>
					<th>Language</th>
						<td>en</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>Neural Recall Network</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>DOI.org (Datacite)</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://zenodo.org/record/6853047">https://zenodo.org/record/6853047</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>5/1/2024, 9:38:44 AM</td>
					</tr>
					<tr>
					<th>Rights</th>
						<td>Creative Commons Attribution 4.0 International, Open Access</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>Publisher: [object Object]</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.5281/ZENODO.6853047">10.5281/ZENODO.6853047</a></td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>5/1/2024, 9:38:44 AM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>5/1/2024, 9:38:44 AM</td>
					</tr>
				</tbody></table>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_L9CTTHBW">Zhiqiang Cai et al. - 2022 - Neural Recall Network A Neural Network Solution t.pdf					</li>
				</ul>
			</li>


			<li id="item_YY6RVVHC" class="item journalArticle">
			<h2>Novel embeddings improve the prediction of risk perception</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Zak Hussain</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Rui Mata</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Dirk U. Wulff</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>We assess whether the classic psychometric paradigm of risk 
perception can be improved or supplanted by novel approaches relying on 
language embeddings. To this end, we introduce the Basel Risk Norms, a 
large data set covering 1004 distinct sources of risk (e.g., 
vaccination, nuclear energy, artificial intelligence) and compare the 
psychometric paradigm against novel text and free-association embeddings
 in predicting risk perception. We find that an ensemble model combining
 text and free association rivals the predictive accuracy of the 
psychometric paradigm, captures additional affect and frequency-related 
dimensions of risk perception not accounted for by the classic approach,
 and has greater range of applicability to real-world text data, such as
 news headlines. Overall, our results establish the ensemble of text and
 free-association embeddings as a promising new tool for researchers and
 policymakers to track real-world risk perception.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2024/12</td>
					</tr>
					<tr>
					<th>Language</th>
						<td>en</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>epjdatascience.springeropen.com</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://epjdatascience.springeropen.com/articles/10.1140/epjds/s13688-024-00478-x">https://epjdatascience.springeropen.com/articles/10.1140/epjds/s13688-024-00478-x</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>1/3/2025, 12:44:40 PM</td>
					</tr>
					<tr>
					<th>Rights</th>
						<td>2024 The Author(s)</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>Number: 1
Publisher: SpringerOpen</td>
					</tr>
					<tr>
					<th>Volume</th>
						<td>13</td>
					</tr>
					<tr>
					<th>Pages</th>
						<td>1-17</td>
					</tr>
					<tr>
					<th>Publication</th>
						<td>EPJ Data Science</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.1140/epjds/s13688-024-00478-x">10.1140/epjds/s13688-024-00478-x</a></td>
					</tr>
					<tr>
					<th>Issue</th>
						<td>1</td>
					</tr>
					<tr>
					<th>Journal Abbr</th>
						<td>EPJ Data Sci.</td>
					</tr>
					<tr>
					<th>ISSN</th>
						<td>2193-1127</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>1/3/2025, 12:44:40 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_AFBHZXB5">Full Text PDF					</li>
				</ul>
			</li>


			<li id="item_D6H7VF6X" class="item preprint">
			<h2>On Generative Artificial Intelligence: Open-Source is the Way</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Preprint</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Juhani Merilehto</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>The rapid advancement of generative artificial intelligence 
(AI) presents both immense opportunities and challenges for society. 
This paper argues that the development of open-source generative AI is 
crucial for promoting transparency, accountability, and inclusion in the
 creation and deployment of these powerful technologies. By making 
models and datasets accessible to a wide range of stakeholders, 
open-source generative AI can foster innovation, collaboration, and 
knowledge-sharing while enabling the identification and mitigation of 
potential risks and harms. To realize the full potential of open-source 
generative AI, a concerted effort from the AI community, policymakers, 
and society at large is necessary. This paper outlines a vision for the 
future of open-source generative AI and provides specific 
recommendations for researchers, industry practitioners, and 
policymakers to promote responsible development practices, establish 
institutional frameworks, and engage the public in decision-making 
processes. The AI community has a unique opportunity and responsibility 
to shape the future of generative AI in a way that promotes the public 
good, and embracing open-source development and prioritizing responsible
 AI practices are key steps in this direction. The time to act is now. 
Collaboration is essential to build a future where generative AI 
empowers individuals, advances societal well-being, and upholds the 
values of transparency, accountability, and inclusivity.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2024-03-15</td>
					</tr>
					<tr>
					<th>Language</th>
						<td>en-us</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>On Generative Artificial Intelligence</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>OSF Preprints</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://osf.io/jnmzg">https://osf.io/jnmzg</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>1/6/2025, 11:10:25 AM</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.31235/osf.io/jnmzg">10.31235/osf.io/jnmzg</a></td>
					</tr>
					<tr>
					<th>Repository</th>
						<td>OSF</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>1/6/2025, 11:10:25 AM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Generative Artificial Intelligence</li>
					<li>Inclusion</li>
					<li>Large Language Model</li>
					<li>Open-Source</li>
					<li>Society</li>
					<li>Transformer</li>
					<li>Transparency</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_ZJVRDZFB">OSF Preprint					</li>
				</ul>
			</li>


			<li id="item_4JDBBBFS" class="item journalArticle">
			<h2>On Measurement Validity and Language Models: Increasing Validity and Decreasing Bias with Instructions</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Moritz Laurer</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Wouter van Atteveldt</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Andreu Casas</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Kasper Welbers</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Language models like BERT or GPT are becoming increasingly 
popular measurement tools, but are the measurements they produce valid? 
Literature suggests that there is still a relevant gap between the 
ambitions of computational text analysis methods and the validity of 
their outputs. One prominent threat to validity is hidden biases in the 
training data, where models learn group-specific language patterns 
instead of the concept researchers want to measure. This paper 
investigates to what extent these biases impact the validity of 
measurements created with language models. We conduct a comparative 
analysis across nine group types in four datasets with three types of 
classification models, focusing on the robustness of models against 
biases and on the validity of their outputs. While we find that all 
types of models learn biases, the effects on validity are surprisingly 
small. In particular when models receive instructions as an additional 
input, they become more robust against biases from the fine-tuning data 
and produce more valid measurements across different groups. An 
instruction-based model (BERT-NLI) sees its average test-set performance
 decrease by only 0.4% F1 macro when trained on biased data and its 
error probability on groups it has not seen during training increases 
only by 0.8%.</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>On Measurement Validity and Language Models</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>Taylor and Francis+NEJM</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://doi.org/10.1080/19312458.2024.2378690">https://doi.org/10.1080/19312458.2024.2378690</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>1/6/2025, 1:55:35 PM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>Publisher: Routledge
_eprint: https://doi.org/10.1080/19312458.2024.2378690</td>
					</tr>
					<tr>
					<th>Volume</th>
						<td>0</td>
					</tr>
					<tr>
					<th>Pages</th>
						<td>1-17</td>
					</tr>
					<tr>
					<th>Publication</th>
						<td>Communication Methods and Measures</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.1080/19312458.2024.2378690">10.1080/19312458.2024.2378690</a></td>
					</tr>
					<tr>
					<th>Issue</th>
						<td>0</td>
					</tr>
					<tr>
					<th>ISSN</th>
						<td>1931-2458</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>1/6/2025, 1:55:35 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_K4RRWH24">Full Text PDF					</li>
				</ul>
			</li>


			<li id="item_7BKSBDNL" class="item conferencePaper">
			<h2>On the Equivalence of Inductive Content Analysis and Topic Modeling</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Conference Paper</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Aneesha Bakharia</td>
					</tr>
					<tr>
						<th class="editor">Editor</th>
						<td>Brendan Eagan</td>
					</tr>
					<tr>
						<th class="editor">Editor</th>
						<td>Morten Misfeldt</td>
					</tr>
					<tr>
						<th class="editor">Editor</th>
						<td>Amanda Siebert-Evenstone</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Inductive content analysis is a research task in which a 
researcher manually reads text and identifies categories or themes that 
emerge from a document corpus. Inductive content analysis is usually 
performed as part of a formal qualitative research methodology such as 
Grounded Theory. Topic modeling algorithms discover the latent topics in
 a document corpus. There has been a general assumption, that topic 
modeling is a suitable algorithmic aid for inductive content analysis. 
In this short paper, the findings from a between-subjects experiment to 
evaluate the differences between topics identified by manual coders and 
topic modeling algorithms is discussed. The findings show that the topic
 modeling algorithm was only comparable to the human coders for broad 
topics and that topic modeling algorithms would require additional 
domain knowledge in order to identify more fine-grained topics. The 
paper also reports issues that impede the use of topic modeling within 
the quantitative ethnography process such as topic interpretation and 
topic size quantification.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2019</td>
					</tr>
					<tr>
					<th>Language</th>
						<td>en</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>Springer Link</td>
					</tr>
					<tr>
					<th>Place</th>
						<td>Cham</td>
					</tr>
					<tr>
					<th>Publisher</th>
						<td>Springer International Publishing</td>
					</tr>
					<tr>
					<th>ISBN</th>
						<td>978-3-030-33232-7</td>
					</tr>
					<tr>
					<th>Pages</th>
						<td>291-298</td>
					</tr>
					<tr>
					<th>Proceedings Title</th>
						<td>Advances in Quantitative Ethnography</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.1007/978-3-030-33232-7_25">10.1007/978-3-030-33232-7_25</a></td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>5/1/2024, 9:50:19 AM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>5/1/2024, 9:50:19 AM</td>
					</tr>
				</tbody></table>
			</li>


			<li id="item_8GZPAY3C" class="item preprint">
			<h2>One fish, two fish, but not the whole sea: Alignment reduces language models' conceptual diversity</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Preprint</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Sonia K. Murthy</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Tomer Ullman</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Jennifer Hu</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Researchers in social science and psychology have recently 
proposed using large language models (LLMs) as replacements for humans 
in behavioral research. In addition to arguments about whether LLMs 
accurately capture population-level patterns, this has raised questions 
about whether LLMs capture human-like conceptual diversity. Separately, 
it is debated whether post-training alignment (RLHF or RLAIF) affects 
models' internal diversity. Inspired by human studies, we use a new way 
of measuring the conceptual diversity of synthetically-generated LLM 
"populations" by relating the internal variability of simulated 
individuals to the population-level variability. We use this approach to
 evaluate non-aligned and aligned LLMs on two domains with rich human 
behavioral data. While no model reaches human-like diversity, aligned 
models generally display less diversity than their instruction 
fine-tuned counterparts. Our findings highlight potential trade-offs 
between increasing models' value alignment and decreasing the diversity 
of their conceptual representations.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2024-11-12</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>One fish, two fish, but not the whole sea</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>arXiv.org</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://arxiv.org/abs/2411.04427">http://arxiv.org/abs/2411.04427</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>1/7/2025, 5:02:09 PM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>arXiv:2411.04427 [cs]</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.48550/arXiv.2411.04427">10.48550/arXiv.2411.04427</a></td>
					</tr>
					<tr>
					<th>Repository</th>
						<td>arXiv</td>
					</tr>
					<tr>
					<th>Archive ID</th>
						<td>arXiv:2411.04427</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>1/7/2025, 5:02:09 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Computer Science - Computation and Language</li>
				</ul>
				<h3 class="notes">Notes:</h3>
				<ul class="notes">
					<li id="item_VD9C3SDY">
<p class="plaintext">Comment: 17 pages, 10 figures; corrected figure version</p>
					</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_WMLUNV4X">Preprint PDF					</li>
					<li id="item_TKQ9KJG4">Snapshot					</li>
				</ul>
			</li>


			<li id="item_YALAFJNB" class="item journalArticle">
			<h2>Online advertisement in a pink-colored market</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Amir Mehrjoo</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Rubén Cuevas</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Ángel Cuevas</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>It is surprising that women are often charged more for 
products and services marketed explicitly to them. This phenomenon, 
known as the pink tax, is a major issue that questions women’s buying 
power. Nevertheless, it is not just limited to physical products – even 
online advertising can be subject to this type of gender-price 
discrimination. That is where our research comes in. We have developed a
 new methodology to measure what we call the digital marketing pink tax –
 the additional expense of delivering advertisements to female 
audiences. Analyzing data from Facebook advertising platforms across 187
 countries and 40 territories shows this issue is systematic. 
Particularly, the digital marketing pink tax is prevalent in 79% of 
audiences across the world and 98% of audiences in highly developed 
countries. Therefore, advertisers incur a median cost of 30% more to 
display advertisements to women than men. In contrast, advertisers have 
to pay less digital marketing pink tax in less-developed countries (5%).
 Our research indicates that countries in the Middle East and Africa 
with a low Human Development Index (HDI) do not experience this 
phenomenon. Our comprehensive investigation of 24 industries reveals 
that advertisers must pay up to 64% of the digital marketing pink tax to
 target women in some industries. Our findings also suggest a connection
 between the digital marketing pink tax and the consumer pink tax – the 
extra charge placed on products marketed to women. Overall, our research
 sheds light on an important issue affecting women worldwide. Raising 
awareness of the digital marketing pink tax and advocating for better 
regulation.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2024/12</td>
					</tr>
					<tr>
					<th>Language</th>
						<td>en</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>epjdatascience.springeropen.com</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://epjdatascience.springeropen.com/articles/10.1140/epjds/s13688-024-00473-2">https://epjdatascience.springeropen.com/articles/10.1140/epjds/s13688-024-00473-2</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>1/3/2025, 12:43:38 PM</td>
					</tr>
					<tr>
					<th>Rights</th>
						<td>2024 The Author(s)</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>Number: 1
Publisher: SpringerOpen</td>
					</tr>
					<tr>
					<th>Volume</th>
						<td>13</td>
					</tr>
					<tr>
					<th>Pages</th>
						<td>1-26</td>
					</tr>
					<tr>
					<th>Publication</th>
						<td>EPJ Data Science</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.1140/epjds/s13688-024-00473-2">10.1140/epjds/s13688-024-00473-2</a></td>
					</tr>
					<tr>
					<th>Issue</th>
						<td>1</td>
					</tr>
					<tr>
					<th>Journal Abbr</th>
						<td>EPJ Data Sci.</td>
					</tr>
					<tr>
					<th>ISSN</th>
						<td>2193-1127</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>1/3/2025, 12:43:38 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_5UWUZG4U">Full Text PDF					</li>
				</ul>
			</li>


			<li id="item_VTVEX22E" class="item journalArticle">
			<h2>Open-source LLMs for text annotation: a practical guide for model setting and fine-tuning</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Meysam Alizadeh</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Maël Kubli</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Zeynab Samei</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Shirin Dehghani</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Mohammadmasiha Zahedivafa</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Juan D. Bermeo</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Maria Korobeynikova</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Fabrizio Gilardi</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>This paper studies the performance of open-source Large 
Language Models (LLMs) in text classification tasks typical for 
political science research. By examining tasks like stance, topic, and 
relevance classification, we aim to guide scholars in making informed 
decisions about their use of LLMs for text analysis and to establish a 
baseline performance benchmark that demonstrates the models’ 
effectiveness. Specifically, we conduct an assessment of both zero-shot 
and fine-tuned LLMs across a range of text annotation tasks using news 
articles and tweets datasets. Our analysis shows that fine-tuning 
improves the performance of open-source LLMs, allowing them to match or 
even surpass zero-shot GPT$$-$$3.5 and GPT-4, though still lagging 
behind fine-tuned GPT$$-$$3.5. We further establish that fine-tuning is 
preferable to few-shot training with a relatively modest quantity of 
annotated text. Our findings show that fine-tuned open-source LLMs can 
be effectively deployed in a broad spectrum of text annotation 
applications. We provide a Python notebook facilitating the application 
of LLMs in text annotation for other researchers.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2024-12-18</td>
					</tr>
					<tr>
					<th>Language</th>
						<td>en</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>Open-source LLMs for text annotation</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>Springer Link</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://doi.org/10.1007/s42001-024-00345-9">https://doi.org/10.1007/s42001-024-00345-9</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>1/3/2025, 2:00:43 PM</td>
					</tr>
					<tr>
					<th>Volume</th>
						<td>8</td>
					</tr>
					<tr>
					<th>Pages</th>
						<td>17</td>
					</tr>
					<tr>
					<th>Publication</th>
						<td>Journal of Computational Social Science</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.1007/s42001-024-00345-9">10.1007/s42001-024-00345-9</a></td>
					</tr>
					<tr>
					<th>Issue</th>
						<td>1</td>
					</tr>
					<tr>
					<th>Journal Abbr</th>
						<td>J Comput Soc Sc</td>
					</tr>
					<tr>
					<th>ISSN</th>
						<td>2432-2725</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>1/3/2025, 2:00:43 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>ChatGPT</li>
					<li>FLAN</li>
					<li>LLaMA</li>
					<li>LLMs</li>
					<li>NLP</li>
					<li>Open source</li>
					<li>Text annotation</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_PM9IS7Q7">Full Text PDF					</li>
				</ul>
			</li>


			<li id="item_NGS8YAIB" class="item conferencePaper">
			<h2>Optimizing the role of human evaluation in LLM-based spoken document summarization systems</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Conference Paper</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Margaret Kroll</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Kelsey Kraus</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>The emergence of powerful LLMs has led to a paradigm shift in 
abstractive summarization of spoken documents. The properties that make 
LLMs so valuable for this task -- creativity, ability to produce fluent 
speech, and ability to abstract information from large corpora -- also 
present new challenges to evaluating their content. Quick, 
cost-effective automatic evaluations such as ROUGE and BERTScore offer 
promise, but do not yet show competitive performance when compared to 
human evaluations. We draw on methodologies from the social sciences to 
propose an evaluation paradigm for spoken document summarization 
explicitly tailored for generative AI content. We provide detailed 
evaluation criteria and best practices guidelines to ensure robustness 
in the experimental design, replicability, and trustworthiness of human 
evaluation studies. We additionally include two case studies that show 
how these human-in-the-loop evaluation methods have been implemented at a
 major U.S. technology company.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2024-9-1</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>arXiv.org</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://arxiv.org/abs/2410.18218">http://arxiv.org/abs/2410.18218</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>1/7/2025, 5:05:36 PM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>arXiv:2410.18218 [cs]</td>
					</tr>
					<tr>
					<th>Pages</th>
						<td>1935-1939</td>
					</tr>
					<tr>
					<th>Proceedings Title</th>
						<td>Interspeech 2024</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.21437/Interspeech.2024-2268">10.21437/Interspeech.2024-2268</a></td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>1/7/2025, 5:05:36 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Computer Science - Artificial Intelligence</li>
					<li>Computer Science - Computation and Language</li>
					<li>Computer Science - Sound</li>
					<li>Electrical Engineering and Systems Science - Audio and Speech Processing</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_TM8WWSQG">Preprint PDF					</li>
					<li id="item_L2PSVWT7">Snapshot					</li>
				</ul>
			</li>


			<li id="item_DVMURWPM" class="item journalArticle">
			<h2>Outlier bias: AI classification of curb ramps, outliers, and context</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Shiloh Deitz</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Technologies in the smart city, such as autonomous vehicles 
and delivery robots, promise to increase the mobility and freedom of 
people with disabilities. These technologies have also failed to “see” 
or comprehend wheelchair riders, people walking with service animals, 
and people walking with bicycles—all outliers to machine learning 
models. Big data and algorithms have been amply critiqued for their 
biases—harmful and systematic errors—but the harms that arise from AI's 
inherent inability to handle nuance, context, and exception have been 
largely overlooked. In this paper, I run two machine learning models 
across nine cities in the United States to attempt to fill a gap in data
 about the location of curb ramps. I find that while curb ramp 
prediction models may achieve up to 88% accuracy, the rate of accuracy 
varied in context in ways both predictable and unpredictable. I look 
closely at cases of unpredictable error (outlier bias), by triangulating
 with aerial and street view imagery. The sampling of cases shows that 
while it may be possible to conjecture about patterns in these errors, 
there is nothing clearly systematic. While more data and bigger models 
might improve the accuracy somewhat, I propose that a bias toward 
outliers is something fundamental to machine learning models which 
gravitate to the mean and require unbiased and not missing data. I 
conclude by arguing that universal design or design for the outliers is 
imperative for justice in the smart city where algorithms and data are 
increasingly embedded as infrastructure.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2023-07-01</td>
					</tr>
					<tr>
					<th>Language</th>
						<td>en</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>Outlier bias</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>SAGE Journals</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://doi.org/10.1177/20539517231203669">https://doi.org/10.1177/20539517231203669</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>12/23/2024, 5:34:17 PM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>Publisher: SAGE Publications Ltd</td>
					</tr>
					<tr>
					<th>Volume</th>
						<td>10</td>
					</tr>
					<tr>
					<th>Pages</th>
						<td>20539517231203669</td>
					</tr>
					<tr>
					<th>Publication</th>
						<td>Big Data &amp; Society</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.1177/20539517231203669">10.1177/20539517231203669</a></td>
					</tr>
					<tr>
					<th>Issue</th>
						<td>2</td>
					</tr>
					<tr>
					<th>ISSN</th>
						<td>2053-9517</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>12/23/2024, 5:34:17 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_NTNSMDLU">SAGE PDF Full Text					</li>
				</ul>
			</li>


			<li id="item_EIVIGBX9" class="item journalArticle">
			<h2>Performing an Inductive Thematic Analysis of Semi-Structured 
Interviews With a Large Language Model: An Exploration and Provocation 
on the Limits of the Approach</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Stefano De Paoli</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Large Language Models (LLMs) have emerged as powerful 
generative Artificial Intelligence solutions. This paper presents 
results and reflections of an experiment done with the LLM GPT 3.5-Turbo
 to perform an inductive Thematic Analysis (TA). Previous research has 
worked on conducting deductive analysis. Thematic Analysis is a 
qualitative method for analysis commonly used in social sciences and it 
is based on interpretations by the human analyst(s) and the 
identification of explicit and latent meanings in qualitative data. The 
paper presents the motivations for attempting this analysis; it reflects
 on how the six phases to a TA proposed by Braun and Clarke can 
partially be reproduced with the LLM and it reflects on what are the 
model’s outputs. The paper uses two datasets of open access 
semi-structured interviews, previously analysed by other researchers. 
The first dataset contains interviews with videogame players, and the 
second is a dataset of interviews with lecturers teaching data science 
in a University. This paper used the analyses previously conducted on 
these datasets to compare with the results produced by the LLM. The 
results show that the model can infer most of the main themes from 
previous research. This shows that using LLMs to perform an inductive TA
 is viable and offers a good degree of validity. The discussion offers 
some recommendations for working with LLMs in qualitative analysis.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2024-08-01</td>
					</tr>
					<tr>
					<th>Language</th>
						<td>en</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>Performing an Inductive Thematic Analysis of Semi-Structured Interviews With a Large Language Model</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>SAGE Journals</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://doi.org/10.1177/08944393231220483">https://doi.org/10.1177/08944393231220483</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>12/24/2024, 5:38:27 PM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>Publisher: SAGE Publications Inc</td>
					</tr>
					<tr>
					<th>Volume</th>
						<td>42</td>
					</tr>
					<tr>
					<th>Pages</th>
						<td>997-1019</td>
					</tr>
					<tr>
					<th>Publication</th>
						<td>Social Science Computer Review</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.1177/08944393231220483">10.1177/08944393231220483</a></td>
					</tr>
					<tr>
					<th>Issue</th>
						<td>4</td>
					</tr>
					<tr>
					<th>ISSN</th>
						<td>0894-4393</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>12/24/2024, 5:38:27 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_J3XD9K2C">SAGE PDF Full Text					</li>
				</ul>
			</li>


			<li id="item_VGHMK3S3" class="item webpage">
			<h2>Political-LLM: Large Language Models in Political Science</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Web Page</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>In recent years, large language models (LLMs) have been widely adopted in political
science tasks such as election prediction, sentiment analysis, policy impact assessment, and
misinformation detection. Meanwhile, the need to systematically understand how LLMs can
further revolutionize the field also becomes urgent. In this work, we—a multidisciplinary team
of researchers spanning computer science and political science—present the first principled
framework termed Political-LLM to advance the comprehensive understanding of integrating
LLMs into computational political science. Specifically, we first introduce a fundamental
taxonomy classifying the existing explorations into two perspectives: political science and
computational methodologies. In particular, from the political science perspective, we
highlight the role of LLMs in automating predictive and generative tasks, simulating behavior
dynamics, and improving causal inference through tools like counterfactual generation; from a
computational perspective, we introduce advancements in data preparation, fine-tuning, and
evaluation methods for LLMs that are tailored to political contexts. We identify key challenges
and future directions, emphasizing the development of domain-specific datasets, addressing
issues of bias and fairness, incorporating human expertise, and redefining evaluation criteria to
align with the unique requirements of computational political science. Political-LLM seeks to
serve as a guidebook for researchers to foster an informed, ethical, and impactful use of Artificial
Intelligence in political science.</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://arxiv.org/pdf/2412.06864">https://arxiv.org/pdf/2412.06864</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>1/8/2025, 1:47:44 PM</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>1/8/2025, 1:47:44 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 8:09:49 PM</td>
					</tr>
				</tbody></table>
			</li>


			<li id="item_G88CNW5B" class="item journalArticle">
			<h2>Politics as Usual? Measuring Populism, Nationalism, and 
Authoritarianism in U.S. Presidential Campaigns (1952–2020) with Neural 
Language Models</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Bart Bonikowski</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Yuchen Luo</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Oscar Stuhler</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Radical-right campaigns commonly employ three discursive 
elements: anti-elite populism, exclusionary and declinist nationalism, 
and authoritarianism. Recent scholarship has explored whether these 
frames have diffused from radical-right to centrist parties in the 
latter’s effort to compete for the former’s voters. This study instead 
investigates whether similar frames had been used by mainstream 
political actors prior to their exploitation by the radical right (in 
the U.S., Donald Trump’s 2016 and 2020 campaigns). To do so, we identify
 instances of populism, nationalism (i.e., exclusionary and inclusive 
definitions of national symbolic boundaries and displays of low and high
 national pride), and authoritarianism in the speeches of Democratic and
 Republican presidential nominees between 1952 and 2020. These frames 
are subtle, infrequent, and polysemic, which makes their measurement 
difficult. We overcome this by leveraging the affordances of neural 
language models—in particular, a robustly optimized variant of 
bidirectional encoder representations from Transformers (RoBERTa) and 
active learning. As we demonstrate, this approach is more effective for 
measuring discursive frames than other methods commonly used by social 
scientists. Our results suggest that what set Donald Trump’s campaign 
apart from those of mainstream presidential candidates was not the 
invention of a new form of politics, but the combination of negative 
evaluations of elites, low national pride, and authoritarianism—all of 
which had long been present among both parties—with an explicit 
evocation of exclusionary nationalism, which had been articulated only 
implicitly by prior presidential nominees. Radical-right discourse—at 
least at the presidential level in the United States—should therefore be
 characterized not as a break with the past but as an amplification and 
creative rearrangement of existing political-cultural tropes.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2022-11-01</td>
					</tr>
					<tr>
					<th>Language</th>
						<td>en</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>Politics as Usual?</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>SAGE Journals</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://doi.org/10.1177/00491241221122317">https://doi.org/10.1177/00491241221122317</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>1/6/2025, 1:14:10 PM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>Publisher: SAGE Publications Inc</td>
					</tr>
					<tr>
					<th>Volume</th>
						<td>51</td>
					</tr>
					<tr>
					<th>Pages</th>
						<td>1721-1787</td>
					</tr>
					<tr>
					<th>Publication</th>
						<td>Sociological Methods &amp; Research</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.1177/00491241221122317">10.1177/00491241221122317</a></td>
					</tr>
					<tr>
					<th>Issue</th>
						<td>4</td>
					</tr>
					<tr>
					<th>ISSN</th>
						<td>0049-1241</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>1/6/2025, 1:14:10 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_VBBQL97U">Submitted Version					</li>
				</ul>
			</li>


			<li id="item_EBHNB9KY" class="item preprint">
			<h2>Predicting Twitter User Socioeconomic Attributes with Network and Language Information</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Preprint</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Nikolaos Aletras</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Benjamin Paul Chamberlain</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Inferring socioeconomic attributes of social media users such 
as occupation and income is an important problem in computational social
 science. Automated inference of such characteristics has applications 
in personalised recommender systems, targeted computational advertising 
and online political campaigning. While previous work has shown that 
language features can reliably predict socioeconomic attributes on 
Twitter, employing information coming from users' social networks has 
not yet been explored for such complex user characteristics. In this 
paper, we describe a method for predicting the occupational class and 
the income of Twitter users given information extracted from their 
extended networks by learning a low-dimensional vector representation of
 users, i.e. graph embeddings. We use this representation to train 
predictive models for occupational class and income. Results on two 
publicly available datasets show that our method consistently 
outperforms the state-of-the-art methods in both tasks. We also obtain 
further significant improvements when we combine graph embeddings with 
textual features, demonstrating that social network and language 
information are complementary.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2018-04-11</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>arXiv.org</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://arxiv.org/abs/1804.04095">http://arxiv.org/abs/1804.04095</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>1/8/2025, 12:56:17 PM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>arXiv:1804.04095 [cs]</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.48550/arXiv.1804.04095">10.48550/arXiv.1804.04095</a></td>
					</tr>
					<tr>
					<th>Repository</th>
						<td>arXiv</td>
					</tr>
					<tr>
					<th>Archive ID</th>
						<td>arXiv:1804.04095</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>1/8/2025, 12:56:17 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Computer Science - Artificial Intelligence</li>
					<li>Computer Science - Computation and Language</li>
					<li>Computer Science - Social and Information Networks</li>
				</ul>
				<h3 class="notes">Notes:</h3>
				<ul class="notes">
					<li id="item_BQUX6YXN">
<p class="plaintext">Comment: Accepted at ACM HT 2018</p>
					</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_RZI72CQU">Preprint PDF					</li>
					<li id="item_Y7M33JXW">Snapshot					</li>
				</ul>
			</li>


			<li id="item_RLVRI22Z" class="item journalArticle">
			<h2>Prediction and explainability in AI: Striking a new balance?</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Aviad Raz</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Bert Heinrichs</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Netta Avnoon</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Gil Eyal</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Yael Inbar</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>The debate regarding prediction and explainability in 
artificial intelligence (AI) centers around the trade-off between 
achieving high-performance accurate models and the ability to understand
 and interpret the decisionmaking process of those models. In recent 
years, this debate has gained significant attention due to the 
increasing adoption of AI systems in various domains, including 
healthcare, finance, and criminal justice. While prediction and 
explainability are desirable goals in principle, the recent spread of 
high accuracy yet opaque machine learning (ML) algorithms has 
highlighted the trade-off between the two, marking this debate as an 
inter-disciplinary, inter-professional arena for negotiating expertise. 
There is no longer an agreement about what should be the “default” 
balance of prediction and explainability, with various positions 
reflecting claims for professional jurisdiction. Overall, there appears 
to be a growing schism between the regulatory and ethics-based call for 
explainability as a condition for trustworthy AI, and how it is being 
designed, assimilated, and negotiated. The impetus for writing this 
commentary comes from recent suggestions that explainability is 
overrated, including the argument that explainability is not guaranteed 
in human healthcare experts either. To shed light on this debate, its 
premises, and its recent twists, we provide an overview of key arguments
 representing different frames, focusing on AI in healthcare.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2024-03-01</td>
					</tr>
					<tr>
					<th>Language</th>
						<td>en</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>Prediction and explainability in AI</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>SAGE Journals</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://doi.org/10.1177/20539517241235871">https://doi.org/10.1177/20539517241235871</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>12/23/2024, 5:29:07 PM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>Publisher: SAGE Publications Ltd</td>
					</tr>
					<tr>
					<th>Volume</th>
						<td>11</td>
					</tr>
					<tr>
					<th>Pages</th>
						<td>20539517241235871</td>
					</tr>
					<tr>
					<th>Publication</th>
						<td>Big Data &amp; Society</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.1177/20539517241235871">10.1177/20539517241235871</a></td>
					</tr>
					<tr>
					<th>Issue</th>
						<td>1</td>
					</tr>
					<tr>
					<th>ISSN</th>
						<td>2053-9517</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>12/23/2024, 5:29:07 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_C7VFPPS8">SAGE PDF Full Text					</li>
				</ul>
			</li>


			<li id="item_6LBLX8C6" class="item journalArticle">
			<h2>Prefix tuning with prompt augmentation for efficient financial news summarization</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Shangyang Mou</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Qiang Xue</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Xunquan Chen</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Jinhui Chen</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Ryoichi Takashima</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Tetsuya Takiguchi</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Yasuo Ariki</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>In financial markets, the sentiment expressed in news articles
 plays a pivotal role in interpreting and forecasting market trends, 
which also holds true for the task of financial news summarization 
(FNS). Leveraging AI models to analyze social science data, this paper 
employs financial sentiment to improve FNS effectiveness by introducing a
 novel method that combines the sentiment polarity extracted from 
financial news with prompt augmentation techniques to ensure that the 
generated summaries are emotionally consistent with the source articles.
 Specifically, the detected sentiments are embedded into prompts and 
provide directive instructions to the model to generate summaries. 
Furthermore, to address the problem of limited large-scale datasets for 
FNS and ensure more tailored results, we employed prefix tuning as a 
fine-tuning strategy. Preliminary results indicate that our combined 
methodology outperforms approaches that use only prefix tuning. The 
experimental findings further validate the significance of sentiment 
analysis in FNS, which enhances the accuracy of capturing and reflecting
 market sentiment, thereby yielding valuable insights into financial 
markets. This method not only improves the accuracy and relevance of 
summaries but also ensures that their content is emotionally consistent 
with the source news, offering a new perspective on financial news 
summarization.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2024-12-26</td>
					</tr>
					<tr>
					<th>Language</th>
						<td>en</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>Springer Link</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://doi.org/10.1007/s42001-024-00352-w">https://doi.org/10.1007/s42001-024-00352-w</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>1/3/2025, 1:59:07 PM</td>
					</tr>
					<tr>
					<th>Volume</th>
						<td>8</td>
					</tr>
					<tr>
					<th>Pages</th>
						<td>19</td>
					</tr>
					<tr>
					<th>Publication</th>
						<td>Journal of Computational Social Science</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.1007/s42001-024-00352-w">10.1007/s42001-024-00352-w</a></td>
					</tr>
					<tr>
					<th>Issue</th>
						<td>1</td>
					</tr>
					<tr>
					<th>Journal Abbr</th>
						<td>J Comput Soc Sc</td>
					</tr>
					<tr>
					<th>ISSN</th>
						<td>2432-2725</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>1/3/2025, 1:59:07 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Financial news summarization</li>
					<li>Financial sentiment analysis</li>
					<li>Natural language processing</li>
					<li>Prefix tuning</li>
					<li>Prompt augmentation</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_L8EQ2D3P">Full Text PDF					</li>
				</ul>
			</li>


			<li id="item_T7DMSAIT" class="item preprint">
			<h2>Prevalence and prevention of large language model use in crowd work</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Preprint</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Veniamin Veselovsky</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Manoel Horta Ribeiro</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Philip Cozzolino</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Andrew Gordon</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>David Rothschild</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Robert West</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>We show that the use of large language models (LLMs) is 
prevalent among crowd workers, and that targeted mitigation strategies 
can significantly reduce, but not eliminate, LLM use. On a text 
summarization task where workers were not directed in any way regarding 
their LLM use, the estimated prevalence of LLM use was around 30%, but 
was reduced by about half by asking workers to not use LLMs and by 
raising the cost of using them, e.g., by disabling copy-pasting. 
Secondary analyses give further insight into LLM use and its prevention:
 LLM use yields high-quality but homogeneous responses, which may harm 
research concerned with human (rather than model) behavior and degrade 
future models trained with crowdsourced data. At the same time, 
preventing LLM use may be at odds with obtaining high-quality responses;
 e.g., when requesting workers not to use LLMs, summaries contained 
fewer keywords carrying essential information. Our estimates will likely
 change as LLMs increase in popularity or capabilities, and as norms 
around their usage change. Yet, understanding the co-evolution of 
LLM-based tools and users is key to maintaining the validity of research
 done using crowdsourcing, and we provide a critical baseline before 
widespread adoption ensues.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2023-10-24</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>arXiv.org</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://arxiv.org/abs/2310.15683">http://arxiv.org/abs/2310.15683</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>1/8/2025, 3:23:29 PM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>arXiv:2310.15683 [cs]</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.48550/arXiv.2310.15683">10.48550/arXiv.2310.15683</a></td>
					</tr>
					<tr>
					<th>Repository</th>
						<td>arXiv</td>
					</tr>
					<tr>
					<th>Archive ID</th>
						<td>arXiv:2310.15683</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>1/8/2025, 3:23:29 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Computer Science - Computation and Language</li>
				</ul>
				<h3 class="notes">Notes:</h3>
				<ul class="notes">
					<li id="item_B7NSX74D">
<p class="plaintext">Comment: VV and MHR equal contribution. 14 pages, 1 figure, 1 table</p>
					</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_ZBY5RLMP">Preprint PDF					</li>
					<li id="item_U52SXYBV">Snapshot					</li>
				</ul>
			</li>


			<li id="item_MD4A2L3D" class="item journalArticle">
			<h2>Principles and Practice of Explainable Machine Learning</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Vaishak Belle</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Ioannis Papantonis</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>&lt;p&gt;Artificial intelligence (AI) provides many 
opportunities to improve private and public life. Discovering patterns 
and structures in large troves of data in an automated manner is a core 
component of data science, and currently drives applications in diverse 
areas such as computational biology, law and finance. However, such a 
highly positive impact is coupled with a significant challenge: how do 
we understand the decisions suggested by these systems in order that we 
can trust them? In this report, we focus specifically on data-driven 
methods—machine learning (ML) and pattern recognition models in 
particular—so as to survey and distill the results and observations from
 the literature. The purpose of this report can be especially 
appreciated by noting that ML models are increasingly deployed in a wide
 range of businesses. However, with the increasing prevalence and 
complexity of methods, business stakeholders in the very least have a 
growing number of concerns about the drawbacks of models, data-specific 
biases, and so on. Analogously, data science practitioners are often not
 aware about approaches emerging from the academic literature or may 
struggle to appreciate the differences between different methods, so end
 up using industry standards such as SHAP. Here, we have undertaken a 
survey to help industry practitioners (but also data scientists more 
broadly) understand the field of explainable machine learning better and
 apply the right tools. Our latter sections build a narrative around a 
putative data scientist, and discuss how she might go about explaining 
her models by asking the right questions. From an organization 
viewpoint, after motivating the area broadly, we discuss the main 
developments, including the principles that allow us to study 
transparent models vs. opaque models, as well as model-specific or 
model-agnostic post-hoc explainability approaches. We also briefly 
reflect on deep learning models, and conclude with a discussion about 
future research directions.&lt;/p&gt;</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2021-07-01</td>
					</tr>
					<tr>
					<th>Language</th>
						<td>English</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>Frontiers</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://www.frontiersin.org/journals/big-data/articles/10.3389/fdata.2021.688969/full">https://www.frontiersin.org/journals/big-data/articles/10.3389/fdata.2021.688969/full</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>1/3/2025, 12:27:42 PM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>Publisher: Frontiers</td>
					</tr>
					<tr>
					<th>Volume</th>
						<td>4</td>
					</tr>
					<tr>
					<th>Publication</th>
						<td>Frontiers in Big Data</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.3389/fdata.2021.688969">10.3389/fdata.2021.688969</a></td>
					</tr>
					<tr>
					<th>Journal Abbr</th>
						<td>Front. Big Data</td>
					</tr>
					<tr>
					<th>ISSN</th>
						<td>2624-909X</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>1/3/2025, 12:27:42 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Black-box models</li>
					<li>Explainable AI</li>
					<li>machine learning</li>
					<li>Survey</li>
					<li>Transparent models</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_P62WDXEA">Full Text PDF					</li>
				</ul>
			</li>


			<li id="item_RPJ643XD" class="item preprint">
			<h2>Prompt Design Matters for Computational Social Science Tasks but in Unpredictable Ways</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Preprint</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Shubham Atreja</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Joshua Ashkinaze</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Lingyao Li</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Julia Mendelsohn</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Libby Hemphill</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Manually annotating data for computational social science 
tasks can be costly, time-consuming, and emotionally draining. While 
recent work suggests that LLMs can perform such annotation tasks in 
zero-shot settings, little is known about how prompt design impacts 
LLMs' compliance and accuracy. We conduct a large-scale multi-prompt 
experiment to test how model selection (ChatGPT, PaLM2, and Falcon7b) 
and prompt design features (definition inclusion, output type, 
explanation, and prompt length) impact the compliance and accuracy of 
LLM-generated annotations on four CSS tasks (toxicity, sentiment, rumor 
stance, and news frames). Our results show that LLM compliance and 
accuracy are highly prompt-dependent. For instance, prompting for 
numerical scores instead of labels reduces all LLMs' compliance and 
accuracy. The overall best prompting setup is task-dependent, and minor 
prompt changes can cause large changes in the distribution of generated 
labels. By showing that prompt design significantly impacts the quality 
and distribution of LLM-generated annotations, this work serves as both a
 warning and practical guide for researchers and practitioners.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2024-06-17</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>arXiv.org</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://arxiv.org/abs/2406.11980">http://arxiv.org/abs/2406.11980</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>1/7/2025, 5:28:31 PM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>arXiv:2406.11980 [cs]</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.48550/arXiv.2406.11980">10.48550/arXiv.2406.11980</a></td>
					</tr>
					<tr>
					<th>Repository</th>
						<td>arXiv</td>
					</tr>
					<tr>
					<th>Archive ID</th>
						<td>arXiv:2406.11980</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>1/7/2025, 5:28:31 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Computer Science - Artificial Intelligence</li>
					<li>Computer Science - Computers and Society</li>
				</ul>
				<h3 class="notes">Notes:</h3>
				<ul class="notes">
					<li id="item_2JZKUZSY">
<p class="plaintext">Comment: under review</p>
					</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_DS69Y8EM">Preprint PDF					</li>
					<li id="item_9DR3XZ32">Snapshot					</li>
				</ul>
			</li>


			<li id="item_5AHRRIG7" class="item preprint">
			<h2>Prompt Refinement or Fine-tuning? Best Practices for using LLMs in Computational Social Science Tasks</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Preprint</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Anders Giovanni Møller</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Luca Maria Aiello</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Large Language Models are expressive tools that enable complex
 tasks of text understanding within Computational Social Science. Their 
versatility, while beneficial, poses a barrier for establishing 
standardized best practices within the field. To bring clarity on the 
values of different strategies, we present an overview of the 
performance of modern LLM-based classification methods on a benchmark of
 23 social knowledge tasks. Our results point to three best practices: 
select models with larger vocabulary and pre-training corpora; avoid 
simple zero-shot in favor of AI-enhanced prompting; fine-tune on 
task-specific data, and consider more complex forms instruction-tuning 
on multiple datasets only when only training data is more abundant.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2024-08-02</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>Prompt Refinement or Fine-tuning?</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>arXiv.org</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://arxiv.org/abs/2408.01346">http://arxiv.org/abs/2408.01346</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>1/7/2025, 5:25:57 PM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>arXiv:2408.01346 [cs]</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.48550/arXiv.2408.01346">10.48550/arXiv.2408.01346</a></td>
					</tr>
					<tr>
					<th>Repository</th>
						<td>arXiv</td>
					</tr>
					<tr>
					<th>Archive ID</th>
						<td>arXiv:2408.01346</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>1/7/2025, 5:25:57 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Computer Science - Computation and Language</li>
					<li>Computer Science - Computers and Society</li>
					<li>Physics - Physics and Society</li>
				</ul>
				<h3 class="notes">Notes:</h3>
				<ul class="notes">
					<li id="item_TRL5VI6M">
<p class="plaintext">Comment: 5 pages, 1 table</p>
					</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_V9KSW9T5">Preprint PDF					</li>
					<li id="item_7CIGWRHY">Snapshot					</li>
				</ul>
			</li>


			<li id="item_JHLG52D8" class="item preprint">
			<h2>Public Sector Preference under Unbalanced Marketization: An 
Analysis of Labor Market Segmentation and University Graduates’ Sectoral
 Choices in China</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Preprint</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Wei Tang</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>This study employs large language models (LLMs), K-means 
clustering, and propensity score matching to thoroughly examine the 
impact of regional marketization on labor market segmentation and 
university graduates’ sectoral choices. The findings reveal that 
regional variations in marketization levels lead to heterogeneity in the
 ownership composition of the primary labor market, resulting in 
different labor market segmentation structures. In southeastern coastal 
and metropolitan regions, the abundance of high-quality private-sector 
job opportunities makes the traditional “public-private” dichotomy 
insufficient for accurately describing the two-tiered labor market, thus
 challenging prevailing academic assumptions.
Due to variations in local marketization levels, university graduates 
across regions exhibit divergent preferences for public and private 
sector employment. Moreover, limited job availability creates a complex 
sectoral choice pattern. Specifically, in inland provinces and smaller 
cities, individuals with higher human capital are more likely to enter 
the public sector, while no significant correlation is found in 
southeastern coastal and metropolitan regions.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2024-12-11</td>
					</tr>
					<tr>
					<th>Language</th>
						<td>en-us</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>Public Sector Preference under Unbalanced Marketization</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>OSF Preprints</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://osf.io/acb8q">https://osf.io/acb8q</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>1/6/2025, 11:49:39 AM</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.31235/osf.io/acb8q">10.31235/osf.io/acb8q</a></td>
					</tr>
					<tr>
					<th>Repository</th>
						<td>OSF</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>1/6/2025, 11:49:39 AM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Labor Market Segregation</li>
					<li>Large Language Models</li>
					<li>Marketization</li>
					<li>Public Sector Preference</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_27FYJZXL">OSF Preprint					</li>
				</ul>
			</li>


			<li id="item_2GGVXTYC" class="item preprint">
			<h2>Reassembling Agency: Epistemic Practices in the Age of Artificial Intelligence</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Preprint</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Francis Lee</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>This article reflects on how sociology can understand and 
analyze the role of AI and data-driven methods in scientific practice 
without buying into the current AI hype. Drawing on analytical 
sensibilities developed in actor-network theory the article introduces 
the concept of agencing (agency as a verb) which refers to how 
scientists debate and configure the balance between human and machine 
agency in their work. The article suggests that we can come to a more 
nuanced understanding of the effects of AI in science by attending to 
actors’ agencing practices. The article draws on a decade long 
engagement with various digital practices in biosciences and uses this 
as a foil to discuss the more general question of agency in science. By 
discussing three ideal types of agencing, the article argues that AI and
 big data should not be regarded as a sweeping rupture in the tooling 
and practices of science, but rather as a continuation of long-standing 
patterns of practice. That is, agency, and the space for action and 
judgment, is organized differently in a big data or AI driven 
laboratory, however, it would be a mistake to understand this as a 
completely new configuration of epistemic agency. Rather we might 
understand these changes in the AI laboratory as building on statistical
 epistemic configurations going as far back as the birth of statistics 
in sociology in the 1700s and 1800s. 

But what are the consequences of introducing AI-analyses for biomedical 
knowledge production? What happens to biomedicine when human judgment 
and the traditional scientific method are supplemented with, and 
sometimes replaced by, AI and the analysis of large amounts of data? 

This text responds to a call for reflection on how “Artificial entities 
with humanlike properties would blur the distinction between human and 
non-human entities in important respects and challenge the particularity
 of man.”  This paper explores these questions from the point of view of
 agency and human judgment. What happens to human agency and judgment in
 scientific experiments with the introduction of AI? The paper discusses
 these changes from the point of view of the concepts hybrid agency, 
asking how the AI and data revolutions are reshaping how science is 
done, and how knowledge is produced in a world imbued with non-human 
actors that seem to be making inroads in all arenas of human life..</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2023-06-02</td>
					</tr>
					<tr>
					<th>Language</th>
						<td>en-us</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>Reassembling Agency</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>OSF Preprints</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://osf.io/nzswg">https://osf.io/nzswg</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>1/6/2025, 11:39:25 AM</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.31235/osf.io/nzswg">10.31235/osf.io/nzswg</a></td>
					</tr>
					<tr>
					<th>Repository</th>
						<td>OSF</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>1/6/2025, 11:39:25 AM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>actor-network theory</li>
					<li>agency</li>
					<li>artificial intelligence</li>
					<li>big data</li>
					<li>bioscience</li>
					<li>hybrid agency</li>
					<li>judgment</li>
					<li>styles of valuation</li>
					<li>valuation</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_MNNLZW8B">OSF Preprint					</li>
				</ul>
			</li>


			<li id="item_9LYS889J" class="item journalArticle">
			<h2>Reclaiming artificial intelligence accounts: A plea for a participatory turn in artificial intelligence inquiries</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Pauline Gourlet</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Donato Ricci</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Maxime Crépel</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>How to participate in artificial intelligence otherwise? Put 
simply, when it comes to technological developments, participation is 
either understood as public debates with non-expert voices to anticipate
 risks and potential harms, or as a way to better design technical 
systems by involving diverse stakeholders in the design process. We 
advocate for a third path that considers participation as crucial to 
problematise what is at stake and to get a grip on the situated 
developments of artificial intelligence technologies.This study 
addresses how the production of accounts shape problems that arise with 
artificial intelligence technologies. Taking France as a field of study,
 we first inspected how media narratives account for the entities and 
issues of artificial intelligence, as reported by the national press 
over the last decade. From this inspection, we identified four genres 
and described their performative effects. We then conducted a 
participatory inquiry with 25 French artificial intelligence 
practitioners’ to ground artificial intelligence in situated experiences
 and trajectories. These experiential accounts enabled a plural 
problematisation of artificial intelligence, playing with the geometries
 of artificial intelligence and its constituencies, while diversifying 
and thickening its problems.To conclude, we discuss how participatory 
inquiries, through experiential and plural accounts offer a refreshing 
weaving of artificial intelligence problems into the fabric of its 
deployments. Our participatory approach seeks to re-politicise 
artificial intelligence from practitioners’ situated experiences, by 
making the ongoing relationships between past trajectories, current 
frictions and future developments tangible and contestable, opening 
avenues to contribute otherwise.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2024-06-01</td>
					</tr>
					<tr>
					<th>Language</th>
						<td>en</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>Reclaiming artificial intelligence accounts</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>SAGE Journals</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://doi.org/10.1177/20539517241248093">https://doi.org/10.1177/20539517241248093</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>12/23/2024, 4:52:07 PM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>Publisher: SAGE Publications Ltd</td>
					</tr>
					<tr>
					<th>Volume</th>
						<td>11</td>
					</tr>
					<tr>
					<th>Pages</th>
						<td>20539517241248093</td>
					</tr>
					<tr>
					<th>Publication</th>
						<td>Big Data &amp; Society</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.1177/20539517241248093">10.1177/20539517241248093</a></td>
					</tr>
					<tr>
					<th>Issue</th>
						<td>2</td>
					</tr>
					<tr>
					<th>ISSN</th>
						<td>2053-9517</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>12/23/2024, 4:52:07 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="notes">Notes:</h3>
				<ul class="notes">
					<li id="item_F2RMMVZH">
<div><div data-schema-version="8"><p>This might work into the conclusion, regarding CSS actively participating in the development of LLM usage.</p>
</div></div>
					</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_UAVWMXCK">SAGE PDF Full Text					</li>
				</ul>
			</li>


			<li id="item_PJLX48RR" class="item journalArticle">
			<h2>Responsible AI literacy: A stakeholder-first approach</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Daniel Domínguez Figaredo</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Julia Stoyanovich</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>The need for citizens to better understand the ethical and 
social challenges of algorithmic systems has led to a rapid 
proliferation of AI literacy initiatives. After reviewing the literature
 on AI literacy projects, we found that most educational practices in 
this area are based on teaching programming fundamentals, primarily to 
K-12 students. This leaves out citizens and those who are primarily 
interested in understanding the implications of automated decision- 
making systems, rather than in learning to code. To address these gaps, 
this article explores the methodological contributions of responsible AI
 education practices that focus first on stakeholders when designing 
learning experiences for different audiences and contexts. The article 
examines the weaknesses identified in current AI literacy projects, 
explains the stakeholder-first approach, and analyzes several 
responsible AI education case studies, to illustrate how such an 
approach can help overcome the aforementioned limitations. The results 
suggest that the stakeholder-first approach allows to address audiences 
beyond the usual ones in the field of AI literacy, and to incorporate 
new content and methodologies depending on the needs of the respective 
audiences, thus opening new avenues for teaching and research in the 
field.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2023-07-01</td>
					</tr>
					<tr>
					<th>Language</th>
						<td>en</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>Responsible AI literacy</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>SAGE Journals</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://doi.org/10.1177/20539517231219958">https://doi.org/10.1177/20539517231219958</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>12/23/2024, 5:36:21 PM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>Publisher: SAGE Publications Ltd</td>
					</tr>
					<tr>
					<th>Volume</th>
						<td>10</td>
					</tr>
					<tr>
					<th>Pages</th>
						<td>20539517231219958</td>
					</tr>
					<tr>
					<th>Publication</th>
						<td>Big Data &amp; Society</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.1177/20539517231219958">10.1177/20539517231219958</a></td>
					</tr>
					<tr>
					<th>Issue</th>
						<td>2</td>
					</tr>
					<tr>
					<th>ISSN</th>
						<td>2053-9517</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>12/23/2024, 5:36:21 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_JF42BYAG">SAGE PDF Full Text					</li>
				</ul>
			</li>


			<li id="item_3LKY58IX" class="item preprint">
			<h2>Rethinking Scale: The Efficacy of Fine-Tuned Open-Source LLMs in Large-Scale Reproducible Social Science Research</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Preprint</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Marcello Carammia</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Stefano Maria Iacus</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Giuseppe Porro</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Large Language Models (LLMs) are distinguished by their 
architecture, which dictates their parameter size and performance 
capabilities. Social scientists have increasingly adopted LLMs for text 
classification tasks, which are difficult to scale with human coders. 
While very large, closed-source models often deliver superior 
performance, their use presents significant risks. These include lack of
 transparency, potential exposure of sensitive data, challenges to 
replicability, and dependence on proprietary systems. Additionally, 
their high costs make them impractical for large-scale research 
projects. In contrast, open-source models, although available in various
 sizes, may underperform compared to commercial alternatives if used 
without further fine-tuning. However, open-source models offer distinct 
advantages: they can be run locally (ensuring data privacy), fine-tuned 
for specific tasks, shared within the research community, and integrated
 into reproducible workflows. This study demonstrates that small, 
fine-tuned open-source LLMs can achieve equal or superior performance to
 models such as ChatGPT-4. We further explore the relationship between 
training set size and fine-tuning efficacy in open-source models. 
Finally, we propose a hybrid workflow that leverages the strengths of 
both open and closed models, offering a balanced approach to 
performance, transparency, and reproducibility.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2024-10-31</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>Rethinking Scale</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>arXiv.org</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://arxiv.org/abs/2411.00890">http://arxiv.org/abs/2411.00890</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>1/7/2025, 5:02:49 PM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>arXiv:2411.00890 [cs]</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.48550/arXiv.2411.00890">10.48550/arXiv.2411.00890</a></td>
					</tr>
					<tr>
					<th>Repository</th>
						<td>arXiv</td>
					</tr>
					<tr>
					<th>Archive ID</th>
						<td>arXiv:2411.00890</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>1/7/2025, 5:02:49 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Computer Science - Artificial Intelligence</li>
					<li>Computer Science - Computation and Language</li>
					<li>Statistics - Machine Learning</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_HAD89S88">Preprint PDF					</li>
					<li id="item_7ZPIWC5X">Snapshot					</li>
				</ul>
			</li>


			<li id="item_5C5P658C" class="item journalArticle">
			<h2>Retweet-BERT: Political Leaning Detection Using Language Features and Information Diffusion on Social Networks</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Julie Jiang</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Xiang Ren</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Emilio Ferrara</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Estimating the political leanings of social media users is a 
challenging and ever more pressing problem given the increase in social 
media consumption. We introduce Retweet-BERT, a simple and scalable 
model to estimate the political leanings of Twitter users. Retweet-BERT 
leverages the retweet network structure and the language used in users' 
profile descriptions. Our assumptions stem from patterns of networks and
 linguistics homophily among people who share similar ideologies.  
Retweet-BERT demonstrates competitive performance against other 
state-of-the-art baselines, achieving 96%-97% macro-F1 on two recent 
Twitter datasets (a COVID-19 dataset and a 2020 United States 
presidential elections dataset). We also perform manual validation to 
validate the performance of Retweet-BERT on users not in the training 
data. Finally, in a case study of COVID-19, we illustrate the presence 
of political echo chambers on Twitter and show that it exists primarily 
among right-leaning users. Our code is open-sourced and our data is 
publicly available.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2023-06-02</td>
					</tr>
					<tr>
					<th>Language</th>
						<td>en</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>Retweet-BERT</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>ojs.aaai.org</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://ojs.aaai.org/index.php/ICWSM/article/view/22160">https://ojs.aaai.org/index.php/ICWSM/article/view/22160</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>1/8/2025, 2:16:28 PM</td>
					</tr>
					<tr>
					<th>Rights</th>
						<td>Copyright (c) 2023 Association for the Advancement of Artificial Intelligence</td>
					</tr>
					<tr>
					<th>Volume</th>
						<td>17</td>
					</tr>
					<tr>
					<th>Pages</th>
						<td>459-469</td>
					</tr>
					<tr>
					<th>Publication</th>
						<td>Proceedings of the International AAAI Conference on Web and Social Media</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.1609/icwsm.v17i1.22160">10.1609/icwsm.v17i1.22160</a></td>
					</tr>
					<tr>
					<th>ISSN</th>
						<td>2334-0770</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>1/8/2025, 2:16:28 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Human computer interaction</li>
					<li>navigation and visualization</li>
					<li>social media tools</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_Y4MSCF5B">Full Text PDF					</li>
				</ul>
			</li>


			<li id="item_D9ZNIPMD" class="item preprint">
			<h2>Risks of Cultural Erasure in Large Language Models</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Preprint</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Rida Qadri</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Aida M. Davani</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Kevin Robinson</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Vinodkumar Prabhakaran</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Large language models are increasingly being integrated into 
applications that shape the production and discovery of societal 
knowledge such as search, online education, and travel planning. As a 
result, language models will shape how people learn about, perceive and 
interact with global cultures making it important to consider whose 
knowledge systems and perspectives are represented in models. 
Recognizing this importance, increasingly work in Machine Learning and 
NLP has focused on evaluating gaps in global cultural representational 
distribution within outputs. However, more work is needed on developing 
benchmarks for cross-cultural impacts of language models that stem from a
 nuanced sociologically-aware conceptualization of cultural impact or 
harm. We join this line of work arguing for the need of metricizable 
evaluations of language technologies that interrogate and account for 
historical power inequities and differential impacts of representation 
on global cultures, particularly for cultures already under-represented 
in the digital corpora. We look at two concepts of erasure: omission: 
where cultures are not represented at all and simplification i.e. when 
cultural complexity is erased by presenting one-dimensional views of a 
rich culture. The former focuses on whether something is represented, 
and the latter on how it is represented. We focus our analysis on two 
task contexts with the potential to influence global cultural 
production. First, we probe representations that a language model 
produces about different places around the world when asked to describe 
these contexts. Second, we analyze the cultures represented in the 
travel recommendations produced by a set of language model applications.
 Our study shows ways in which the NLP community and application 
developers can begin to operationalize complex socio-cultural 
considerations into standard evaluations and benchmarks.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2025-01-02</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>arXiv.org</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://arxiv.org/abs/2501.01056">http://arxiv.org/abs/2501.01056</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>1/7/2025, 4:39:44 PM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>arXiv:2501.01056 [cs]</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.48550/arXiv.2501.01056">10.48550/arXiv.2501.01056</a></td>
					</tr>
					<tr>
					<th>Repository</th>
						<td>arXiv</td>
					</tr>
					<tr>
					<th>Archive ID</th>
						<td>arXiv:2501.01056</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>1/7/2025, 4:39:44 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Computer Science - Artificial Intelligence</li>
					<li>Computer Science - Computation and Language</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_W22QL9R3">Preprint PDF					</li>
					<li id="item_5XW2QZ8F">Snapshot					</li>
				</ul>
			</li>


			<li id="item_WCZFVSAK" class="item journalArticle">
			<h2>Sample Size Considerations for Fine-Tuning Large Language Models for Named Entity Recognition Tasks: Methodological Study</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Large language models (LLMs) have the potential to support 
promising new applications in health informatics. However, practical 
data on sample size considerations for fine-tuning LLMs to perform 
specific tasks in biomedical and health policy contexts are lacking. 
This study aims to evaluate sample size and sample selection techniques 
for fine-tuning LLMs to support improved named entity recognition (NER) 
for a custom data set of conflicts of interest disclosure statements.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2024/01/01</td>
					</tr>
					<tr>
					<th>Language</th>
						<td>en-US</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>Sample Size Considerations for Fine-Tuning Large Language Models for Named Entity Recognition Tasks</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>www.sciencedirect.com</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://www.sciencedirect.com/org/science/article/pii/S2817170524000279">https://www.sciencedirect.com/org/science/article/pii/S2817170524000279</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>1/8/2025, 3:28:51 PM</td>
					</tr>
					<tr>
					<th>Volume</th>
						<td>3</td>
					</tr>
					<tr>
					<th>Publication</th>
						<td>JMIR AI</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.2196/52095">10.2196/52095</a></td>
					</tr>
					<tr>
					<th>ISSN</th>
						<td>2817-1705</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>1/8/2025, 3:28:51 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 8:10:55 PM</td>
					</tr>
				</tbody></table>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_MCEA9H2C">Full Text					</li>
					<li id="item_NZUKKVHC">Snapshot					</li>
				</ul>
			</li>


			<li id="item_VGR4W5JZ" class="item journalArticle">
			<h2>Seeded Topic Models in Digital Archives: Analyzing Interpretations of Immigration in Swedish Newspapers, 1945–2019</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Miriam Hurtado Bodell</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Måns Magnusson</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Marc Keuschnigg</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Sociologists are discussing the need for more formal ways to 
extract meaning from digital text archives. We focus attention on the 
seeded topic model, a semi-supervised extension to the standard topic 
model that allows sociological knowledge to be infused into the 
computational learning of meaning structures. Seed words help 
crystallize topics around known concepts, while utilizing topic models’ 
functionality to identify associations in text based on word 
co-occurrences. The method estimates a concept’s shared interpretation 
(or framing) via its associations with other frequently co-occurring 
topics. In a case study, we extract longitudinal measures of media 
frames regarding immigration from a vast corpus of millions of Swedish 
newspaper articles from the period 1945–2019. We infer turning points 
that partition the immigration discourse into meaningful eras and locate
 Sweden’s era of multicultural ideals that coined its tolerant 
reputation.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2024-08-22</td>
					</tr>
					<tr>
					<th>Language</th>
						<td>en</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>Seeded Topic Models in Digital Archives</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>SAGE Journals</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://doi.org/10.1177/00491241241268453">https://doi.org/10.1177/00491241241268453</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>1/6/2025, 1:28:49 PM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>Publisher: SAGE Publications Inc</td>
					</tr>
					<tr>
					<th>Pages</th>
						<td>00491241241268453</td>
					</tr>
					<tr>
					<th>Publication</th>
						<td>Sociological Methods &amp; Research</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.1177/00491241241268453">10.1177/00491241241268453</a></td>
					</tr>
					<tr>
					<th>ISSN</th>
						<td>0049-1241</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>1/6/2025, 1:28:49 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="notes">Notes:</h3>
				<ul class="notes">
					<li id="item_H97WWZ73">
<div><div data-schema-version="8"><p>excxellent commentary/sources on current state of LLM</p>
<p></p>
</div></div>
					</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_B8MV7U2L">SAGE PDF Full Text					</li>
				</ul>
			</li>


			<li id="item_6QGH4S33" class="item journalArticle">
			<h2>Segmentation using large language models: A new typology of American neighborhoods</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Alex D. Singleton</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Seth Spielman</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>In the United States, recent changes to the National 
Statistical System have amplified the geographic-demographic resolution 
trade-off. That is, when working with demographic and economic data from
 the American Community Survey, as one zooms in geographically one loses
 resolution demographically due to very large margins of error. In this 
paper, we present a solution to this problem in the form of an AI based 
open and reproducible geodemographic classification system for the 
United States using small area estimates from the American Community 
Survey (ACS). We employ a partitioning clustering algorithm to a range 
of socio-economic, demographic, and built environment variables. Our 
approach utilizes an open source software pipeline that ensures 
adaptability to future data updates. A key innovation is the integration
 of GPT4, a state-of-the-art large language model, to generate intuitive
 cluster descriptions and names. This represents a novel application of 
natural language processing in geodemographic research and showcases the
 potential for human-AI collaboration within the geospatial domain.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2024/12</td>
					</tr>
					<tr>
					<th>Language</th>
						<td>en</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>Segmentation using large language models</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>epjdatascience.springeropen.com</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://epjdatascience.springeropen.com/articles/10.1140/epjds/s13688-024-00466-1">https://epjdatascience.springeropen.com/articles/10.1140/epjds/s13688-024-00466-1</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>1/3/2025, 12:37:22 PM</td>
					</tr>
					<tr>
					<th>Rights</th>
						<td>2024 The Author(s)</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>Number: 1
Publisher: SpringerOpen</td>
					</tr>
					<tr>
					<th>Volume</th>
						<td>13</td>
					</tr>
					<tr>
					<th>Pages</th>
						<td>1-21</td>
					</tr>
					<tr>
					<th>Publication</th>
						<td>EPJ Data Science</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.1140/epjds/s13688-024-00466-1">10.1140/epjds/s13688-024-00466-1</a></td>
					</tr>
					<tr>
					<th>Issue</th>
						<td>1</td>
					</tr>
					<tr>
					<th>Journal Abbr</th>
						<td>EPJ Data Sci.</td>
					</tr>
					<tr>
					<th>ISSN</th>
						<td>2193-1127</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>1/3/2025, 12:37:22 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_CNAHHKB8">Full Text PDF					</li>
				</ul>
			</li>


			<li id="item_Z5QEPCVZ" class="item journalArticle">
			<h2>Segmented Summarization and Refinement: A Pipeline for Long-Document Analysis on Social Media</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Guanghua Wang</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Priyanshi Garg</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Weili Wu</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>&lt;p&gt;Social media’s explosive growth has resulted in a 
massive influx of electronic documents influencing various facets of 
daily life. However, the enormous and complex nature of this content 
makes extracting valuable insights challenging. Long document 
summarization emerges as a pivotal technique in this context, serving to
 distill extensive texts into concise and comprehensible summaries. This
 paper presents a novel three-stage pipeline for effective long document
 summarization. The proposed approach combines unsupervised and 
supervised learning techniques, efficiently handling large document sets
 while requiring minimal computational resources. Our methodology 
introduces a unique process for forming semantic chunks through spectral
 dynamic segmentation, effectively reducing redundancy and 
repetitiveness in the summarization process. Contrary to previous 
methods, our approach aligns each semantic chunk with the entire summary
 paragraph, allowing the abstractive summarization model to process 
documents without truncation and enabling the summarization model to 
deduce missing information from other chunks. To enhance the summary 
generation, we utilize a sophisticated rewrite model based on 
Bidirectional and Auto-Regressive Transformers (BART), rearranging and 
reformulating summary constructs to improve their fluidity and 
coherence. Empirical studies conducted on the long documents from the 
Webis-TLDR-17 dataset demonstrate that our approach significantly 
enhances the efficiency of abstractive summarization transformers. The 
contributions of this paper thus offer significant advancements in the 
field of long document summarization, providing a novel and effective 
methodology for summarizing extensive texts in the context of social 
media.&lt;/p&gt;</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2024/06/01</td>
					</tr>
					<tr>
					<th>Language</th>
						<td>en</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>Segmented Summarization and Refinement</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>www.sciopen.com</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://www.sciopen.com/article/10.23919/JSC.2024.0010">https://www.sciopen.com/article/10.23919/JSC.2024.0010</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>12/24/2024, 3:54:57 PM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>Publisher: 清华大学出版社</td>
					</tr>
					<tr>
					<th>Volume</th>
						<td>5</td>
					</tr>
					<tr>
					<th>Pages</th>
						<td>132-144</td>
					</tr>
					<tr>
					<th>Publication</th>
						<td>Journal of Social Computing</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.23919/JSC.2024.0010">10.23919/JSC.2024.0010</a></td>
					</tr>
					<tr>
					<th>Issue</th>
						<td>2</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>12/24/2024, 3:54:57 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_2VGHLKSG">Full Text PDF					</li>
				</ul>
			</li>


			<li id="item_YMAK4WT7" class="item preprint">
			<h2>Self-Alignment of Large Language Models via Monopolylogue-based Social Scene Simulation</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Preprint</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Xianghe Pang</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Shuo Tang</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Rui Ye</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Yuxin Xiong</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Bolun Zhang</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Yanfeng Wang</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Siheng Chen</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Aligning large language models (LLMs) with human values is 
imperative to mitigate potential adverse effects resulting from their 
misuse. Drawing from the sociological insight that acknowledging all 
parties' concerns is a key factor in shaping human values, this paper 
proposes a novel direction to align LLMs by themselves: social scene 
simulation. To achieve this, we present MATRIX, a novel social scene 
simulator that emulates realistic scenes around a user's input query, 
enabling the LLM to take social consequences into account before 
responding. MATRIX serves as a virtual rehearsal space, akin to a 
Monopolylogue, where the LLM performs diverse roles related to the query
 and practice by itself. To inject this alignment, we fine-tune the LLM 
with MATRIX-simulated data, ensuring adherence to human values without 
compromising inference speed. We theoretically show that the LLM with 
MATRIX outperforms Constitutional AI under mild assumptions. Finally, 
extensive experiments validate that our method outperforms over 10 
baselines across 4 benchmarks. As evidenced by 875 user ratings, our 
tuned 13B-size LLM exceeds GPT-4 in aligning with human values. See our 
project page at https://shuotang123.github.io/MATRIX.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2024-06-08</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>arXiv.org</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://arxiv.org/abs/2402.05699">http://arxiv.org/abs/2402.05699</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>1/7/2025, 4:53:22 PM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>arXiv:2402.05699 [cs]</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.48550/arXiv.2402.05699">10.48550/arXiv.2402.05699</a></td>
					</tr>
					<tr>
					<th>Repository</th>
						<td>arXiv</td>
					</tr>
					<tr>
					<th>Archive ID</th>
						<td>arXiv:2402.05699</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>1/7/2025, 4:53:22 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Computer Science - Artificial Intelligence</li>
					<li>Computer Science - Computation and Language</li>
					<li>Computer Science - Computers and Society</li>
				</ul>
				<h3 class="notes">Notes:</h3>
				<ul class="notes">
					<li id="item_I5RHI6I4">
<p class="plaintext">Comment: 32 pages, 9 figures</p>
					</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_93X5NKV2">Preprint PDF					</li>
					<li id="item_2NBXAGWP">Snapshot					</li>
				</ul>
			</li>


			<li id="item_QCFMYKKW" class="item preprint">
			<h2>Sense and Sensitivity: Evaluating the simulation of social dynamics via Large Language Models</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Preprint</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Da Ju</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Adina Williams</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Brian Karrer</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Maximilian Nickel</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Large language models have increasingly been proposed as a 
powerful replacement for classical agent-based models (ABMs) to simulate
 social dynamics. By using LLMs as a proxy for human behavior, the hope 
of this new approach is to be able to simulate significantly more 
complex dynamics than with classical ABMs and gain new insights in 
fields such as social science, political science, and economics. 
However, due to the black box nature of LLMs, it is unclear whether LLM 
agents actually execute the intended semantics that are encoded in their
 natural language instructions and, if the resulting dynamics of 
interactions are meaningful. To study this question, we propose a new 
evaluation framework that grounds LLM simulations within the dynamics of
 established reference models of social science. By treating LLMs as a 
black-box function, we evaluate their input-output behavior relative to 
this reference model, which allows us to evaluate detailed aspects of 
their behavior. Our results show that, while it is possible to engineer 
prompts that approximate the intended dynamics, the quality of these 
simulations is highly sensitive to the particular choice of prompts. 
Importantly, simulations are even sensitive to arbitrary variations such
 as minor wording changes and whitespace. This puts into question the 
usefulness of current versions of LLMs for meaningful simulations, as 
without a reference model, it is impossible to determine a priori what 
impact seemingly meaningless changes in prompt will have on the 
simulation.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2024-12-06</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>Sense and Sensitivity</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>arXiv.org</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://arxiv.org/abs/2412.05093">http://arxiv.org/abs/2412.05093</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>1/7/2025, 5:00:41 PM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>arXiv:2412.05093 [cs]</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.48550/arXiv.2412.05093">10.48550/arXiv.2412.05093</a></td>
					</tr>
					<tr>
					<th>Repository</th>
						<td>arXiv</td>
					</tr>
					<tr>
					<th>Archive ID</th>
						<td>arXiv:2412.05093</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>1/7/2025, 5:00:41 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Computer Science - Computers and Society</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_VSVPM2UG">Preprint PDF					</li>
					<li id="item_PY8YS2ST">Snapshot					</li>
				</ul>
			</li>


			<li id="item_UTT47RA3" class="item journalArticle">
			<h2>Sentiment and semantic analysis: Urban quality inference using machine learning algorithms</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Emily Ho</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Michelle Schneider</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Sanjay Somanath</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Yinan Yu</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Liane Thuvander</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Sustainable urban transformation requires comprehensive 
knowledge about the built environment, including people’s perceptions, 
use of sites, and wishes. Qualitative interviews are conducted to 
understand better people’s opinions about a specific topic or location. 
This study explores the automatization of the interview coding process 
by investigating how state-of-the-art natural language processing 
techniques classify sentiment and semantic orientation from interviews 
transcribed in Swedish. For the sentiment analysis, the Swedish 
bidirectional encoder representations from transformers (BERT) model 
KB-BERT was used to perform a multi-class classification task on a text 
sentence level into three different classes: positive, negative, and 
neutral. Named entity recognition (NER) and string search were used for 
the semantic analysis to perform multi-label classification to match 
domain-related topics to the sentence. The models were trained and 
evaluated on partially annotated datasets. The results demonstrate that 
the implemented deep learning techniques are a possible and promising 
solution to achieve the stated goal.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2024-07-19</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>Sentiment and semantic analysis</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>ScienceDirect</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://www.sciencedirect.com/science/article/pii/S2589004224014172">https://www.sciencedirect.com/science/article/pii/S2589004224014172</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>1/8/2025, 3:29:20 PM</td>
					</tr>
					<tr>
					<th>Volume</th>
						<td>27</td>
					</tr>
					<tr>
					<th>Pages</th>
						<td>110192</td>
					</tr>
					<tr>
					<th>Publication</th>
						<td>iScience</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.1016/j.isci.2024.110192">10.1016/j.isci.2024.110192</a></td>
					</tr>
					<tr>
					<th>Issue</th>
						<td>7</td>
					</tr>
					<tr>
					<th>Journal Abbr</th>
						<td>iScience</td>
					</tr>
					<tr>
					<th>ISSN</th>
						<td>2589-0042</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>1/8/2025, 3:29:20 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Artificial intelligence</li>
					<li>Machine learning</li>
					<li>Urban planning</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_F9WLH4RF">ScienceDirect Snapshot					</li>
				</ul>
			</li>


			<li id="item_PDZCILYW" class="item journalArticle">
			<h2>Separating the wheat from the chaff: A topic and keyword-based procedure for identifying research-relevant text*✰</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Alicia Eads</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Alexandra Schofield</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Fauna Mahootian</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>David Mimno</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Rens Wilderom</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Social scientists are using computational tools to expand 
their content research beyond what is humanly readable. This often 
requires filtering corpora for complex research concepts. The commonly 
used off-the-shelf filtering techniques are untested at this task. 
Dictionaries may not recognize language outside of investigators’ 
expectations and thresholding on topic proportions from topic models may
 fail to identify brief references to concepts. We develop a typology of
 texts as they relate to a research concept and use this to structure a 
filtering procedure. We compare our procedure's performance with 
dictionary-only and topic-proportion-only approaches on two 
corpora—government speeches and academic articles—and two research 
concepts—housing crisis and inequality. Our procedure outperforms 
overall and on each type of relevant text in the typology. An 
open-source software package is available for implementing the 
procedure. This provides researchers with a more structured and tested 
approach for filtering text data. Additionally, the types-of-text 
typology analysis provides a unique examination of what constitutes a 
filtered dataset, allowing researchers to consider how conclusions may 
be affected.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2021-06-01</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>Separating the wheat from the chaff</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>ScienceDirect</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://www.sciencedirect.com/science/article/pii/S0304422X20302813">https://www.sciencedirect.com/science/article/pii/S0304422X20302813</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>5/5/2024, 8:27:19 PM</td>
					</tr>
					<tr>
					<th>Volume</th>
						<td>86</td>
					</tr>
					<tr>
					<th>Pages</th>
						<td>101527</td>
					</tr>
					<tr>
					<th>Publication</th>
						<td>Poetics</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.1016/j.poetic.2020.101527">10.1016/j.poetic.2020.101527</a></td>
					</tr>
					<tr>
					<th>Journal Abbr</th>
						<td>Poetics</td>
					</tr>
					<tr>
					<th>ISSN</th>
						<td>0304-422X</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>5/5/2024, 8:27:19 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>5/5/2024, 8:27:19 PM</td>
					</tr>
				</tbody></table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Filtering</li>
					<li>Latent dirichlet allocation</li>
					<li>Text analysis</li>
					<li>Topic model</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_6K8JUMSH">ScienceDirect Snapshot					</li>
				</ul>
			</li>


			<li id="item_7MCLGVFE" class="item journalArticle">
			<h2>Should ChatGPT be Biased? Challenges and Risks of Bias in Large Language Models</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Emilio Ferrara</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>As the capabilities of generative language models continue to 
advance, the implications of biases ingrained within these models have 
garnered increasing attention from researchers, practitioners, and the 
broader public. This article investigates the challenges and risks 
associated with biases in large-scale language models like ChatGPT. We 
discuss the origins of biases, stemming from, among others, the nature 
of training data, model specifications, algorithmic constraints, product
 design, and policy decisions. We explore the ethical concerns arising 
from the unintended consequences of biased model outputs. We further 
analyze the potential opportunities to mitigate biases, the 
inevitability of some biases, and the implications of deploying these 
models in various applications, such as virtual assistants, content 
generation, and chatbots. Finally, we review the current approaches to 
identify, quantify, and mitigate biases in language models, emphasizing 
the need for a multi-disciplinary, collaborative effort to develop more 
equitable, transparent, and responsible AI systems. This article aims to
 stimulate a thoughtful dialogue within the artificial intelligence 
community, encouraging researchers and developers to reflect on the role
 of biases in generative language models and the ongoing pursuit of 
ethical AI.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2023-11-07</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>Should ChatGPT be Biased?</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>arXiv.org</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://arxiv.org/abs/2304.03738">http://arxiv.org/abs/2304.03738</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>1/8/2025, 2:15:52 PM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>arXiv:2304.03738 [cs]</td>
					</tr>
					<tr>
					<th>Publication</th>
						<td>First Monday</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.5210/fm.v28i11.13346">10.5210/fm.v28i11.13346</a></td>
					</tr>
					<tr>
					<th>Journal Abbr</th>
						<td>FM</td>
					</tr>
					<tr>
					<th>ISSN</th>
						<td>1396-0466</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>1/8/2025, 2:15:52 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Computer Science - Computation and Language</li>
					<li>Computer Science - Computers and Society</li>
				</ul>
				<h3 class="notes">Notes:</h3>
				<ul class="notes">
					<li id="item_JMLSJ65Y">
<p class="plaintext">Comment: Published on First Monday https://firstmonday.org/ojs/index.php/fm/article/view/13346/11365</p>
					</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_M7K4CHHX">Preprint PDF					</li>
					<li id="item_9MXP8GYW">Snapshot					</li>
				</ul>
			</li>


			<li id="item_AA8KCVW7" class="item preprint">
			<h2>Simple dynamic word embeddings for mapping perceptions in the public sphere</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Preprint</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Nabeel Gillani</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Roger Levy</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Word embeddings trained on large-scale historical corpora can 
illuminate human biases and stereotypes that perpetuate social 
inequalities. These embeddings are often trained in separate vector 
space models defined according to different attributes of interest. In 
this paper, we develop a unified dynamic embedding model that learns 
attribute-specific word embeddings. We apply our model to investigate i)
 20th century gender and ethnic occupation biases embedded in the Corpus
 of Historical American English (COHA), and ii) biases against refugees 
embedded in a novel corpus of talk radio transcripts containing 119 
million words produced over one month across 83 stations and 64 cities. 
Our results shed preliminary light on scenarios when dynamic embedding 
models may be more suitable for representing linguistic biases than 
individual vector space models, and vice-versa.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2020-09-30</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>arXiv.org</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://arxiv.org/abs/1904.03352">http://arxiv.org/abs/1904.03352</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>1/8/2025, 12:55:55 PM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>arXiv:1904.03352 [cs]</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.48550/arXiv.1904.03352">10.48550/arXiv.1904.03352</a></td>
					</tr>
					<tr>
					<th>Repository</th>
						<td>arXiv</td>
					</tr>
					<tr>
					<th>Archive ID</th>
						<td>arXiv:1904.03352</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>1/8/2025, 12:55:55 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Computer Science - Computers and Society</li>
				</ul>
				<h3 class="notes">Notes:</h3>
				<ul class="notes">
					<li id="item_E5C8TXB3">
<p class="plaintext">Comment: This is a modified version of a paper that originally appeared in the 2019 NAACL workshop on NLP+CSS. Since its initial publication, we discovered implementation errors that invalidate the results presented in the original version of the paper. These errors are corrected in this version. A corrigendum at the end of this paper summarizes the original errors and how we corrected them</p>
					</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_FFVI3ZUK">Preprint PDF					</li>
					<li id="item_4G29NI8U">Snapshot					</li>
				</ul>
			</li>


			<li id="item_QXXDPBJ2" class="item preprint">
			<h2>Simulating Subjects: The Promise and Peril of AI Stand-ins for Social Agents and Interactions</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Preprint</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Austin Kozlowski</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>James Evans</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Large Language Models (LLMs), through their exposure to 
massive collections of online text, learn to reproduce the perspectives 
and linguistic styles of diverse social and cultural groups. This 
capability suggests a powerful social scientific application – the 
simulation of empirically realistic, culturally situated human subjects.
 Synthesizing recent research in artificial intelligence and 
computational social science, we outline a methodological foundation for
 simulating human subjects and their social interactions. We then 
identify nine characteristics of current models that are likely to 
impair realistic simulation human subjects, including atemporality, 
social acceptability bias, uniformity, and poverty of sensory 
experience. For each of these areas, we discuss promising approaches for
 overcoming their associated shortcomings. Given the rate of change of 
these models, we advocate for an ongoing methodological program on the 
simulation of human subjects that keeps pace with rapid technical 
progress.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2024-09-11</td>
					</tr>
					<tr>
					<th>Language</th>
						<td>en-us</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>Simulating Subjects</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>OSF Preprints</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://osf.io/vp3j2">https://osf.io/vp3j2</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>1/6/2025, 11:49:05 AM</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.31235/osf.io/vp3j2">10.31235/osf.io/vp3j2</a></td>
					</tr>
					<tr>
					<th>Repository</th>
						<td>OSF</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>1/6/2025, 11:49:05 AM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>AI</li>
					<li>Culture</li>
					<li>Interaction</li>
					<li>Simulation</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_EAG35LWJ">OSF Preprint					</li>
				</ul>
			</li>


			<li id="item_HU6IFTE9" class="item preprint">
			<h2>Social Science Meets LLMs: How Reliable Are Large Language Models in Social Simulations?</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Preprint</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Yue Huang</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Zhengqing Yuan</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Yujun Zhou</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Kehan Guo</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Xiangqi Wang</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Haomin Zhuang</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Weixiang Sun</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Lichao Sun</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Jindong Wang</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Yanfang Ye</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Xiangliang Zhang</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Large Language Models (LLMs) are increasingly employed for 
simulations, enabling applications in role-playing agents and 
Computational Social Science (CSS). However, the reliability of these 
simulations is under-explored, which raises concerns about the 
trustworthiness of LLMs in these applications. In this paper, we aim to 
answer ``How reliable is LLM-based simulation?'' To address this, we 
introduce TrustSim, an evaluation dataset covering 10 CSS-related 
topics, to systematically investigate the reliability of the LLM 
simulation. We conducted experiments on 14 LLMs and found that 
inconsistencies persist in the LLM-based simulated roles. In addition, 
the consistency level of LLMs does not strongly correlate with their 
general performance. To enhance the reliability of LLMs in simulation, 
we proposed Adaptive Learning Rate Based ORPO (AdaORPO), a reinforcement
 learning-based algorithm to improve the reliability in simulation 
across 7 LLMs. Our research provides a foundation for future studies to 
explore more robust and trustworthy LLM-based simulations.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2024-10-30</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>Social Science Meets LLMs</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>arXiv.org</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://arxiv.org/abs/2410.23426">http://arxiv.org/abs/2410.23426</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>1/7/2025, 5:04:19 PM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>arXiv:2410.23426 [cs]</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.48550/arXiv.2410.23426">10.48550/arXiv.2410.23426</a></td>
					</tr>
					<tr>
					<th>Repository</th>
						<td>arXiv</td>
					</tr>
					<tr>
					<th>Archive ID</th>
						<td>arXiv:2410.23426</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>1/7/2025, 5:04:19 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Computer Science - Computation and Language</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_GRSR2W9H">Preprint PDF					</li>
					<li id="item_2HY4AMKB">Snapshot					</li>
				</ul>
			</li>


			<li id="item_V3JRT8RP" class="item preprint">
			<h2>Social-LLM: Modeling User Behavior at Scale using Language Models and Social Network Data</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Preprint</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Julie Jiang</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Emilio Ferrara</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>The proliferation of social network data has unlocked 
unprecedented opportunities for extensive, data-driven exploration of 
human behavior. The structural intricacies of social networks offer 
insights into various computational social science issues, particularly 
concerning social influence and information diffusion. However, modeling
 large-scale social network data comes with computational challenges. 
Though large language models make it easier than ever to model textual 
content, any advanced network representation methods struggle with 
scalability and efficient deployment to out-of-sample users. In 
response, we introduce a novel approach tailored for modeling social 
network data in user detection tasks. This innovative method integrates 
localized social network interactions with the capabilities of large 
language models. Operating under the premise of social network 
homophily, which posits that socially connected users share 
similarities, our approach is designed to address these challenges. We 
conduct a thorough evaluation of our method across seven real-world 
social network datasets, spanning a diverse range of topics and 
detection tasks, showcasing its applicability to advance research in 
computational social science.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2023-12-31</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>Social-LLM</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>arXiv.org</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://arxiv.org/abs/2401.00893">http://arxiv.org/abs/2401.00893</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>1/8/2025, 2:14:55 PM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>arXiv:2401.00893 [cs]</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.48550/arXiv.2401.00893">10.48550/arXiv.2401.00893</a></td>
					</tr>
					<tr>
					<th>Repository</th>
						<td>arXiv</td>
					</tr>
					<tr>
					<th>Archive ID</th>
						<td>arXiv:2401.00893</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>1/8/2025, 2:14:55 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Computer Science - Artificial Intelligence</li>
					<li>Computer Science - Social and Information Networks</li>
				</ul>
				<h3 class="notes">Notes:</h3>
				<ul class="notes">
					<li id="item_SE7LDVFF">
<p class="plaintext">Comment: 10 pages, 5 figures, 2 tables</p>
					</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_27A723HU">Preprint PDF					</li>
					<li id="item_78KMX4E2">Snapshot					</li>
				</ul>
			</li>


			<li id="item_9SAQG5II" class="item preprint">
			<h2>SOCIALITE-LLAMA: An Instruction-Tuned Model for Social Scientific Tasks</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Preprint</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Gourab Dey</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Adithya V. Ganesan</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Yash Kumar Lal</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Manal Shah</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Shreyashee Sinha</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Matthew Matero</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Salvatore Giorgi</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Vivek Kulkarni</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>H. Andrew Schwartz</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Social science NLP tasks, such as emotion or humor detection, 
are required to capture the semantics along with the implicit pragmatics
 from text, often with limited amounts of training data. Instruction 
tuning has been shown to improve the many capabilities of large language
 models (LLMs) such as commonsense reasoning, reading comprehension, and
 computer programming. However, little is known about the effectiveness 
of instruction tuning on the social domain where implicit pragmatic cues
 are often needed to be captured. We explore the use of instruction 
tuning for social science NLP tasks and introduce Socialite-Llama -- an 
open-source, instruction-tuned Llama. On a suite of 20 social science 
tasks, Socialite-Llama improves upon the performance of Llama as well as
 matches or improves upon the performance of a state-of-the-art, 
multi-task finetuned model on a majority of them. Further, 
Socialite-Llama also leads to improvement on 5 out of 6 related social 
tasks as compared to Llama, suggesting instruction tuning can lead to 
generalized social understanding. All resources including our code, 
model and dataset can be found through bit.ly/socialitellama.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2024-03-14</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>SOCIALITE-LLAMA</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>arXiv.org</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://arxiv.org/abs/2402.01980">http://arxiv.org/abs/2402.01980</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>1/7/2025, 5:33:04 PM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>arXiv:2402.01980 [cs]</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.48550/arXiv.2402.01980">10.48550/arXiv.2402.01980</a></td>
					</tr>
					<tr>
					<th>Repository</th>
						<td>arXiv</td>
					</tr>
					<tr>
					<th>Archive ID</th>
						<td>arXiv:2402.01980</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>1/7/2025, 5:33:04 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Computer Science - Computation and Language</li>
				</ul>
				<h3 class="notes">Notes:</h3>
				<ul class="notes">
					<li id="item_M5ZDLPXT">
<p class="plaintext">Comment: Short paper accepted to EACL 2024. 4 pgs, 2 tables</p>
					</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_SRMIEXBZ">Preprint PDF					</li>
					<li id="item_BNNX4CL9">Snapshot					</li>
				</ul>
			</li>


			<li id="item_VMWL6KSR" class="item preprint">
			<h2>SPIN: Sparsifying and Integrating Internal Neurons in Large Language Models for Text Classification</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Preprint</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Difan Jiao</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Yilun Liu</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Zhenwei Tang</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Daniel Matter</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Jürgen Pfeffer</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Ashton Anderson</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Among the many tasks that Large Language Models (LLMs) have 
revolutionized is text classification. Current text classification 
paradigms, however, rely solely on the output of the final layer in the 
LLM, with the rich information contained in internal neurons largely 
untapped. In this study, we present SPIN: a model-agnostic framework 
that sparsifies and integrates internal neurons of intermediate layers 
of LLMs for text classification. Specifically, SPIN sparsifies internal 
neurons by linear probing-based salient neuron selection layer by layer,
 avoiding noise from unrelated neurons and ensuring efficiency. The 
cross-layer salient neurons are then integrated to serve as 
multi-layered features for the classification head. Extensive 
experimental results show our proposed SPIN significantly improves text 
classification accuracy, efficiency, and interpretability.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2024-06-05</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>SPIN</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>arXiv.org</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://arxiv.org/abs/2311.15983">http://arxiv.org/abs/2311.15983</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>1/8/2025, 3:22:21 PM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>arXiv:2311.15983 [cs]</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.48550/arXiv.2311.15983">10.48550/arXiv.2311.15983</a></td>
					</tr>
					<tr>
					<th>Repository</th>
						<td>arXiv</td>
					</tr>
					<tr>
					<th>Archive ID</th>
						<td>arXiv:2311.15983</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>1/8/2025, 3:22:21 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Computer Science - Artificial Intelligence</li>
					<li>Computer Science - Computation and Language</li>
					<li>Computer Science - Machine Learning</li>
				</ul>
				<h3 class="notes">Notes:</h3>
				<ul class="notes">
					<li id="item_ZFHBLQBR">
<p class="plaintext">Comment: 17 pages, 7 figures, 12 tables Code available at https://github.com/difanj0713/SPIN</p>
					</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_VW92V4J2">Preprint PDF					</li>
					<li id="item_9RIKZD8T">Snapshot					</li>
				</ul>
			</li>


			<li id="item_TGCPIY2V" class="item preprint">
			<h2>Start Generating: Harnessing Generative Artificial Intelligence for Sociological Research</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Preprint</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Thomas Davidson</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>The development of Generative Artificial Intelligence (GAI), 
exemplified by the introduction of ChatGPT, has sparked significant 
public debate. This paper explores the applications of these 
technologies as methods for sociological research, examining potential 
applications across three areas of methodology: computational, 
experimental, and qualitative research. Drawing upon recent research and
 stylized experiments with DALL-E and GPT-4, the paper argues that these
 technologies make advanced computational methods more accessible to the
 discipline and illustrates the generative potential of text-to-text, 
text-to-image, and image-to-text models. The paper also examines several
 new challenges raised by GAI, including interpretability, transparency,
 reliability, reproducibility, ethics, and privacy, as well as the 
implications for bias and bias mitigation efforts. The trade-offs 
between closed-source, proprietary models and open-source alternatives 
are also considered in the context of these challenges. Overall, this 
paper encourages sociologists to exercise caution with these models 
while embracing the opportunities presented by these technologies.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2023-11-02</td>
					</tr>
					<tr>
					<th>Language</th>
						<td>en</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>Start Generating</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>SocArXiv</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://osf.io/u9nft">https://osf.io/u9nft</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>5/2/2024, 4:47:46 PM</td>
					</tr>
					<tr>
					<th>Rights</th>
						<td>https://creativecommons.org/licenses/by-nd/4.0/legalcode</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.31235/osf.io/u9nft">10.31235/osf.io/u9nft</a></td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>5/2/2024, 4:47:46 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>5/2/2024, 4:47:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_CSC6K9P8">Davidson - 2023 - Start Generating Harnessing Generative Artificial.pdf					</li>
				</ul>
			</li>


			<li id="item_5DNI8PHJ" class="item journalArticle">
			<h2>Structured like a language model:  Analysing AI as an automated subject</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Liam Magee</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Vanicka Arora</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Luke Munn</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Drawing from the resources of psychoanalysis and critical 
media studies, in this article we develop an analysis of large language 
models (LLMs) as ‘automated subjects’. We argue the intentional 
fictional projection of subjectivity onto LLMs can yield an alternate 
frame through which artificial intelligence (AI) behaviour, including 
its productions of bias and harm, can be analysed. First, we introduce 
language models, discuss their significance and risks, and outline our 
case for interpreting model design and outputs with support from 
psychoanalytic concepts. We trace a brief history of language models, 
culminating with the releases, in 2022, of systems that realise 
‘state-of-the-art’ natural language processing performance. We engage 
with one such system, OpenAI's InstructGPT, as a case study, detailing 
the layers of its construction and conducting exploratory and 
semi-structured interviews with chatbots. These interviews probe the 
model's moral imperatives to be ‘helpful’, ‘truthful’ and ‘harmless’ by 
design. The model acts, we argue, as the condensation of often competing
 social desires, articulated through the internet and harvested into 
training data, which must then be regulated and repressed. This 
foundational structure can however be redirected via prompting, so that 
the model comes to identify with, and transfer, its commitments to the 
immediate human subject before it. In turn, these automated productions 
of language can lead to the human subject projecting agency upon the 
model, effecting occasionally further forms of countertransference. We 
conclude that critical media methods and psychoanalytic theory together 
offer a productive frame for grasping the powerful new capacities of 
AI-driven language systems.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2023-07-01</td>
					</tr>
					<tr>
					<th>Language</th>
						<td>en</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>Structured like a language model</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>SAGE Journals</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://doi.org/10.1177/20539517231210273">https://doi.org/10.1177/20539517231210273</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>12/23/2024, 5:34:50 PM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>Publisher: SAGE Publications Ltd</td>
					</tr>
					<tr>
					<th>Volume</th>
						<td>10</td>
					</tr>
					<tr>
					<th>Pages</th>
						<td>20539517231210273</td>
					</tr>
					<tr>
					<th>Publication</th>
						<td>Big Data &amp; Society</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.1177/20539517231210273">10.1177/20539517231210273</a></td>
					</tr>
					<tr>
					<th>Issue</th>
						<td>2</td>
					</tr>
					<tr>
					<th>ISSN</th>
						<td>2053-9517</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>12/23/2024, 5:34:50 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_IBW2AXH2">SAGE PDF Full Text					</li>
				</ul>
			</li>


			<li id="item_A84568NX" class="item journalArticle">
			<h2>Sunlight alone is not a disinfectant: Consent and the futility of opening Big Data black boxes (without assistance)</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Jonathan A. Obar</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>In our attempts to achieve privacy and reputation 
deliverables, advocating for service providers and other data managers 
to open Big Data black boxes and be more transparent about consent 
processes, algorithmic details, and data practice is easy. Moving from 
this call to meaningful forms of transparency, where the Big Data 
details are available, useful, and manageable is more difficult. Most 
challenging is moving from that difficult task of meaningful 
transparency to the seemingly impossible scenario of achieving, 
consistently and ubiquitously, meaningful forms of consent, where 
individuals are aware of data practices and implications, understand 
these realities, and agree to them as well. This commentary unpacks 
these concerns in the online consent context. It emphasizes that 
self-governance fallacy pervades current approaches to achieving digital
 forms of privacy, exemplified by the assertion that transparency and 
information access alone are enough to help individuals achieve privacy 
and reputation protections.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2020-01-01</td>
					</tr>
					<tr>
					<th>Language</th>
						<td>en</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>Sunlight alone is not a disinfectant</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>SAGE Journals</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://doi.org/10.1177/2053951720935615">https://doi.org/10.1177/2053951720935615</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>12/23/2024, 6:20:33 PM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>Publisher: SAGE Publications Ltd</td>
					</tr>
					<tr>
					<th>Volume</th>
						<td>7</td>
					</tr>
					<tr>
					<th>Pages</th>
						<td>2053951720935615</td>
					</tr>
					<tr>
					<th>Publication</th>
						<td>Big Data &amp; Society</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.1177/2053951720935615">10.1177/2053951720935615</a></td>
					</tr>
					<tr>
					<th>Issue</th>
						<td>1</td>
					</tr>
					<tr>
					<th>ISSN</th>
						<td>2053-9517</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>12/23/2024, 6:20:33 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_4A8GWXRC">SAGE PDF Full Text					</li>
				</ul>
			</li>


			<li id="item_D4D7FWZH" class="item preprint">
			<h2>Survey of Cultural Awareness in Language Models: Text and Beyond</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Preprint</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Siddhesh Pawar</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Junyeong Park</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Jiho Jin</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Arnav Arora</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Junho Myung</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Srishti Yadav</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Faiz Ghifari Haznitrama</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Inhwa Song</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Alice Oh</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Isabelle Augenstein</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Large-scale deployment of large language models (LLMs) in 
various applications, such as chatbots and virtual assistants, requires 
LLMs to be culturally sensitive to the user to ensure inclusivity. 
Culture has been widely studied in psychology and anthropology, and 
there has been a recent surge in research on making LLMs more culturally
 inclusive in LLMs that goes beyond multilinguality and builds on 
findings from psychology and anthropology. In this paper, we survey 
efforts towards incorporating cultural awareness into text-based and 
multimodal LLMs. We start by defining cultural awareness in LLMs, taking
 the definitions of culture from anthropology and psychology as a point 
of departure. We then examine methodologies adopted for creating 
cross-cultural datasets, strategies for cultural inclusion in downstream
 tasks, and methodologies that have been used for benchmarking cultural 
awareness in LLMs. Further, we discuss the ethical implications of 
cultural alignment, the role of Human-Computer Interaction in driving 
cultural inclusion in LLMs, and the role of cultural alignment in 
driving social science research. We finally provide pointers to future 
research based on our findings about gaps in the literature.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2024-10-30</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>Survey of Cultural Awareness in Language Models</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>arXiv.org</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://arxiv.org/abs/2411.00860">http://arxiv.org/abs/2411.00860</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>1/7/2025, 5:03:08 PM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>arXiv:2411.00860 [cs]</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.48550/arXiv.2411.00860">10.48550/arXiv.2411.00860</a></td>
					</tr>
					<tr>
					<th>Repository</th>
						<td>arXiv</td>
					</tr>
					<tr>
					<th>Archive ID</th>
						<td>arXiv:2411.00860</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>1/7/2025, 5:03:08 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Computer Science - Computation and Language</li>
					<li>Computer Science - Computer Vision and Pattern Recognition</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_SHC8RVR9">Preprint PDF					</li>
					<li id="item_TQFWE4TJ">Snapshot					</li>
				</ul>
			</li>


			<li id="item_7AYQ2UPV" class="item journalArticle">
			<h2>Synthetic ethnography: Field devices for the qualitative study of generative models</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Gabriele de Seta</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Matti Pohjonen</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Aleksi Knuutila</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Advancements in generative artificial intelligence have led to
 a rapid proliferation of machine learning models capable of 
synthesizing text, images, sounds, and other kinds of content. While the
 increasing realism of synthetic content stokes fears about 
misinformation and triggers debates around intellectual property, 
generative models are adopted across creative industries and synthetic 
media seep into cultural production. Qualitative research in the social 
and human sciences has dedicated comparatively little attention to this 
category of machine learning, particularly in terms of what types of 
novel research methodology they both demand and facilitate. In this 
article, we propose a methodological approach for the qualitative study 
of generative models grounded on the experimentation with field devices 
which we call synthetic ethnography. Synthetic ethnography is not simply
 a qualitative research methodology applied to the study of the social 
and cultural contexts developing around generative artificial 
intelligence, but also strives to envision practical and experimental 
ways to repurpose these models as research tools in their own right. 
After briefly introducing generative models and revisiting the 
trajectory of digital ethnographic research, we discuss three case 
studies for which the authors have developed experimental field devices 
to study different aspects of generative artificial intelligence 
ethnographically. In the conclusion, we derive a broader methodological 
proposal from these case studies, arguing that synthetic ethnography 
facilitates insights into how the algorithmic processes, training 
datasets and latent spaces behind these systems modulate bias, 
reconfigure agency, and challenge epistemological categories.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2024-12-01</td>
					</tr>
					<tr>
					<th>Language</th>
						<td>en</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>Synthetic ethnography</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>SAGE Journals</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://doi.org/10.1177/20539517241303126">https://doi.org/10.1177/20539517241303126</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>12/23/2024, 4:25:16 PM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>Publisher: SAGE Publications Ltd</td>
					</tr>
					<tr>
					<th>Volume</th>
						<td>11</td>
					</tr>
					<tr>
					<th>Pages</th>
						<td>20539517241303126</td>
					</tr>
					<tr>
					<th>Publication</th>
						<td>Big Data &amp; Society</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.1177/20539517241303126">10.1177/20539517241303126</a></td>
					</tr>
					<tr>
					<th>Issue</th>
						<td>4</td>
					</tr>
					<tr>
					<th>ISSN</th>
						<td>2053-9517</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>12/23/2024, 4:25:16 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_XCM86IZA">SAGE PDF Full Text					</li>
				</ul>
			</li>


			<li id="item_CSZGU9VE" class="item journalArticle">
			<h2>Tackling racial bias in automated online hate detection: Towards 
fair and accurate detection of hateful users with geometric deep 
learning</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Zo Ahmed</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Bertie Vidgen</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Scott A. Hale</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Online hate is a growing concern on many social media 
platforms, making them unwelcoming and unsafe. To combat this, 
technology companies are increasingly developing techniques to 
automatically identify and sanction hateful users. However, accurate 
detection of such users remains a challenge due to the contextual nature
 of speech, whose meaning depends on the social setting in which it is 
used. This contextual nature of speech has also led to minoritized 
users, especially African–Americans, to be unfairly detected as 
‘hateful’ by the very algorithms designed to protect them. To resolve 
this problem of inaccurate and unfair hate detection, research has 
focused on developing machine learning (ML) systems that better 
understand textual context. Incorporating social networks of hateful 
users has not received as much attention, despite social science 
research suggesting it provides rich contextual information. We present a
 system for more accurately and fairly detecting hateful users by 
incorporating social network information through geometric deep 
learning. Geometric deep learning is a ML technique that dynamically 
learns information-rich network representations. We make two main 
contributions: first, we demonstrate that adding network information 
with geometric deep learning produces a more accurate classifier 
compared with other techniques that either exclude network information 
entirely or incorporate it through manual feature engineering. Our best 
performing model achieves an AUC score of 90.8% on a previously released
 hateful user dataset. Second, we show that such information also leads 
to fairer outcomes: using the ‘predictive equality’ fairness criteria, 
we compare the false positive rates of our geometric learning algorithm 
to other ML techniques and find that our best-performing classifier has 
no false positives among a subset of African–American users. A neural 
network without network information has the largest number of false 
positives at 26, while a neural network incorporating manual network 
features has 13 false positives among African–American users. The system
 we present highlights the importance of effectively incorporating 
social network features in automated hateful user detection, raising new
 opportunities to improve how online hate is tackled.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2022/12</td>
					</tr>
					<tr>
					<th>Language</th>
						<td>en</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>Tackling racial bias in automated online hate detection</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>epjdatascience.springeropen.com</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://epjdatascience.springeropen.com/articles/10.1140/epjds/s13688-022-00319-9">https://epjdatascience.springeropen.com/articles/10.1140/epjds/s13688-022-00319-9</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>1/3/2025, 12:40:56 PM</td>
					</tr>
					<tr>
					<th>Rights</th>
						<td>2022 The Author(s)</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>Number: 1
Publisher: SpringerOpen</td>
					</tr>
					<tr>
					<th>Volume</th>
						<td>11</td>
					</tr>
					<tr>
					<th>Pages</th>
						<td>1-22</td>
					</tr>
					<tr>
					<th>Publication</th>
						<td>EPJ Data Science</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.1140/epjds/s13688-022-00319-9">10.1140/epjds/s13688-022-00319-9</a></td>
					</tr>
					<tr>
					<th>Issue</th>
						<td>1</td>
					</tr>
					<tr>
					<th>Journal Abbr</th>
						<td>EPJ Data Sci.</td>
					</tr>
					<tr>
					<th>ISSN</th>
						<td>2193-1127</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>1/3/2025, 12:40:56 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_2TFUXQ3R">Full Text PDF					</li>
				</ul>
			</li>


			<li id="item_E7T7ICAA" class="item preprint">
			<h2>Take Caution in Using LLMs as Human Surrogates: Scylla Ex Machina</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Preprint</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Yuan Gao</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Dokyun Lee</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Gordon Burtch</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Sina Fazelpour</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Recent studies suggest large language models (LLMs) can 
exhibit human-like reasoning, aligning with human behavior in economic 
experiments, surveys, and political discourse. This has led many to 
propose that LLMs can be used as surrogates or simulations for humans in
 social science research. However, LLMs differ fundamentally from 
humans, relying on probabilistic patterns, absent the embodied 
experiences or survival objectives that shape human cognition. We assess
 the reasoning depth of LLMs using the 11-20 money request game. Nearly 
all advanced approaches fail to replicate human behavior distributions 
across many models. Causes of failure are diverse and unpredictable, 
relating to input language, roles, and safeguarding. These results 
advise caution when using LLMs to study human behavior or as surrogates 
or simulations.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2024-11-16</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>Take Caution in Using LLMs as Human Surrogates</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>arXiv.org</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://arxiv.org/abs/2410.19599">http://arxiv.org/abs/2410.19599</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>1/7/2025, 5:04:49 PM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>arXiv:2410.19599 [econ]</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.48550/arXiv.2410.19599">10.48550/arXiv.2410.19599</a></td>
					</tr>
					<tr>
					<th>Repository</th>
						<td>arXiv</td>
					</tr>
					<tr>
					<th>Archive ID</th>
						<td>arXiv:2410.19599</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>1/7/2025, 5:04:49 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Computer Science - Artificial Intelligence</li>
					<li>Computer Science - Computers and Society</li>
					<li>Computer Science - Human-Computer Interaction</li>
					<li>Economics - General Economics</li>
					<li>Quantitative Finance - Economics</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_LSNXG8RD">Preprint PDF					</li>
					<li id="item_ZJUH2H8S">Snapshot					</li>
				</ul>
			</li>


			<li id="item_VYR6EGMP" class="item webpage">
			<h2>Tensor Embedding: A Supervised Framework for Human Behavioral 
Data Mining and Prediction | IEEE Conference Publication | IEEE Xplore</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Web Page</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Today’s densely instrumented world offers tremendous 
opportunities for continuous acquisition and analysis of multimodal 
sensor data, providing temporal characterization of individual 
behaviors. Is it possible to efficiently couple such rich sensor data 
with predictive modeling techniques to provide contextual and insightful
 assessments of individual behavior? Such data is noisy, incomplete, and
 collected from multiple sources, each with a different temporal 
resolution and dimensionality. In addition, longitudinal studies tend to
 examine a number of aspects of human behavior, such as well being, 
performance, or personality, which poses a challenge for handcrafted 
features engineering. To address these challenges, we propose supervised
 tensor embedding (STE), an algorithm for the joint decomposition of 
input and target variables in high-dimensional multimodal data. Latent 
features obtained from STE can be fed into any regression model for the 
estimation of the target variable(s). Following the embedding of 
higher-order data, features from different sources can be coupled. 
Additionally, we demonstrate that feature selection on higher-order data
 can enhance performance. The efficiency of our method is tested on two 
real-world datasets consisting of 29 target variables. Compared to the 
state of the art, we find that (i) our method outperforms others in 21 
prediction tasks, and (ii) there are a few aspects of human behavior 
that are unpredictable regardless of the method, which can be the 
limitation of multilinear methods or lack of proper independent 
variables.</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://ieeexplore.ieee.org/abstract/document/10337139">https://ieeexplore.ieee.org/abstract/document/10337139</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>1/8/2025, 2:17:11 PM</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>1/8/2025, 2:17:11 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 8:12:03 PM</td>
					</tr>
				</tbody></table>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_Y7D8DYIU">Tensor Embedding: A Supervised Framework for
 Human Behavioral Data Mining and Prediction | IEEE Conference 
Publication | IEEE Xplore					</li>
				</ul>
			</li>


			<li id="item_4QUZTUZC" class="item journalArticle">
			<h2>Text mining for social science – The state and the future of computational text analysis in sociology</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Ana Macanovic</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>The emergence of big data and computational tools has 
introduced new possibilities for using large-scale textual sources in 
sociological research. Recent work in sociology of culture, science, and
 economic sociology has shown how computational text analysis can be 
used in theory building and testing. This review starts with an 
introduction of the history of computer-assisted text analysis in 
sociology and then proceeds to discuss five families of computational 
methods used in contemporary research. Using exemplary studies, it shows
 how dictionary methods, semantic and network analysis tools, language 
models, unsupervised, and supervised machine learning can assist 
sociologists with different analytical tasks. After presenting recent 
methodological developments, this review summarizes several important 
implications of using large datasets and computational methods to infer 
complex meaning in texts. Finally, it calls researchers from different 
methodological traditions to adopt text mining tools while remaining 
mindful of lessons learned from working with conventional data and 
methods.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2022-11-01</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>ScienceDirect</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://www.sciencedirect.com/science/article/pii/S0049089X22000904">https://www.sciencedirect.com/science/article/pii/S0049089X22000904</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>1/7/2025, 4:08:48 PM</td>
					</tr>
					<tr>
					<th>Volume</th>
						<td>108</td>
					</tr>
					<tr>
					<th>Pages</th>
						<td>102784</td>
					</tr>
					<tr>
					<th>Publication</th>
						<td>Social Science Research</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.1016/j.ssresearch.2022.102784">10.1016/j.ssresearch.2022.102784</a></td>
					</tr>
					<tr>
					<th>Journal Abbr</th>
						<td>Social Science Research</td>
					</tr>
					<tr>
					<th>ISSN</th>
						<td>0049-089X</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>1/7/2025, 4:08:48 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Big data</li>
					<li>Content analysis</li>
					<li>Machine learning</li>
					<li>Natural language processing</li>
					<li>Text analysis</li>
					<li>Text mining</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_TYTE62UB">Full Text					</li>
					<li id="item_J6WZ5EJF">ScienceDirect Snapshot					</li>
				</ul>
			</li>


			<li id="item_CSU5KXI3" class="item journalArticle">
			<h2>Textual Indicators of Deliberative Dialogue: A Systematic Review of Methods for Studying the Quality of Online Dialogues</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Alex Goddard</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Alex Gillespie</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>High-quality online dialogues help sustain democracy. 
Deliberative theory, which predates the internet, provides the primary 
model for assessing the quality of online dialogues. It conceptualizes 
high-quality online dialogue as civil, rational, constructive, equal, 
interactive, and for the common good. More recently, advances in 
computation have driven an upsurge of empirical studies using automated 
methods for operationalizing online dialogue and measuring its quality. 
While related in their aims, deliberative theory and the wider empirical
 literature generally operate independently. To bridge the gap between 
the two literatures, we introduce Textual Indicators of Deliberative 
Dialogue (TIDDs). TIDDs are defined as text-based measures of online 
dialogue quality under a deliberative model (e.g., disagreement, 
incivility, and justifications). In this study, we identified 123 TIDDs 
by systematically reviewing 67 empirical studies of online dialogue. We 
found them to have mid-low reliability, low criterion validity, and high
 construct validity for measuring two deliberative dimensions (civility 
and rationality). Our results highlight the limitations of deliberative 
theory for conceptualizing the variety of ways online dialogues can be 
operationalized. We report the most promising TIDDs for measuring the 
quality of online dialogue and suggest deliberative theory would benefit
 from altering its models in line with the broader empirical literature.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2023-12-01</td>
					</tr>
					<tr>
					<th>Language</th>
						<td>en</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>Textual Indicators of Deliberative Dialogue</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>SAGE Journals</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://doi.org/10.1177/08944393231156629">https://doi.org/10.1177/08944393231156629</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>12/24/2024, 7:35:26 PM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>Publisher: SAGE Publications Inc</td>
					</tr>
					<tr>
					<th>Volume</th>
						<td>41</td>
					</tr>
					<tr>
					<th>Pages</th>
						<td>2364-2385</td>
					</tr>
					<tr>
					<th>Publication</th>
						<td>Social Science Computer Review</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.1177/08944393231156629">10.1177/08944393231156629</a></td>
					</tr>
					<tr>
					<th>Issue</th>
						<td>6</td>
					</tr>
					<tr>
					<th>ISSN</th>
						<td>0894-4393</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>12/24/2024, 7:35:26 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_5GM9VGPD">SAGE PDF Full Text					</li>
				</ul>
			</li>


			<li id="item_M9JIBKGR" class="item preprint">
			<h2>The Augmented Qualitative Researcher: Using Generative AI in Qualitative Text Analysis</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Preprint</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Elida Izani Ibrahim</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Andrea Voyer</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Large language models (LLMs) such as ChatGPT are fueling 
academic discussions about generative artificial intelligence's role in 
research. In this article, we view LLMs as technological tools to 
support qualitative research and we aim to contribute to the 
long-standing history of articles that provide qualitative researchers 
with recommendations that can help them incorporate technological 
developments in their research practice while maintaining the 
interpretive, hands-on, and human-led approach that characterizes 
qualitative research. We present two approaches to using ChatGPT and 
integrating it into the qualitative research pipeline. We outline four 
main benefits of incorporating generative AI in qualitative work: 1) 
streamlined and expedited research; 2) enhanced transparency in 
researchers' ideas and clearer concept and category development; 3) 
identification of additional categories and themes beyond researchers' 
findings; and 4) increased reliability as generative AI support or 
challenge researchers' coding and analysis. We offer a practical 
framework for iterative prompt construction and evaluation for thematic 
and index coding, along with considerations for further analytical 
queries. These guidelines assist qualitative sociologists in conducting 
comprehensive, systematic, transparent, and reproducible research using 
generative AI. We also recognize generative AI’s limitations and 
underscore the indispensable role of human researchers throughout the 
process.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2024-01-24</td>
					</tr>
					<tr>
					<th>Language</th>
						<td>en-us</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>The Augmented Qualitative Researcher</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>OSF Preprints</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://osf.io/gkc8w">https://osf.io/gkc8w</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>1/3/2025, 4:21:10 PM</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.31235/osf.io/gkc8w">10.31235/osf.io/gkc8w</a></td>
					</tr>
					<tr>
					<th>Repository</th>
						<td>OSF</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>1/3/2025, 4:21:10 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>generative AI</li>
					<li>interpretive coding</li>
					<li>large language models</li>
					<li>qualitative methods</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_4MW8L7W9">OSF Preprint					</li>
				</ul>
			</li>


			<li id="item_6TITF9W2" class="item journalArticle">
			<h2>The Augmented Social Scientist: Using Sequential Transfer Learning to Annotate Millions of Texts with Human-Level Accuracy</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Salomé Do</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Étienne Ollion</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Rubing Shen</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>The last decade witnessed a spectacular rise in the volume of 
available textual data. With this new abundance came the question of how
 to analyze it. In the social sciences, scholars mostly resorted to two 
well-established approaches, human annotation on sampled data on the one
 hand (either performed by the researcher, or outsourced to 
microworkers), and quantitative methods on the other. Each approach has 
its own merits - a potentially very fine-grained analysis for the 
former, a very scalable one for the latter - but the combination of 
these two properties has not yielded highly accurate results so far. 
Leveraging recent advances in sequential transfer learning, we 
demonstrate via an experiment that an expert can train a precise, 
efficient automatic classifier in a very limited amount of time. We also
 show that, under certain conditions, expert-trained models produce 
better annotations than humans themselves. We demonstrate these points 
using a classic research question in the sociology of journalism, the 
rise of a “horse race” coverage of politics. We conclude that recent 
advances in transfer learning help us augment ourselves when analyzing 
unstructured data.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2022-12-04</td>
					</tr>
					<tr>
					<th>Language</th>
						<td>en</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>The Augmented Social Scientist</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>SAGE Journals</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://doi.org/10.1177/00491241221134526">https://doi.org/10.1177/00491241221134526</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>5/5/2024, 11:05:40 PM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>Publisher: SAGE Publications Inc</td>
					</tr>
					<tr>
					<th>Pages</th>
						<td>00491241221134526</td>
					</tr>
					<tr>
					<th>Publication</th>
						<td>Sociological Methods &amp; Research</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.1177/00491241221134526">10.1177/00491241221134526</a></td>
					</tr>
					<tr>
					<th>ISSN</th>
						<td>0049-1241</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>5/5/2024, 11:05:40 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>5/5/2024, 11:05:40 PM</td>
					</tr>
				</tbody></table>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_LB2Q8R2W">SAGE PDF Full Text					</li>
				</ul>
			</li>


			<li id="item_RDN3LFZ9" class="item journalArticle">
			<h2>The Augmented Social Scientist: Using Sequential Transfer Learning to Annotate Millions of Texts with Human-Level Accuracy</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Salomé Do</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Étienne Ollion</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Rubing Shen</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>The last decade witnessed a spectacular rise in the volume of 
available textual data. With this new abundance came the question of how
 to analyze it. In the social sciences, scholars mostly resorted to two 
well-established approaches, human annotation on sampled data on the one
 hand (either performed by the researcher, or outsourced to 
microworkers), and quantitative methods on the other. Each approach has 
its own merits - a potentially very fine-grained analysis for the 
former, a very scalable one for the latter - but the combination of 
these two properties has not yielded highly accurate results so far. 
Leveraging recent advances in sequential transfer learning, we 
demonstrate via an experiment that an expert can train a precise, 
efficient automatic classifier in a very limited amount of time. We also
 show that, under certain conditions, expert-trained models produce 
better annotations than humans themselves. We demonstrate these points 
using a classic research question in the sociology of journalism, the 
rise of a “horse race” coverage of politics. We conclude that recent 
advances in transfer learning help us augment ourselves when analyzing 
unstructured data.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>08/2024</td>
					</tr>
					<tr>
					<th>Language</th>
						<td>en</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>The Augmented Social Scientist</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>DOI.org (Crossref)</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://journals.sagepub.com/doi/10.1177/00491241221134526">https://journals.sagepub.com/doi/10.1177/00491241221134526</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>1/6/2025, 12:13:24 PM</td>
					</tr>
					<tr>
					<th>Volume</th>
						<td>53</td>
					</tr>
					<tr>
					<th>Pages</th>
						<td>1167-1200</td>
					</tr>
					<tr>
					<th>Publication</th>
						<td>Sociological Methods &amp; Research</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.1177/00491241221134526">10.1177/00491241221134526</a></td>
					</tr>
					<tr>
					<th>Issue</th>
						<td>3</td>
					</tr>
					<tr>
					<th>Journal Abbr</th>
						<td>Sociological Methods &amp; Research</td>
					</tr>
					<tr>
					<th>ISSN</th>
						<td>0049-1241, 1552-8294</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>1/6/2025, 12:13:24 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_CW56N9KI">Do et al. - 2024 - The Augmented Social Scientist Using Sequential T.pdf					</li>
				</ul>
			</li>


			<li id="item_KSBAZYPF" class="item preprint">
			<h2>The Bot Delusion. Large language models and anticipated consequences for academics’ publication and citation behavior</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Preprint</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Oliver Wieczorek</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Isabel Steinhardt</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Christian Schneijderberg</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Rebecca Schmidt</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Sylvi Mauermeister</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>The reproduction of social inequalities through artificial 
intelligence and large language models (LLMs) has been demonstrated 
empirically in various areas of society, for example in policing and 
personnel hiring decisions. Yet, a broader discussion is missing to what
 extent LLMs may affect the scientific enterprise, reinforce or mitigate
 existing structural inequalities, and introduce a “bot delusion” in 
academia. Focusing on publications and citations behavior, we devise a 
thought experiment regarding the impact of LLMs. These differentiate 
between the reproduction of preexisting structurally conditioned 
inequalities in science (socio-cognitive stasis), or to a catharsis, 
that may counteract structural inequalities and Matthew Effects. We 
develop three scenarios of the consequences of using LLMs for citations:
 The LLM anticipated consequences are reproducing content and status quo
 (scenario 1), enabling content coherence evaluation (scenario 2) and 
content evaluation (scenario 3). In face of the fast-paced evolution of 
LLMs, to attribute meaning to anticipated consequences on citations from
 a sociological perspective, we discuss the normative significance of 
LLM-use for selecting citations. Considered as ideal types, Merton’s 
CUDOS norms of communalism, universalism, disinterestedness, and 
organized skepticism capture the catharsis opportunity offered by LLM, 
and stasis is reflected in the Mitroff’s SPIOD counter-norms of secrecy,
 particularism, self-interestedness and organized dogmatism. As SPIOD 
only captures individual counter-norms, we introduce communal 
counter-norms to capture academics’ loyal citation behavior. The latter 
insinuates a status quo future of science (scenario 1), while the 
mixed-access (scenario 2) and open science (scenario 3) futures suggest a
 more cognitively and less socially structured scientific endeavor.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2023-04-29</td>
					</tr>
					<tr>
					<th>Language</th>
						<td>en-us</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>OSF Preprints</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://osf.io/493qg">https://osf.io/493qg</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>1/3/2025, 3:53:33 PM</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.31235/osf.io/493qg">10.31235/osf.io/493qg</a></td>
					</tr>
					<tr>
					<th>Repository</th>
						<td>OSF</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>1/3/2025, 3:53:33 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>(Un-)Anticipated Consequences</li>
					<li>Academic Publishing</li>
					<li>Artificial Neural Networks</li>
					<li>Scientific Norms</li>
					<li>Thought Experiment</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_C7C2SC5S">OSF Preprint					</li>
				</ul>
			</li>


			<li id="item_D3ELDQER" class="item preprint">
			<h2>The Context Window Fallacy in Large Language Models</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Preprint</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Inês Hipólito</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>The integration of Large Language Models (LLMs) into various 
applications as quasi-search engines raises significant concerns about 
their impact on misinformation and societal decision-making processes. 
LLMs are designed to generate text that mimics human speech, often 
detached from factual reality, which poses risks when used unchecked. 
Developers and corporations advancing LLM technology argue for enhanced 
effectiveness through increased context window size and computational 
power. However, this paper challenges this assertion, arguing that 
augmenting the context window primarily improves the LLMs' ability to 
generate human-like narratives rather than enhancing their capacity for 
real-world decision-making. The paper advocates for a paradigm shift 
where LLMs move beyond merely sounding human to effectively adjudicating
 real-world challenges, emphasizing the need for ethical and practical 
advancements in AI development to mitigate the risks associated with 
misinformation and naive use of LLMs in critical decision-making 
processes. Finally, the paper proposes alternative approaches and 
criteria to address identified limitations; including grounded language 
models, causal reasoning integration, ethical frameworks, hybrid 
systems, and interactive learning.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2024-09-27</td>
					</tr>
					<tr>
					<th>Language</th>
						<td>en-us</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>OSF Preprints</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://osf.io/yv8he">https://osf.io/yv8he</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>1/6/2025, 11:47:11 AM</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.31235/osf.io/yv8he">10.31235/osf.io/yv8he</a></td>
					</tr>
					<tr>
					<th>Repository</th>
						<td>OSF</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>1/6/2025, 11:47:11 AM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_3XVQLSKT">OSF Preprint					</li>
				</ul>
			</li>


			<li id="item_EBW96BFD" class="item journalArticle">
			<h2>The Convert as a Social Type</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>David A. Snow</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Richard Machalek</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>This essay treats the convert as as social type with four 
specifiable formal properties: biographical reconstruction; adoption of a
 master attribution scheme; suspension of analogical reasoning; and 
embracement of the convert role. These properties are derived from the 
talk and reasoning of converts to a culturally transplanted Buddhist 
movement and from accounts of other proselytizers and converts. We 
conclude that it is the convert's rhetoric rather than institutional 
context or ideological content that denotes the convert as a social 
type.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>1983</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>JSTOR</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://www.jstor.org/stable/202053">https://www.jstor.org/stable/202053</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>4/30/2024, 10:50:29 AM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>Publisher: [American Sociological Association, Wiley, Sage Publications, Inc.]</td>
					</tr>
					<tr>
					<th>Volume</th>
						<td>1</td>
					</tr>
					<tr>
					<th>Pages</th>
						<td>259-289</td>
					</tr>
					<tr>
					<th>Publication</th>
						<td>Sociological Theory</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.2307/202053">10.2307/202053</a></td>
					</tr>
					<tr>
					<th>ISSN</th>
						<td>0735-2751</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>4/30/2024, 10:50:29 AM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>4/30/2024, 10:50:29 AM</td>
					</tr>
				</tbody></table>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_E96NLFEW">JSTOR Full Text PDF					</li>
				</ul>
			</li>


			<li id="item_4LUI4DAA" class="item journalArticle">
			<h2>The data revolution in social science needs qualitative research</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Nikolitsa Grigoropoulou</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Mario L. Small</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Although large-scale data are increasingly used to study human
 behaviour, researchers now recognize their limits for producing sound 
social science. Qualitative research can prevent some of these problems.
 Such methods can help to understand data quality, inform design and 
analysis decisions and guide interpretation of results.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2022-07</td>
					</tr>
					<tr>
					<th>Language</th>
						<td>en</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>www-nature-com.silk.library.umass.edu</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://www.nature.com/articles/s41562-022-01333-7">https://www.nature.com/articles/s41562-022-01333-7</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>12/23/2024, 3:58:11 PM</td>
					</tr>
					<tr>
					<th>Rights</th>
						<td>2022 Springer Nature Limited</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>Publisher: Nature Publishing Group</td>
					</tr>
					<tr>
					<th>Volume</th>
						<td>6</td>
					</tr>
					<tr>
					<th>Pages</th>
						<td>904-906</td>
					</tr>
					<tr>
					<th>Publication</th>
						<td>Nature Human Behaviour</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.1038/s41562-022-01333-7">10.1038/s41562-022-01333-7</a></td>
					</tr>
					<tr>
					<th>Issue</th>
						<td>7</td>
					</tr>
					<tr>
					<th>Journal Abbr</th>
						<td>Nat Hum Behav</td>
					</tr>
					<tr>
					<th>ISSN</th>
						<td>2397-3374</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>12/23/2024, 3:58:11 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Science</li>
					<li>Social sciences</li>
					<li>technology and society</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_4LRU6ASP">Full Text PDF					</li>
				</ul>
			</li>


			<li id="item_ESZ5C6B3" class="item journalArticle">
			<h2>The Ethico-Political Universe of ChatGPT</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>John Levi Martin</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>&lt;p&gt;There have been widespread concerns about two aspects
 of the current explosion of predictive text models and other 
algorithm-based computational tools. On one hand, it is often insisted 
that Artificial Intelligence (AI) should be made “ethical”, and software
 providers take this seriously, attempting to make sure that their tools
 are not used to facilitate grossly criminal or widely condemned 
activities. On the other hand, it is also widely understood that those 
who create these tools have a responsibility to ensure that they are 
“unbiased”, as opposed to simply helping one side in political 
contestation define their perspectives as reality for all. 
Unfortunately, these two goals cannot be jointly satisfied, as there are
 perhaps no ethical prescriptions worthy of notice that are not 
contested by some. Here I investigate the current ethico-political 
sensibility of ChatGPT, demonstrating that the very attempt to give it 
an ethical keel has also given it a measurably left position in the 
political space and a concomitant position in social space among the 
privileged.&lt;/p&gt;</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2023/03/01</td>
					</tr>
					<tr>
					<th>Language</th>
						<td>en</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>www.sciopen.com</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://www.sciopen.com/article/10.23919/JSC.2023.0003">https://www.sciopen.com/article/10.23919/JSC.2023.0003</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>12/24/2024, 4:23:35 PM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>Publisher: 清华大学出版社</td>
					</tr>
					<tr>
					<th>Volume</th>
						<td>4</td>
					</tr>
					<tr>
					<th>Pages</th>
						<td>1-11</td>
					</tr>
					<tr>
					<th>Publication</th>
						<td>Journal of Social Computing</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.23919/JSC.2023.0003">10.23919/JSC.2023.0003</a></td>
					</tr>
					<tr>
					<th>Issue</th>
						<td>1</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>12/24/2024, 4:23:35 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_GKD7F24C">Full Text PDF					</li>
				</ul>
			</li>


			<li id="item_HLTW238M" class="item journalArticle">
			<h2>The Extended Computational Case Method: A Framework for Research Design</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Juan Pablo Pardo-Guerra</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Prithviraj Pahwa</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>This paper considers the adoption of computational techniques 
within research designs modeled after the extended case method. Echoing 
calls to augment the power of contemporary researchers through the 
adoption of computational text analysis methods, we offer a framework 
for thinking about how such techniques can be integrated into 
quasi-ethnographic workflows to address broad, structural sociological 
claims. We focus, in particular, on how this adoption of novel forms of 
evidence impacts corpus design and interpretation (which we tie to 
matters of casing), theoretical elaboration (which we associate to 
moving empirical claims across scales and empirical domains), and 
verification (which we see as a process of reflexive scaffolding of 
theoretical claims). We provide an example of the use of this framework 
through a study of the marketization of social scientific knowledge in 
the United Kingdom.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2022-11-01</td>
					</tr>
					<tr>
					<th>Language</th>
						<td>en</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>The Extended Computational Case Method</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>SAGE Journals</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://doi.org/10.1177/00491241221122616">https://doi.org/10.1177/00491241221122616</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>10/8/2024, 2:31:33 PM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>Publisher: SAGE Publications Inc</td>
					</tr>
					<tr>
					<th>Volume</th>
						<td>51</td>
					</tr>
					<tr>
					<th>Pages</th>
						<td>1826-1867</td>
					</tr>
					<tr>
					<th>Publication</th>
						<td>Sociological Methods &amp; Research</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.1177/00491241221122616">10.1177/00491241221122616</a></td>
					</tr>
					<tr>
					<th>Issue</th>
						<td>4</td>
					</tr>
					<tr>
					<th>ISSN</th>
						<td>0049-1241</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>10/8/2024, 2:31:33 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>10/8/2024, 2:31:33 PM</td>
					</tr>
				</tbody></table>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_5SYI8559">SAGE PDF Full Text					</li>
				</ul>
			</li>


			<li id="item_EEYLM4KJ" class="item webpage">
			<h2>The Future of Coding: A Comparison of Hand-Coding and Three Types of Computer-Assisted Text Analysis Methods</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Web Page</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Abstract
    References
    Biographies
    Supplementary Material

Abstract
Advances in computer science and computational linguistics have yielded 
new, and faster, computational approaches to structuring and analyzing 
textual data. These approaches perform well on tasks like information 
extraction, but their ability to identify complex, socially constructed,
 and unsettled theoretical concepts—a central goal of sociological 
content analysis—has not been tested. To fill this gap, we compare the 
results produced by three common computer-assisted 
approaches—dictionary, supervised machine learning (SML), and 
unsupervised machine learning—to those produced through a rigorous 
hand-coding analysis of inequality in the news (N = 1,253 articles). 
Although we find that SML methods perform best in replicating hand-coded
 results, we document and clarify the strengths and weaknesses of each 
approach, including how they can complement one another. We argue that 
content analysts in the social sciences would do well to keep all these 
approaches in their toolkit, deploying them purposefully according to 
the task at hand.</td>
					</tr>
					<tr>
					<th>Language</th>
						<td>en</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>The Future of Coding</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://journals.sagepub.com/doi/epub/10.1177/0049124118769114">https://journals.sagepub.com/doi/epub/10.1177/0049124118769114</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>5/2/2024, 7:23:16 PM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>DOI: 10.1177/0049124118769114</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>5/2/2024, 7:23:16 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 8:13:26 PM</td>
					</tr>
				</tbody></table>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_MZQ6ZMDW">Snapshot					</li>
					<li id="item_EURSDU7E">The Future of Coding A Comparison of Hand-Coding .pdf					</li>
				</ul>
			</li>


			<li id="item_Y5S2BXD8" class="item journalArticle">
			<h2>The Geometry of Culture: Analyzing the Meanings of Class through Word Embeddings</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Austin C. Kozlowski</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Matt Taddy</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>James A. Evans</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>We argue word embedding models are a useful tool for the study
 of culture using a historical analysis of shared understandings of 
social class as an empirical case. Word embeddings represent semantic 
relations between words as relationships between vectors in a 
high-dimensional space, specifying a relational model of meaning 
consistent with contemporary theories of culture. Dimensions induced by 
word differences (rich – poor) in these spaces correspond to dimensions 
of cultural meaning, and the projection of words onto these dimensions 
reflects widely shared associations, which we validate with surveys. 
Analyzing text from millions of books published over 100 years, we show 
that the markers of class continuously shifted amidst the economic 
transformations of the twentieth century, yet the basic cultural 
dimensions of class remained remarkably stable. The notable exception is
 education, which became tightly linked to affluence independent of its 
association with cultivated taste.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2019-10-01</td>
					</tr>
					<tr>
					<th>Language</th>
						<td>en</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>The Geometry of Culture</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>SAGE Journals</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://doi.org/10.1177/0003122419877135">https://doi.org/10.1177/0003122419877135</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>1/8/2025, 1:35:42 PM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>Publisher: SAGE Publications Inc</td>
					</tr>
					<tr>
					<th>Volume</th>
						<td>84</td>
					</tr>
					<tr>
					<th>Pages</th>
						<td>905-949</td>
					</tr>
					<tr>
					<th>Publication</th>
						<td>American Sociological Review</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.1177/0003122419877135">10.1177/0003122419877135</a></td>
					</tr>
					<tr>
					<th>Issue</th>
						<td>5</td>
					</tr>
					<tr>
					<th>Journal Abbr</th>
						<td>Am Sociol Rev</td>
					</tr>
					<tr>
					<th>ISSN</th>
						<td>0003-1224</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>1/8/2025, 1:35:42 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_98GJ3ERE">Submitted Version					</li>
				</ul>
			</li>


			<li id="item_QS7Q28HV" class="item journalArticle">
			<h2>The great Transformer: Examining the role of large language models in the political economy of AI</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Dieuwertje Luitse</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Wiebke Denkena</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>In recent years, AI research has become more and more 
computationally demanding. In natural language processing (NLP), this 
tendency is reflected in the emergence of large language models (LLMs) 
like GPT-3. These powerful neural network-based models can be used for a
 range of NLP tasks and their language generation capacities have become
 so sophisticated that it can be very difficult to distinguish their 
outputs from human language. LLMs have raised concerns over their 
demonstrable biases, heavy environmental footprints, and future social 
ramifications. In December 2020, critical research on LLMs led Google to
 fire Timnit Gebru, co-lead of the company’s AI Ethics team, which 
sparked a major public controversy around LLMs and the growing corporate
 influence over AI research. This article explores the role LLMs play in
 the political economy of AI as infrastructural components for AI 
research and development. Retracing the technical developments that have
 led to the emergence of LLMs, we point out how they are intertwined 
with the business model of big tech companies and further shift power 
relations in their favour. This becomes visible through the Transformer,
 which is the underlying architecture of most LLMs today and started the
 race for ever bigger models when it was introduced by Google in 2017. 
Using the example of GPT-3, we shed light on recent corporate efforts to
 commodify LLMs through paid API access and exclusive licensing, raising
 questions around monopolization and dependency in a field that is 
increasingly divided by access to large-scale computing power.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2021-07-01</td>
					</tr>
					<tr>
					<th>Language</th>
						<td>en</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>The great Transformer</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>SAGE Journals</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://doi.org/10.1177/20539517211047734">https://doi.org/10.1177/20539517211047734</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>12/23/2024, 6:05:08 PM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>Publisher: SAGE Publications Ltd</td>
					</tr>
					<tr>
					<th>Volume</th>
						<td>8</td>
					</tr>
					<tr>
					<th>Pages</th>
						<td>20539517211047734</td>
					</tr>
					<tr>
					<th>Publication</th>
						<td>Big Data &amp; Society</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.1177/20539517211047734">10.1177/20539517211047734</a></td>
					</tr>
					<tr>
					<th>Issue</th>
						<td>2</td>
					</tr>
					<tr>
					<th>ISSN</th>
						<td>2053-9517</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>12/23/2024, 6:05:08 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_L372CW7A">SAGE PDF Full Text					</li>
				</ul>
			</li>


			<li id="item_6SZIPCZ4" class="item journalArticle">
			<h2>The growing amplification of social media: measuring temporal and
 social contagion dynamics for over 150 languages on Twitter for 
2009–2020</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Thayer Alshaabi</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>David Rushing Dewhurst</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Joshua R. Minot</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Michael V. Arnold</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Jane L. Adams</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Christopher M. Danforth</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Peter Sheridan Dodds</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Working from a dataset of 118 billion messages running from 
the start of 2009 to the end of 2019, we identify and explore the 
relative daily use of over 150 languages on Twitter. We find that eight 
languages comprise 80% of all tweets, with English, Japanese, Spanish, 
Arabic, and Portuguese being the most dominant. To quantify social 
spreading in each language over time, we compute the ‘contagion ratio’: 
The balance of retweets to organic messages. We find that for the most 
common languages on Twitter there is a growing tendency, though not 
universal, to retweet rather than share new content. By the end of 2019,
 the contagion ratios for half of the top 30 languages, including 
English and Spanish, had reached above 1—the naive contagion threshold. 
In 2019, the top 5 languages with the highest average daily ratios were,
 in order, Thai (7.3), Hindi, Tamil, Urdu, and Catalan, while the bottom
 5 were Russian, Swedish, Esperanto, Cebuano, and Finnish (0.26). 
Further, we show that over time, the contagion ratios for most common 
languages are growing more strongly than those of rare languages.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2021/12</td>
					</tr>
					<tr>
					<th>Language</th>
						<td>en</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>The growing amplification of social media</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>epjdatascience.springeropen.com</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://epjdatascience.springeropen.com/articles/10.1140/epjds/s13688-021-00271-0">https://epjdatascience.springeropen.com/articles/10.1140/epjds/s13688-021-00271-0</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>1/3/2025, 12:43:54 PM</td>
					</tr>
					<tr>
					<th>Rights</th>
						<td>2021 The Author(s)</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>Number: 1
Publisher: SpringerOpen</td>
					</tr>
					<tr>
					<th>Volume</th>
						<td>10</td>
					</tr>
					<tr>
					<th>Pages</th>
						<td>1-28</td>
					</tr>
					<tr>
					<th>Publication</th>
						<td>EPJ Data Science</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.1140/epjds/s13688-021-00271-0">10.1140/epjds/s13688-021-00271-0</a></td>
					</tr>
					<tr>
					<th>Issue</th>
						<td>1</td>
					</tr>
					<tr>
					<th>Journal Abbr</th>
						<td>EPJ Data Sci.</td>
					</tr>
					<tr>
					<th>ISSN</th>
						<td>2193-1127</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>1/3/2025, 12:43:54 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_RD8YKW6S">Full Text PDF					</li>
				</ul>
			</li>


			<li id="item_FSEFNPVJ" class="item journalArticle">
			<h2>The moral embeddedness of cryptomarkets: text mining feedback on economic exchanges on the dark web</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Ana Macanovic</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Wojtek Przepiorka</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Reputation systems promote cooperation in large-scale online 
markets for illegal goods. These so-called cryptomarkets operate on the 
Dark Web, where legal, social, and moral trust-building mechanisms are 
difficult to establish. However, for the reputation mechanism to be 
effective in promoting cooperation, traders have to leave feedback after
 completed transactions in the form of ratings and short texts. Here we 
investigate the motivational landscape of the reputation systems of 
three large cryptomarkets. We employ manual and automatic text mining 
methods to code 2 million feedback texts for a range of motives for 
leaving feedback. We find that next to self-regarding motives and 
reciprocity, moral norms (i.e. unconditional considerations for others’ 
outcomes) drive traders’ voluntary supply of information to reputation 
systems. Our results show how psychological mechanisms interact with 
organizational features of markets to provide a collective good that 
promotes mutually beneficial economic exchange.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2023-12-26</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>The moral embeddedness of cryptomarkets</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>Silverchair</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://doi.org/10.1093/ser/mwad069">https://doi.org/10.1093/ser/mwad069</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>5/3/2024, 5:48:01 PM</td>
					</tr>
					<tr>
					<th>Pages</th>
						<td>mwad069</td>
					</tr>
					<tr>
					<th>Publication</th>
						<td>Socio-Economic Review</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.1093/ser/mwad069">10.1093/ser/mwad069</a></td>
					</tr>
					<tr>
					<th>Journal Abbr</th>
						<td>Socio-Economic Review</td>
					</tr>
					<tr>
					<th>ISSN</th>
						<td>1475-1461</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>5/3/2024, 5:48:01 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>5/3/2024, 5:48:01 PM</td>
					</tr>
				</tbody></table>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_U37DP3RL">Full Text PDF					</li>
					<li id="item_HPEVKYMU">Macanovic 2023 Supplementary Material.docx					</li>
					<li id="item_JCAX53M7">Snapshot					</li>
				</ul>
			</li>


			<li id="item_H35EYUSF" class="item journalArticle">
			<h2>The More Competent, the Better? The Effects of Perceived 
Competencies on Disclosure Towards Conversational Artificial 
Intelligence</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Miriam Gieselmann</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Kai Sassenberg</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Conversational AI (e.g., Google Assistant or Amazon Alexa) is 
present in many people’s everyday life and, at the same time, becomes 
more and more capable of solving more complex tasks. However, it is 
unclear how the growing capabilities of conversational AI affect 
people’s disclosure towards the system as previous research has revealed
 mixed effects of technology competence. To address this research 
question, we propose a framework systematically disentangling 
conversational AI competencies along the lines of the dimensions of 
human competencies suggested by the action regulation theory. Across two
 correlational studies and three experiments (Ntotal = 1453), we 
investigated how these competencies differentially affect users’ and 
non-users’ disclosure towards conversational AI. Results indicate that 
intellectual competencies (e.g., planning actions and anticipating 
problems) in a conversational AI heighten users’ willingness to disclose
 and reduce their privacy concerns. In contrast, meta-cognitive 
heuristics (e.g., deriving universal strategies based on previous 
interactions) raise privacy concerns for users and, even more so, for 
non-users but reduce willingness to disclose only for non-users. Thus, 
the present research suggests that not all competencies of a 
conversational AI are seen as merely positive, and the proposed 
differentiation of competencies is informative to explain effects on 
disclosure.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2023-12-01</td>
					</tr>
					<tr>
					<th>Language</th>
						<td>en</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>The More Competent, the Better?</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>SAGE Journals</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://doi.org/10.1177/08944393221142787">https://doi.org/10.1177/08944393221142787</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>12/24/2024, 7:34:59 PM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>Publisher: SAGE Publications Inc</td>
					</tr>
					<tr>
					<th>Volume</th>
						<td>41</td>
					</tr>
					<tr>
					<th>Pages</th>
						<td>2342-2363</td>
					</tr>
					<tr>
					<th>Publication</th>
						<td>Social Science Computer Review</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.1177/08944393221142787">10.1177/08944393221142787</a></td>
					</tr>
					<tr>
					<th>Issue</th>
						<td>6</td>
					</tr>
					<tr>
					<th>ISSN</th>
						<td>0894-4393</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>12/24/2024, 7:34:59 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_LKDWKPVN">SAGE PDF Full Text					</li>
				</ul>
			</li>


			<li id="item_UBT6S8UG" class="item preprint">
			<h2>The Parrot Dilemma: Human-Labeled vs. LLM-augmented Data in Classification Tasks</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Preprint</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Anders Giovanni Møller</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Jacob Aarup Dalsgaard</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Arianna Pera</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Luca Maria Aiello</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>In the realm of Computational Social Science (CSS), 
practitioners often navigate complex, low-resource domains and face the 
costly and time-intensive challenges of acquiring and annotating data. 
We aim to establish a set of guidelines to address such challenges, 
comparing the use of human-labeled data with synthetically generated 
data from GPT-4 and Llama-2 in ten distinct CSS classification tasks of 
varying complexity. Additionally, we examine the impact of training data
 sizes on performance. Our findings reveal that models trained on 
human-labeled data consistently exhibit superior or comparable 
performance compared to their synthetically augmented counterparts. 
Nevertheless, synthetic augmentation proves beneficial, particularly in 
improving performance on rare classes within multi-class tasks. 
Furthermore, we leverage GPT-4 and Llama-2 for zero-shot classification 
and find that, while they generally display strong performance, they 
often fall short when compared to specialized classifiers trained on 
moderately sized training sets.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2024-02-05</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>The Parrot Dilemma</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>arXiv.org</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://arxiv.org/abs/2304.13861">http://arxiv.org/abs/2304.13861</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>1/8/2025, 3:18:42 PM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>arXiv:2304.13861 [cs]</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.48550/arXiv.2304.13861">10.48550/arXiv.2304.13861</a></td>
					</tr>
					<tr>
					<th>Repository</th>
						<td>arXiv</td>
					</tr>
					<tr>
					<th>Archive ID</th>
						<td>arXiv:2304.13861</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>1/8/2025, 3:18:42 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Computer Science - Computation and Language</li>
					<li>Computer Science - Computers and Society</li>
					<li>Physics - Physics and Society</li>
				</ul>
				<h3 class="notes">Notes:</h3>
				<ul class="notes">
					<li id="item_JKBNYQ72">
<p class="plaintext">Comment: Accepted at EACL 2024. 14 pages, 4 figures, 2 tables</p>
					</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_G4LZI543">Preprint PDF					</li>
					<li id="item_R85Q83RM">Snapshot					</li>
				</ul>
			</li>


			<li id="item_VJS3GI5A" class="item preprint">
			<h2>The Persuasive Power of Large Language Models</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Preprint</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Simon Martin Breum</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Daniel Vædele Egdal</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Victor Gram Mortensen</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Anders Giovanni Møller</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Luca Maria Aiello</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>The increasing capability of Large Language Models to act as 
human-like social agents raises two important questions in the area of 
opinion dynamics. First, whether these agents can generate effective 
arguments that could be injected into the online discourse to steer the 
public opinion. Second, whether artificial agents can interact with each
 other to reproduce dynamics of persuasion typical of human social 
systems, opening up opportunities for studying synthetic social systems 
as faithful proxies for opinion dynamics in human populations. To 
address these questions, we designed a synthetic persuasion dialogue 
scenario on the topic of climate change, where a 'convincer' agent 
generates a persuasive argument for a 'skeptic' agent, who subsequently 
assesses whether the argument changed its internal opinion state. 
Different types of arguments were generated to incorporate different 
linguistic dimensions underpinning psycho-linguistic theories of opinion
 change. We then asked human judges to evaluate the persuasiveness of 
machine-generated arguments. Arguments that included factual knowledge, 
markers of trust, expressions of support, and conveyed status were 
deemed most effective according to both humans and agents, with humans 
reporting a marked preference for knowledge-based arguments. Our 
experimental framework lays the groundwork for future in-silico studies 
of opinion dynamics, and our findings suggest that artificial agents 
have the potential of playing an important role in collective processes 
of opinion formation in online social media.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2023-12-24</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>arXiv.org</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://arxiv.org/abs/2312.15523">http://arxiv.org/abs/2312.15523</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>1/8/2025, 2:55:46 PM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>arXiv:2312.15523 [cs]</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.48550/arXiv.2312.15523">10.48550/arXiv.2312.15523</a></td>
					</tr>
					<tr>
					<th>Repository</th>
						<td>arXiv</td>
					</tr>
					<tr>
					<th>Archive ID</th>
						<td>arXiv:2312.15523</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>1/8/2025, 2:55:46 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Computer Science - Computation and Language</li>
					<li>Computer Science - Computers and Society</li>
					<li>Computer Science - Human-Computer Interaction</li>
					<li>Physics - Physics and Society</li>
				</ul>
				<h3 class="notes">Notes:</h3>
				<ul class="notes">
					<li id="item_3WS2E4DZ">
<p class="plaintext">Comment: 9 pages, 6 figures, 3 tables, 1 page in appendix</p>
					</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_B6VPFDWY">Preprint PDF					</li>
					<li id="item_NWTVTKBT">Snapshot					</li>
				</ul>
			</li>


			<li id="item_2AH9VUHH" class="item journalArticle">
			<h2>The Search for Solid Ground in Text as Data: A Systematic Review 
of Validation Practices and Practical Recommendations for Validation</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Lukas Birkenmaier</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Clemens M. Lechner</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Claudia Wagner</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Communication research frequently applies computational text 
analysis methods (CTAM) to detect and measure social science constructs.
 However, the validity of these measures can be difficult to assess. In 
addition, there are hardly any established standards and little guidance
 for researchers on how to best validate CTAM. But how do these 
challenges affect current validation practices in applied research? And 
what practical recommendations for better validation of text-based 
measures can we derive? To answer these questions, we conducted a 
systematic review of current validation practices and qualitative expert
 interviews. We focused on political communication, a subfield that has 
arguably played a pioneering role in embracing the application of CTAM 
in communication research. Our results show that researchers apply a 
great variety of validation steps, which, however, are rarely selected 
based on a unified understanding of validity. The qualitative interviews
 further reinforce this notion, as interviewees bemoan a lack of 
established guidelines and frameworks for validation. Based on our 
empirical findings, we then derive practical recommendations to guide 
researchers on how to approach validation conceptually. Additionally, we
 provide insight into ongoing research that focuses on applied 
validation frameworks, designed to provide hands-on guidance to 
researchers involved in CTAM validation.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2024-07-02</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>The Search for Solid Ground in Text as Data</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>Taylor and Francis+NEJM</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://doi.org/10.1080/19312458.2023.2285765">https://doi.org/10.1080/19312458.2023.2285765</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>1/6/2025, 1:58:37 PM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>Publisher: Routledge
_eprint: https://doi.org/10.1080/19312458.2023.2285765</td>
					</tr>
					<tr>
					<th>Volume</th>
						<td>18</td>
					</tr>
					<tr>
					<th>Pages</th>
						<td>249-277</td>
					</tr>
					<tr>
					<th>Publication</th>
						<td>Communication Methods and Measures</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.1080/19312458.2023.2285765">10.1080/19312458.2023.2285765</a></td>
					</tr>
					<tr>
					<th>Issue</th>
						<td>3</td>
					</tr>
					<tr>
					<th>ISSN</th>
						<td>1931-2458</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>1/6/2025, 1:58:37 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="notes">Notes:</h3>
				<ul class="notes">
					<li id="item_L7MVJCJ5">
<div><div data-schema-version="8"><p>discusses lack of common standards/dis-unification in computational methods (in communications research)</p>
<p></p>
</div></div>
					</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_Q2INQ6SF">Full Text PDF					</li>
				</ul>
			</li>


			<li id="item_J2AW88SS" class="item journalArticle">
			<h2>The Sociological Power of Methodological Rhetoric</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Jack Katz</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Taking a sociological view, we can investigate the empirical 
consequences of variations in the rhetoric of sociological methodology. 
The standards advocated in Qualitative Literacy divide communities of 
qualitative researchers, as they are not explicitly connected to an 
understanding of social ontology, unlike previous qualitative 
methodologies; they continue the long-growing segregation of the 
rhetorical worlds of qualitative and quantitative research methodology; 
and they draw attention to the personal competencies of the researcher. I
 compare a rhetoric of qualitative methodology that: derives evaluation 
criteria from perspectives on social ontology that have been developing 
progressively since the early twentieth century; applies the 
discipline-wide evaluation criteria of reactivity, reliability, 
representativeness, and replicability; and asks evaluators to focus on 
the adequacy of the textual depiction of research subjects.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>05/2023</td>
					</tr>
					<tr>
					<th>Language</th>
						<td>en</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>DOI.org (Crossref)</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://journals.sagepub.com/doi/10.1177/00491241221140427">https://journals.sagepub.com/doi/10.1177/00491241221140427</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>1/6/2025, 12:28:28 PM</td>
					</tr>
					<tr>
					<th>Volume</th>
						<td>52</td>
					</tr>
					<tr>
					<th>Pages</th>
						<td>1086-1102</td>
					</tr>
					<tr>
					<th>Publication</th>
						<td>Sociological Methods &amp; Research</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.1177/00491241221140427">10.1177/00491241221140427</a></td>
					</tr>
					<tr>
					<th>Issue</th>
						<td>2</td>
					</tr>
					<tr>
					<th>Journal Abbr</th>
						<td>Sociological Methods &amp; Research</td>
					</tr>
					<tr>
					<th>ISSN</th>
						<td>0049-1241, 1552-8294</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>1/6/2025, 12:28:28 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_VPR9DLBN">Katz - 2023 - The Sociological Power of Methodological Rhetoric.pdf					</li>
				</ul>
			</li>


			<li id="item_MSYMVVYX" class="item journalArticle">
			<h2>The Thick Machine: Anthropological AI between explanation and explication</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Anders Kristian Munk</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Asger Gehrt Olesen</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Mathieu Jacomy</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>According to Clifford Geertz, the purpose of anthropology is 
not to explain culture but to explicate it. That should cause us to 
rethink our relationship with machine learning. It is, we contend, 
perfectly possible that machine learning algorithms, which are unable to
 explain, and could even be unexplainable themselves, can still be of 
critical use in a process of explication. Thus, we report on an 
experiment with anthropological AI. From a dataset of 175K Facebook 
comments, we trained a neural network to predict the emoji reaction 
associated with a comment and asked a group of human players to compete 
against the machine. We show that a) the machine can reach the same 
(poor) accuracy as the players (51%), b) it fails in roughly the same 
ways as the players, and c) easily predictable emoji reactions tend to 
reflect unambiguous situations where interpretation is easy. We 
therefore repurpose the failures of the neural network to point us to 
deeper and more ambiguous situations where interpretation is hard and 
explication becomes both necessary and interesting. We use this 
experiment as a point of departure for discussing how experiences from 
anthropology, and in particular the tension between formalist 
ethnoscience and interpretive thick description, might contribute to 
debates about explainable AI.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2022-01-01</td>
					</tr>
					<tr>
					<th>Language</th>
						<td>en</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>The Thick Machine</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>SAGE Journals</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://doi.org/10.1177/20539517211069891">https://doi.org/10.1177/20539517211069891</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>12/23/2024, 4:10:12 PM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>Publisher: SAGE Publications Ltd</td>
					</tr>
					<tr>
					<th>Volume</th>
						<td>9</td>
					</tr>
					<tr>
					<th>Pages</th>
						<td>20539517211069891</td>
					</tr>
					<tr>
					<th>Publication</th>
						<td>Big Data &amp; Society</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.1177/20539517211069891">10.1177/20539517211069891</a></td>
					</tr>
					<tr>
					<th>Issue</th>
						<td>1</td>
					</tr>
					<tr>
					<th>ISSN</th>
						<td>2053-9517</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>12/23/2024, 4:10:12 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_6JTBCPFH">SAGE PDF Full Text					</li>
				</ul>
			</li>


			<li id="item_ICVKN4RS" class="item journalArticle">
			<h2>Theoretical Foundations and Limits of Word Embeddings: What Types of Meaning can They Capture?</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Alina Arseniev-Koehler</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Measuring meaning is a central problem in cultural sociology 
and word embeddings may offer powerful new tools to do so. But like any 
tool, they build on and exert theoretical assumptions. In this paper, I 
theorize the ways in which word embeddings model three core premises of a
 structural linguistic theory of meaning: that meaning is coherent, 
relational, and may be analyzed as a static system. In certain ways, 
word embeddings are vulnerable to the enduring critiques of these 
premises. In other ways, word embeddings offer novel solutions to these 
critiques. More broadly, formalizing the study of meaning with word 
embeddings offers theoretical opportunities to clarify core concepts and
 debates in cultural sociology, such as the coherence of meaning. Just 
as network analysis speciﬁed the once vague notion of social relations, 
formalizing meaning with embeddings can push us to specify and reimagine
 meaning itself.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>11/2024</td>
					</tr>
					<tr>
					<th>Language</th>
						<td>en</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>Theoretical Foundations and Limits of Word Embeddings</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>DOI.org (Crossref)</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://journals.sagepub.com/doi/10.1177/00491241221140142">https://journals.sagepub.com/doi/10.1177/00491241221140142</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>1/6/2025, 12:11:43 PM</td>
					</tr>
					<tr>
					<th>Volume</th>
						<td>53</td>
					</tr>
					<tr>
					<th>Pages</th>
						<td>1753-1793</td>
					</tr>
					<tr>
					<th>Publication</th>
						<td>Sociological Methods &amp; Research</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.1177/00491241221140142">10.1177/00491241221140142</a></td>
					</tr>
					<tr>
					<th>Issue</th>
						<td>4</td>
					</tr>
					<tr>
					<th>Journal Abbr</th>
						<td>Sociological Methods &amp; Research</td>
					</tr>
					<tr>
					<th>ISSN</th>
						<td>0049-1241, 1552-8294</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>1/6/2025, 12:11:43 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_Q7R5IKQP">Arseniev-Koehler - 2024 - Theoretical Foundations and Limits of Word Embeddi.pdf					</li>
				</ul>
			</li>


			<li id="item_WCRN6SNZ" class="item journalArticle">
			<h2>Theory In, Theory Out: The Uses of Social Theory in Machine Learning for Social Science</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Jason Radford</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Kenneth Joseph</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>&lt;p&gt;Research at the intersection of machine learning and 
the social sciences has provided critical new insights into social 
behavior. At the same time, a variety of issues have been identified 
with the machine learning models used to analyze social data. These 
issues range from technical problems with the data used and features 
constructed, to problematic modeling assumptions, to limited 
interpretability, to the models' contributions to bias and inequality. 
Computational researchers have sought out technical solutions to these 
problems. The primary contribution of the present work is to argue that 
there is a limit to these technical solutions. At this limit, we must 
instead turn to social theory. We show how social theory can be used to 
answer basic methodological and interpretive questions that technical 
solutions cannot when building machine learning models, and when 
assessing, comparing, and using those models. In both cases, we draw on 
related existing critiques, provide examples of how social theory has 
already been used constructively in existing work, and discuss where 
other existing work may have benefited from the use of specific social 
theories. We believe this paper can act as a guide for computer and 
social scientists alike to navigate the substantive questions involved 
in applying the tools of machine learning to social data.&lt;/p&gt;</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2020-05-19</td>
					</tr>
					<tr>
					<th>Language</th>
						<td>English</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>Theory In, Theory Out</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>Frontiers</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://www.frontiersin.org/journals/big-data/articles/10.3389/fdata.2020.00018/full">https://www.frontiersin.org/journals/big-data/articles/10.3389/fdata.2020.00018/full</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>1/3/2025, 12:27:59 PM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>Publisher: Frontiers</td>
					</tr>
					<tr>
					<th>Volume</th>
						<td>3</td>
					</tr>
					<tr>
					<th>Publication</th>
						<td>Frontiers in Big Data</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.3389/fdata.2020.00018">10.3389/fdata.2020.00018</a></td>
					</tr>
					<tr>
					<th>Journal Abbr</th>
						<td>Front. Big Data</td>
					</tr>
					<tr>
					<th>ISSN</th>
						<td>2624-909X</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>1/3/2025, 12:27:59 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Bias</li>
					<li>computational social science</li>
					<li>fairness</li>
					<li>machine learning</li>
					<li>machine learning and social science</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_JKDJHWZM">Full Text PDF					</li>
				</ul>
			</li>


			<li id="item_CW74HD4W" class="item journalArticle">
			<h2>Three families of automated text analysis</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Austin van Loon</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Since the beginning of this millennium, data in the form of 
human-generated text in a machine-readable format has become 
increasingly available to social scientists, presenting a unique window 
into social life. However, harnessing vast quantities of this highly 
unstructured data in a systematic way presents a unique combination of 
analytical and methodological challenges. Luckily, our understanding of 
how to overcome these challenges has also developed greatly over this 
same period. In this article, I present a novel typology of the methods 
social scientists have used to analyze text data at scale in the 
interest of testing and developing social theory. I describe three 
“families” of methods: analyses of (1) term frequency, (2) document 
structure, and (3) semantic similarity. For each family of methods, I 
discuss their logical and statistical foundations, analytical strengths 
and weaknesses, as well as prominent variants and applications.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2022-11-01</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>ScienceDirect</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://www.sciencedirect.com/science/article/pii/S0049089X22001090">https://www.sciencedirect.com/science/article/pii/S0049089X22001090</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>1/7/2025, 4:10:48 PM</td>
					</tr>
					<tr>
					<th>Volume</th>
						<td>108</td>
					</tr>
					<tr>
					<th>Pages</th>
						<td>102798</td>
					</tr>
					<tr>
					<th>Publication</th>
						<td>Social Science Research</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.1016/j.ssresearch.2022.102798">10.1016/j.ssresearch.2022.102798</a></td>
					</tr>
					<tr>
					<th>Journal Abbr</th>
						<td>Social Science Research</td>
					</tr>
					<tr>
					<th>ISSN</th>
						<td>0049-089X</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>1/7/2025, 4:10:48 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Automated text analysis</li>
					<li>Social science</li>
					<li>Text analysis</li>
					<li>Text as data</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_GQ2FQ82R">ScienceDirect Snapshot					</li>
					<li id="item_IMC7JUZC">Submitted Version					</li>
				</ul>
			</li>


			<li id="item_MAEDZQ6R" class="item journalArticle">
			<h2>Three Gaps in Computational Text Analysis Methods for Social Sciences: A Research Agenda</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Christian Baden</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Christian Pipal</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Martijn Schoonvelde</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Mariken A. C. G van der Velden</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>We identify three gaps that limit the utility and obstruct the
 progress of computational text analysis methods (CTAM) for social 
science research. First, we contend that CTAM development has 
prioritized technological over validity concerns, giving limited 
attention to the operationalization of social scientific measurements. 
Second, we identify a mismatch between CTAMs’ focus on extracting 
specific contents and document-level patterns, and social science 
researchers’ need for measuring multiple, often complex contents in the 
text. Third, we argue that the dominance of English language tools 
depresses comparative research and inclusivity toward scholarly 
communities examining languages other than English. We substantiate our 
claims by drawing upon a broad review of methodological work in the 
computational social sciences, as well as an inventory of leading 
research publications using quantitative textual analysis. Subsequently,
 we discuss implications of these three gaps for social scientists’ 
uneven uptake of CTAM, as well as the field of computational social 
science text research as a whole. Finally, we propose a research agenda 
intended to bridge the identified gaps and improve the validity, 
utility, and inclusiveness of CTAM.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2022-01-02</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>Three Gaps in Computational Text Analysis Methods for Social Sciences</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>Taylor and Francis+NEJM</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://doi.org/10.1080/19312458.2021.2015574">https://doi.org/10.1080/19312458.2021.2015574</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>3/15/2024, 8:05:27 AM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>Publisher: Routledge
_eprint: https://doi.org/10.1080/19312458.2021.2015574</td>
					</tr>
					<tr>
					<th>Volume</th>
						<td>16</td>
					</tr>
					<tr>
					<th>Pages</th>
						<td>1-18</td>
					</tr>
					<tr>
					<th>Publication</th>
						<td>Communication Methods and Measures</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.1080/19312458.2021.2015574">10.1080/19312458.2021.2015574</a></td>
					</tr>
					<tr>
					<th>Issue</th>
						<td>1</td>
					</tr>
					<tr>
					<th>ISSN</th>
						<td>1931-2458</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>3/15/2024, 8:05:27 AM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>3/15/2024, 8:05:27 AM</td>
					</tr>
				</tbody></table>
				<h3 class="notes">Notes:</h3>
				<ul class="notes">
					<li id="item_2G4JGUNP">
<div><div data-citation-items="%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F3957624%2Fitems%2FMAEDZQ6R%22%5D%2C%22itemData%22%3A%7B%22id%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F3957624%2Fitems%2FMAEDZQ6R%22%2C%22type%22%3A%22article-journal%22%2C%22abstract%22%3A%22We%20identify%20three%20gaps%20that%20limit%20the%20utility%20and%20obstruct%20the%20progress%20of%20computational%20text%20analysis%20methods%20(CTAM)%20for%20social%20science%20research.%20First%2C%20we%20contend%20that%20CTAM%20development%20has%20prioritized%20technological%20over%20validity%20concerns%2C%20giving%20limited%20attention%20to%20the%20operationalization%20of%20social%20scientific%20measurements.%20Second%2C%20we%20identify%20a%20mismatch%20between%20CTAMs%E2%80%99%20focus%20on%20extracting%20specific%20contents%20and%20document-level%20patterns%2C%20and%20social%20science%20researchers%E2%80%99%20need%20for%20measuring%20multiple%2C%20often%20complex%20contents%20in%20the%20text.%20Third%2C%20we%20argue%20that%20the%20dominance%20of%20English%20language%20tools%20depresses%20comparative%20research%20and%20inclusivity%20toward%20scholarly%20communities%20examining%20languages%20other%20than%20English.%20We%20substantiate%20our%20claims%20by%20drawing%20upon%20a%20broad%20review%20of%20methodological%20work%20in%20the%20computational%20social%20sciences%2C%20as%20well%20as%20an%20inventory%20of%20leading%20research%20publications%20using%20quantitative%20textual%20analysis.%20Subsequently%2C%20we%20discuss%20implications%20of%20these%20three%20gaps%20for%20social%20scientists%E2%80%99%20uneven%20uptake%20of%20CTAM%2C%20as%20well%20as%20the%20field%20of%20computational%20social%20science%20text%20research%20as%20a%20whole.%20Finally%2C%20we%20propose%20a%20research%20agenda%20intended%20to%20bridge%20the%20identified%20gaps%20and%20improve%20the%20validity%2C%20utility%2C%20and%20inclusiveness%20of%20CTAM.%22%2C%22container-title%22%3A%22Communication%20Methods%20and%20Measures%22%2C%22DOI%22%3A%2210.1080%2F19312458.2021.2015574%22%2C%22ISSN%22%3A%221931-2458%22%2C%22issue%22%3A%221%22%2C%22note%22%3A%22publisher%3A%20Routledge%5Cn_eprint%3A%20https%3A%2F%2Fdoi.org%2F10.1080%2F19312458.2021.2015574%22%2C%22page%22%3A%221-18%22%2C%22source%22%3A%22Taylor%20and%20Francis%2BNEJM%22%2C%22title%22%3A%22Three%20Gaps%20in%20Computational%20Text%20Analysis%20Methods%20for%20Social%20Sciences%3A%20A%20Research%20Agenda%22%2C%22title-short%22%3A%22Three%20Gaps%20in%20Computational%20Text%20Analysis%20Methods%20for%20Social%20Sciences%22%2C%22URL%22%3A%22https%3A%2F%2Fdoi.org%2F10.1080%2F19312458.2021.2015574%22%2C%22volume%22%3A%2216%22%2C%22author%22%3A%5B%7B%22family%22%3A%22Baden%22%2C%22given%22%3A%22Christian%22%7D%2C%7B%22family%22%3A%22Pipal%22%2C%22given%22%3A%22Christian%22%7D%2C%7B%22family%22%3A%22Schoonvelde%22%2C%22given%22%3A%22Martijn%22%7D%2C%7B%22family%22%3A%22Velden%22%2C%22given%22%3A%22Mariken%20A.%20C.%20G%22%2C%22non-dropping-particle%22%3A%22van%20der%22%7D%5D%2C%22accessed%22%3A%7B%22date-parts%22%3A%5B%5B%222024%22%2C3%2C15%5D%5D%7D%2C%22issued%22%3A%7B%22date-parts%22%3A%5B%5B%222022%22%2C1%2C2%5D%5D%7D%7D%7D%5D" data-schema-version="8"><p>Note</p>
<p><span class="highlight" data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F3957624%2Fitems%2FSUP4ESN4%22%2C%22pageLabel%22%3A%221%22%2C%22position%22%3A%7B%22pageIndex%22%3A1%2C%22rects%22%3A%5B%5B122.798%2C104.761%2C456.019%2C114.761%5D%2C%5B48.021%2C92.742%2C129.859%2C102.742%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F3957624%2Fitems%2FMAEDZQ6R%22%5D%2C%22locator%22%3A%221%22%7D%7D">“high-powered algorithms, such as Neural Network classifiers (e.g., Choi et al., 2021) – remain a rare sight”</span> <span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F3957624%2Fitems%2FMAEDZQ6R%22%5D%2C%22locator%22%3A%221%22%2C%22itemData%22%3A%7B%22id%22%3A133%2C%22type%22%3A%22article-journal%22%2C%22abstract%22%3A%22We%20identify%20three%20gaps%20that%20limit%20the%20utility%20and%20obstruct%20the%20progress%20of%20computational%20text%20analysis%20methods%20(CTAM)%20for%20social%20science%20research.%20First%2C%20we%20contend%20that%20CTAM%20development%20has%20prioritized%20technological%20over%20validity%20concerns%2C%20giving%20limited%20attention%20to%20the%20operationalization%20of%20social%20scientific%20measurements.%20Second%2C%20we%20identify%20a%20mismatch%20between%20CTAMs%E2%80%99%20focus%20on%20extracting%20specific%20contents%20and%20document-level%20patterns%2C%20and%20social%20science%20researchers%E2%80%99%20need%20for%20measuring%20multiple%2C%20often%20complex%20contents%20in%20the%20text.%20Third%2C%20we%20argue%20that%20the%20dominance%20of%20English%20language%20tools%20depresses%20comparative%20research%20and%20inclusivity%20toward%20scholarly%20communities%20examining%20languages%20other%20than%20English.%20We%20substantiate%20our%20claims%20by%20drawing%20upon%20a%20broad%20review%20of%20methodological%20work%20in%20the%20computational%20social%20sciences%2C%20as%20well%20as%20an%20inventory%20of%20leading%20research%20publications%20using%20quantitative%20textual%20analysis.%20Subsequently%2C%20we%20discuss%20implications%20of%20these%20three%20gaps%20for%20social%20scientists%E2%80%99%20uneven%20uptake%20of%20CTAM%2C%20as%20well%20as%20the%20field%20of%20computational%20social%20science%20text%20research%20as%20a%20whole.%20Finally%2C%20we%20propose%20a%20research%20agenda%20intended%20to%20bridge%20the%20identified%20gaps%20and%20improve%20the%20validity%2C%20utility%2C%20and%20inclusiveness%20of%20CTAM.%22%2C%22container-title%22%3A%22Communication%20Methods%20and%20Measures%22%2C%22DOI%22%3A%2210.1080%2F19312458.2021.2015574%22%2C%22ISSN%22%3A%221931-2458%22%2C%22issue%22%3A%221%22%2C%22note%22%3A%22publisher%3A%20Routledge%5Cn_eprint%3A%20https%3A%2F%2Fdoi.org%2F10.1080%2F19312458.2021.2015574%22%2C%22page%22%3A%221-18%22%2C%22source%22%3A%22Taylor%20and%20Francis%2BNEJM%22%2C%22title%22%3A%22Three%20Gaps%20in%20Computational%20Text%20Analysis%20Methods%20for%20Social%20Sciences%3A%20A%20Research%20Agenda%22%2C%22title-short%22%3A%22Three%20Gaps%20in%20Computational%20Text%20Analysis%20Methods%20for%20Social%20Sciences%22%2C%22volume%22%3A%2216%22%2C%22author%22%3A%5B%7B%22family%22%3A%22Baden%22%2C%22given%22%3A%22Christian%22%7D%2C%7B%22family%22%3A%22Pipal%22%2C%22given%22%3A%22Christian%22%7D%2C%7B%22family%22%3A%22Schoonvelde%22%2C%22given%22%3A%22Martijn%22%7D%2C%7B%22family%22%3A%22Velden%22%2C%22given%22%3A%22Mariken%20A.%20C.%20G%22%2C%22non-dropping-particle%22%3A%22van%20der%22%7D%5D%2C%22issued%22%3A%7B%22date-parts%22%3A%5B%5B%222022%22%2C1%2C2%5D%5D%7D%7D%7D%5D%2C%22properties%22%3A%7B%7D%7D">(<span class="citation-item">Baden et al., 2022, p. 1</span>)</span></p>
</div></div>
					</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_SUP4ESN4">Full Text PDF					</li>
				</ul>
			</li>


			<li id="item_62DQHFSC" class="item journalArticle">
			<h2>Toward a sociology of machine learning explainability: Human–machine interaction in deep neural  network-based automated trading</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Christian Borch</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Bo Hee Min</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Machine learning systems are making considerable inroads in 
society owing to their ability to recognize and predict patterns. 
However, the decision-making logic of some widely used machine learning 
models, such as deep neural networks, is characterized by opacity, 
thereby rendering them exceedingly difficult for humans to understand 
and explain and, as a result, potentially risky to use. Considering the 
importance of addressing this opacity, this paper calls for research 
that studies empirically and theoretically how machine learning experts 
and users seek to attain machine learning explainability. Focusing on 
automated trading, we take steps in this direction by analyzing a 
trading firm’s quest for explaining its deep neural network system’s 
actionable predictions. We demonstrate that this explainability effort 
involves a particular form of human–machine interaction that contains 
both anthropomorphic and technomorphic elements. We discuss this attempt
 to attain machine learning explainability in light of reflections on 
cross-species companionship and consider it an example of human–machine 
companionship.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2022-07-01</td>
					</tr>
					<tr>
					<th>Language</th>
						<td>en</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>Toward a sociology of machine learning explainability</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>SAGE Journals</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://doi.org/10.1177/20539517221111361">https://doi.org/10.1177/20539517221111361</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>12/23/2024, 5:47:17 PM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>Publisher: SAGE Publications Ltd</td>
					</tr>
					<tr>
					<th>Volume</th>
						<td>9</td>
					</tr>
					<tr>
					<th>Pages</th>
						<td>20539517221111361</td>
					</tr>
					<tr>
					<th>Publication</th>
						<td>Big Data &amp; Society</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.1177/20539517221111361">10.1177/20539517221111361</a></td>
					</tr>
					<tr>
					<th>Issue</th>
						<td>2</td>
					</tr>
					<tr>
					<th>ISSN</th>
						<td>2053-9517</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>12/23/2024, 5:47:17 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_MCI3NKNV">SAGE PDF Full Text					</li>
				</ul>
			</li>


			<li id="item_X3W8RZFZ" class="item journalArticle">
			<h2>Toward a Stronger Theoretical Grounding of Computational Communication Science: How Macro Frameworks Shape Our Research Agendas</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Annie Waldherr</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Stephanie Geise</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Merja Mahrt</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Christian Katzenbach</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Christian Nuernbergk</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Abstract Computational communication science (CCS) is embraced
 by many as a fruitful methodological approach to studying communication
 in the digital era. However, theoretical advances have not been 
considered equally important in CCS. Specifically, we observe an 
emphasis on mid-range and micro theories that misses a larger discussion
 on how macro-theoretical frameworks can serve CCS scholarship. With 
this article, we aim to stimulate such a discussion. Although macro 
frameworks might not point directly to specific questions and 
hypotheses, they shape our research through influencing which kinds of 
questions we ask, which kinds of hypotheses we formulate, and which 
methods we find adequate and useful. We showcase how three selected 
theoretical frameworks might advance CCS scholarship in this way: (1) 
complexity theory, (2) theories of the public sphere, and (3) 
mediatization theory. Using online protest as an example, we discuss how
 the focus (and the blind spots) of our research designs shifts with 
each framework.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2021/10/01</td>
					</tr>
					<tr>
					<th>Language</th>
						<td>en</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>Toward a Stronger Theoretical Grounding of Computational Communication Science</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>www.aup-online.com</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://www.aup-online.com/content/journals/10.5117/CCR2021.02.002.WALD">https://www.aup-online.com/content/journals/10.5117/CCR2021.02.002.WALD</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>1/6/2025, 1:46:51 PM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>Publisher: Amsterdam University Press</td>
					</tr>
					<tr>
					<th>Volume</th>
						<td>3</td>
					</tr>
					<tr>
					<th>Pages</th>
						<td>1-28</td>
					</tr>
					<tr>
					<th>Publication</th>
						<td>Computational Communication Research</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.5117/CCR2021.02.002.WALD">10.5117/CCR2021.02.002.WALD</a></td>
					</tr>
					<tr>
					<th>Issue</th>
						<td>2</td>
					</tr>
					<tr>
					<th>ISSN</th>
						<td>2665-9085</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>1/6/2025, 1:46:51 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_L7K8S764">Snapshot					</li>
				</ul>
			</li>


			<li id="item_ASCYZLS9" class="item preprint">
			<h2>Towards "Differential AI Psychology" and in-context Value-driven Statement Alignment with Moral Foundations Theory</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Preprint</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Simon Münker</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Contemporary research in social sciences is increasingly 
utilizing state-of-the-art statistical language models to annotate or 
generate content. While these models perform benchmark-leading on common
 language tasks and show exemplary task-independent emergent abilities, 
transferring them to novel out-of-domain tasks is only insufficiently 
explored. The implications of the statistical black-box approach - 
stochastic parrots - are prominently criticized in the language model 
research community; however, the significance for novel generative tasks
 is not. This work investigates the alignment between personalized 
language models and survey participants on a Moral Foundation Theory 
questionnaire. We adapt text-to-text models to different political 
personas and survey the questionnaire repetitively to generate a 
synthetic population of persona and model combinations. Analyzing the 
intra-group variance and cross-alignment shows significant differences 
across models and personas. Our findings indicate that adapted models 
struggle to represent the survey-captured assessment of political 
ideologies. Thus, using language models to mimic social interactions 
requires measurable improvements in in-context optimization or parameter
 manipulation to align with psychological and sociological stereotypes. 
Without quantifiable alignment, generating politically nuanced content 
remains unfeasible. To enhance these representations, we propose a 
testable framework to generate agents based on moral value statements 
for future research.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2024-08-21</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>arXiv.org</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://arxiv.org/abs/2408.11415">http://arxiv.org/abs/2408.11415</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>1/7/2025, 4:33:16 PM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>arXiv:2408.11415 [cs]</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.48550/arXiv.2408.11415">10.48550/arXiv.2408.11415</a></td>
					</tr>
					<tr>
					<th>Repository</th>
						<td>arXiv</td>
					</tr>
					<tr>
					<th>Archive ID</th>
						<td>arXiv:2408.11415</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>1/7/2025, 4:33:16 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Computer Science - Artificial Intelligence</li>
					<li>Computer Science - Computation and Language</li>
				</ul>
				<h3 class="notes">Notes:</h3>
				<ul class="notes">
					<li id="item_RWFSMXWE">
<p class="plaintext">Comment: 8 pages, 6 tables</p>
					</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_ZC5JLUMC">Preprint PDF					</li>
					<li id="item_EDXKPRBA">Snapshot					</li>
				</ul>
			</li>


			<li id="item_MQU396GW" class="item preprint">
			<h2>Training TikTok creators in mental health communication benefits their audience, too: An analysis of TikTok user comments</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Preprint</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Yuning Liu</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Matthew Motta</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Amanda Yarnell</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Background: Social media platforms like TikTok are pivotal for
 mental health communication (MHC), providing accessible, trusted 
content. Despite known benefits of creator training programs in 
improving content quality, it's uncertain if or how these enhancements 
affect people who watch that content. This study examines how training 
programs for TikTok creators influence TikTok users’ mental health 
knowledge construction and intended behaviors by analyzing user comments
 on TikTok videos.

Method: We employed LLM-assisted content analysis to annotate TikTok 
video comments for mental health knowledge construction and behavioral 
intentions. We then analyzed the impact of a creator training program on
 users' mental health knowledge construction through within-subject 
field experiments. Additionally, we extended this method to measure how 
TikTok's Mental Health Awareness Month (MHAM) in May 2023 affected user 
mental health knowledge construction and behavioral intentions.

Results: LLM-assisted analysis showed that asynchronous toolkits 
significantly enhance mental health knowledge construction among video 
commenters, particularly among creators with high engagement, fewer 
followers, who are licensed mental health professionals, or  who do not 
offer paid coaching services. MHAM increased mental health knowledge 
construction among users, though the audiences of the 7 institutions and
 10 content creators that were spotlighted by TikTok during MHAM did not
 show this effect.

Conclusions: Asynchronous toolkits improve mental health knowledge 
construction among video viewers. Future training should focus on 
emerging creators and separate toolkit learning from conference training
 to enhance training efficacy. In addition, promoting hashtags such as 
#MentalHealthAwareness can drive meaningful discussions and knowledge 
construction in video comments, but there is an opportunity for 
platforms to refine their creator selection criteria to improve 
on-platform mental health knowledge construction.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2024-10-04</td>
					</tr>
					<tr>
					<th>Language</th>
						<td>en-us</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>Training TikTok creators in mental health communication benefits their audience, too</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>OSF Preprints</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://osf.io/p65nv">https://osf.io/p65nv</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>1/3/2025, 4:02:15 PM</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.31235/osf.io/p65nv">10.31235/osf.io/p65nv</a></td>
					</tr>
					<tr>
					<th>Repository</th>
						<td>OSF</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>1/3/2025, 4:02:15 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>health content creator programs</li>
					<li>online mental health communication</li>
					<li>TikTok</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_55G3L2JM">OSF Preprint					</li>
				</ul>
			</li>


			<li id="item_6E6HV4MI" class="item journalArticle">
			<h2>Understanding the paradigm shift to computational social science in the presence of big data</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Ray M. Chang</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Robert J. Kauffman</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>YoungOk Kwon</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>The era of big data has created new opportunities for 
researchers to achieve high relevance and impact amid changes and 
transformations in how we study social science phenomena. With the 
emergence of new data collection technologies, advanced data mining and 
analytics support, there seems to be fundamental changes that are 
occurring with the research questions we can ask, and the research 
methods we can apply. The contexts include social networks and blogs, 
political discourse, corporate announcements, digital journalism, mobile
 telephony, home entertainment, online gaming, financial services, 
online shopping, social advertising, and social commerce. The changing 
costs of data collection and the new capabilities that researchers have 
to conduct research that leverages micro-level, meso-level and 
macro-level data suggest the possibility of a scientific paradigm shift 
toward computational social science. The new thinking related to 
empirical regularities analysis, experimental design, and longitudinal 
empirical research further suggests that these approaches can be 
tailored for rapid acquisition of big data sets. This will allow 
business analysts and researchers to achieve frequent, controlled and 
meaningful observations of real-world phenomena. We discuss how our 
philosophy of science should be changing in step with the times, and 
illustrate our perspective with comparisons between earlier and current 
research inquiry. We argue against the assertion that theory no longer 
matters and offer some new research directions.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2014-07-01</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>ScienceDirect</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://www.sciencedirect.com/science/article/pii/S0167923613002212">https://www.sciencedirect.com/science/article/pii/S0167923613002212</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>10/8/2024, 11:25:53 AM</td>
					</tr>
					<tr>
					<th>Volume</th>
						<td>63</td>
					</tr>
					<tr>
					<th>Pages</th>
						<td>67-80</td>
					</tr>
					<tr>
					<th>Publication</th>
						<td>Decision Support Systems</td>
					</tr>
					<tr>
					<th>Series</th>
						<td>1. Business Applications of Web of Things 2. Social Media Use in Decision Making</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.1016/j.dss.2013.08.008">10.1016/j.dss.2013.08.008</a></td>
					</tr>
					<tr>
					<th>Journal Abbr</th>
						<td>Decision Support Systems</td>
					</tr>
					<tr>
					<th>ISSN</th>
						<td>0167-9236</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>10/8/2024, 11:25:53 AM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>10/8/2024, 11:25:53 AM</td>
					</tr>
				</tbody></table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Analytics</li>
					<li>Big data</li>
					<li>Computational social science</li>
					<li>Data analytics</li>
					<li>Interdisciplinary research</li>
					<li>Managerial decision-making</li>
					<li>Paradigm shift</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_TPTHWRPL">ScienceDirect Snapshot					</li>
				</ul>
			</li>


			<li id="item_2PRDM87A" class="item journalArticle">
			<h2>Understanding the Use, Strengths and Limitations of Automated Text Analysis</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Philip Adu</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Unstructured big data continues to grow both in scope and 
availability. However, manually analyzing huge text data is a daunting 
(if not impossible) task for social scientists. Conducting automated 
text analysis can be an appropriate and effective strategy for making 
sense of massive text data within a considerable amount of time and with
 less human labor. But qualitative researchers have limited knowledge of
 the role, strengths, and limitations of automated text analysis. In 
response, this article focuses on simplifying what makes a text analysis
 automatic, how automated text analysis emerged, when it is appropriate 
to use it including its pros and cons, and what automated text analysis 
tools are available and the type of automated text analysis task they 
can perform. The article concludes by emphasizing the need for every 
social science researcher to familiarize themselves with the text 
analysis software-gaining the skill in choosing and using the one that 
would help meet the goal of their studies and, hence, advancing the 
knowledge of their field.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2022-03-18</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>ResearchGate</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>5/5/2024, 2:00:37 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>5/5/2024, 2:00:37 PM</td>
					</tr>
				</tbody></table>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_88T2ABZR">Full Text PDF					</li>
					<li id="item_EFH98X9X">ResearchGate Link					</li>
				</ul>
			</li>


			<li id="item_BGHGLLRR" class="item preprint">
			<h2>United in Diversity? Contextual Biases in LLM-Based Predictions of the 2024 European Parliament Elections</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Preprint</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Leah von der Heyde</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Anna-Carolina Haensch</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Alexander Wenz</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Large language models (LLMs) are perceived by some as having 
the potential to revolutionize social science research, considering 
their training data includes information on human attitudes and 
behavior. If these attitudes are reflected in LLM output, LLM-generated 
"synthetic samples" could be used as a viable and efficient alternative 
to surveys of real humans. However, LLM-synthetic samples might exhibit 
coverage bias due to training data and fine-tuning processes being 
unrepresentative of diverse linguistic, social, political, and digital 
contexts. In this study, we examine to what extent LLM-based predictions
 of public opinion exhibit context-dependent biases by predicting voting
 behavior in the 2024 European Parliament elections using a 
state-of-the-art LLM. We prompt GPT-4-Turbo with anonymized 
individual-level background information, varying prompt content and 
language, ask the LLM to predict each person's voting behavior, and 
compare the weighted aggregates to the real election results. Our 
findings emphasize the limited applicability of LLM-synthetic samples to
 public opinion prediction. We show that (1) the LLM-based prediction of
 future voting behavior largely fails, (2) prediction accuracy is 
unequally distributed across national and linguistic contexts, and (3) 
improving LLM predictions requires detailed attitudinal information 
about individuals for prompting. In investigating the contextual 
differences of LLM-based predictions of public opinion, our research 
contributes to the understanding and mitigation of biases and 
inequalities in the development of LLMs and their applications in 
computational social science.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2024-08-29</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>United in Diversity?</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>arXiv.org</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://arxiv.org/abs/2409.09045">http://arxiv.org/abs/2409.09045</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>1/7/2025, 5:08:40 PM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>arXiv:2409.09045 [cs]</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.48550/arXiv.2409.09045">10.48550/arXiv.2409.09045</a></td>
					</tr>
					<tr>
					<th>Repository</th>
						<td>arXiv</td>
					</tr>
					<tr>
					<th>Archive ID</th>
						<td>arXiv:2409.09045</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>1/7/2025, 5:08:40 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Computer Science - Artificial Intelligence</li>
					<li>Computer Science - Computation and Language</li>
					<li>Computer Science - Computers and Society</li>
					<li>Statistics - Applications</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_DR774HUJ">Preprint PDF					</li>
					<li id="item_Z2ZQCD5M">Snapshot					</li>
				</ul>
			</li>


			<li id="item_4FZRIKNU" class="item preprint">
			<h2>Unveiling Gender Bias in Terms of Profession Across LLMs: Analyzing and Addressing Sociological Implications</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Preprint</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Vishesh Thakur</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Gender bias in artificial intelligence (AI) and natural 
language processing has garnered significant attention due to its 
potential impact on societal perceptions and biases. This research paper
 aims to analyze gender bias in Large Language Models (LLMs) with a 
focus on multiple comparisons between GPT-2 and GPT-3.5, some prominent 
language models, to better understand its implications. Through a 
comprehensive literature review, the study examines existing research on
 gender bias in AI language models and identifies gaps in the current 
knowledge. The methodology involves collecting and preprocessing data 
from GPT-2 and GPT-3.5, and employing in-depth quantitative analysis 
techniques to evaluate gender bias in the generated text. The findings 
shed light on gendered word associations, language usage, and biased 
narratives present in the outputs of these Large Language Models. The 
discussion explores the ethical implications of gender bias and its 
potential consequences on social perceptions and marginalized 
communities. Additionally, the paper presents strategies for reducing 
gender bias in LLMs, including algorithmic approaches and data 
augmentation techniques. The research highlights the importance of 
interdisciplinary collaborations and the role of sociological studies in
 mitigating gender bias in AI models. By addressing these issues, we can
 pave the way for more inclusive and unbiased AI systems that have a 
positive impact on society.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2023-08-31</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>Unveiling Gender Bias in Terms of Profession Across LLMs</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>arXiv.org</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://arxiv.org/abs/2307.09162">http://arxiv.org/abs/2307.09162</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>1/7/2025, 4:58:18 PM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>arXiv:2307.09162 [cs]</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.48550/arXiv.2307.09162">10.48550/arXiv.2307.09162</a></td>
					</tr>
					<tr>
					<th>Repository</th>
						<td>arXiv</td>
					</tr>
					<tr>
					<th>Archive ID</th>
						<td>arXiv:2307.09162</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>1/7/2025, 4:58:18 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Computer Science - Computation and Language</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_ZMREDVFU">Preprint PDF					</li>
					<li id="item_K9QJX6C8">Snapshot					</li>
				</ul>
			</li>


			<li id="item_BPQ7FV2B" class="item preprint">
			<h2>Updating “The Future of Coding”: Qualitative Coding with Generative Large Language Models</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Preprint</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Nga Than</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Leanne Fan</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Tina Law</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Laura K. Nelson</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Leslie McCall</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Over the past decade, social scientists have adapted 
computational methods for qualitative text analysis, with the hope that 
they can match the accuracy and reliability of hand coding. The 
emergence of GPT and open-source generative Large Language Models (LLMs)
 has transformed this process by shifting from programming to engaging 
with models using natural language, potentially mimicking the in-depth, 
inductive and/or iterative process of qualitative analysis. We test the 
ability of generative LLMs to replicate and augment traditional 
qualitative coding, experimenting with multiple prompt structures across
 four closed- and open-source generative LLMs and proposing a workflow 
for conducting qualitative coding with generative LLMs. We find that 
LLMs can perform nearly as well as prior supervised machine learning 
models in accurately matching hand-coding output. Moreover, using 
generative LLMs as a natural language interlocutor closely replicates 
traditional qualitative methods, indicating their potential to transform
 the qualitative research process, despite ongoing challenges.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2024-08-29</td>
					</tr>
					<tr>
					<th>Language</th>
						<td>en-us</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>Updating “The Future of Coding”</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>OSF Preprints</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://osf.io/wg82k">https://osf.io/wg82k</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>1/6/2025, 11:21:13 AM</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.31235/osf.io/wg82k">10.31235/osf.io/wg82k</a></td>
					</tr>
					<tr>
					<th>Repository</th>
						<td>OSF</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>1/6/2025, 11:21:13 AM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>computational methods</li>
					<li>computational text analysis</li>
					<li>epistemology</li>
					<li>generative ai</li>
					<li>gpt</li>
					<li>large language models</li>
					<li>llama</li>
					<li>qualitative coding</li>
					<li>qualitative methods</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_XN9ST2TW">OSF Preprint					</li>
				</ul>
			</li>


			<li id="item_9T274DLY" class="item preprint">
			<h2>Using AI in Grounded Theory research – a proposed framework for a ChatGPT-based research assistant</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Preprint</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Nieky van Veggel</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Hilary Engward</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Melanie Birks</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Professor Jane Mills</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>The purpose of this paper is to explore the potential 
application of ChatGPT in relation to grounded theory. Our focus is 
building a case as to its usefulness to support the research process as 
an assistant to the researcher, rather than to replace the intellectual 
rigour needed to conduct credible grounded theory research. To aid this,
 we present a framework for using ChatGPT to assist researchers’ 
decision making and analysis. By structuring the analytical process into
 clear research phases - from initial coding through to visualisation 
and expansion - and providing specific prompts and instructions for each
 phase, the framework enables researchers to systematically harness AI 
capabilities whilst maintaining the methodological rigour and 
accountability of the researcher in leading this process. We argue that 
the framework's strength lies in its careful alignment with established 
grounded theory processes, particularly in its emphasis on constant 
comparison throughout all analytical phases. As many grounded theory 
methods are employed in other qualitative research designs, we argue 
that the proposed framework may have potential for use in a broad range 
of designs, however, we also suggest that this is the start of new 
conversations about how researchers can harness AI to assist their 
decision making and intellectual work, processes which can never be 
fully replaced.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2024-12-18</td>
					</tr>
					<tr>
					<th>Language</th>
						<td>en-us</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>OSF Preprints</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://osf.io/a2dc4">https://osf.io/a2dc4</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>1/6/2025, 11:47:42 AM</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.31235/osf.io/a2dc4">10.31235/osf.io/a2dc4</a></td>
					</tr>
					<tr>
					<th>Repository</th>
						<td>OSF</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>1/6/2025, 11:47:42 AM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>AI</li>
					<li>ChatGPT</li>
					<li>Grounded Theory</li>
					<li>LLM</li>
					<li>Technological assistance</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_VRC2WR3K">OSF Preprint					</li>
				</ul>
			</li>


			<li id="item_HGAHDGG9" class="item preprint">
			<h2>Using Imperfect Surrogates for Downstream Inference: Design-based
 Supervised Learning for Social Science Applications of Large Language 
Models</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Preprint</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Naoki Egami</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Musashi Hinck</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Brandon M. Stewart</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Hanying Wei</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>In computational social science (CSS), researchers analyze 
documents to explain social and political phenomena. In most scenarios, 
CSS researchers first obtain labels for documents and then explain 
labels using interpretable regression analyses in the second step. One 
increasingly common way to annotate documents cheaply at scale is 
through large language models (LLMs). However, like other scalable ways 
of producing annotations, such surrogate labels are often imperfect and 
biased. We present a new algorithm for using imperfect annotation 
surrogates for downstream statistical analyses while guaranteeing 
statistical properties -- like asymptotic unbiasedness and proper 
uncertainty quantification -- which are fundamental to CSS research. We 
show that direct use of surrogate labels in downstream statistical 
analyses leads to substantial bias and invalid confidence intervals, 
even with high surrogate accuracy of 80-90%. To address this, we build 
on debiased machine learning to propose the design-based supervised 
learning (DSL) estimator. DSL employs a doubly-robust procedure to 
combine surrogate labels with a smaller number of high-quality, 
gold-standard labels. Our approach guarantees valid inference for 
downstream statistical analyses, even when surrogates are arbitrarily 
biased and without requiring stringent assumptions, by controlling the 
probability of sampling documents for gold-standard labeling. Both our 
theoretical analysis and experimental results show that DSL provides 
valid statistical inference while achieving root mean squared errors 
comparable to existing alternatives that focus only on prediction 
without inferential guarantees.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2024-01-14</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>Using Imperfect Surrogates for Downstream Inference</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>arXiv.org</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://arxiv.org/abs/2306.04746">http://arxiv.org/abs/2306.04746</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>1/7/2025, 5:38:37 PM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>arXiv:2306.04746 [stat]</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.48550/arXiv.2306.04746">10.48550/arXiv.2306.04746</a></td>
					</tr>
					<tr>
					<th>Repository</th>
						<td>arXiv</td>
					</tr>
					<tr>
					<th>Archive ID</th>
						<td>arXiv:2306.04746</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>1/7/2025, 5:38:37 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Computer Science - Computation and Language</li>
					<li>Computer Science - Machine Learning</li>
					<li>Statistics - Machine Learning</li>
					<li>Statistics - Methodology</li>
				</ul>
				<h3 class="notes">Notes:</h3>
				<ul class="notes">
					<li id="item_6MV958JJ">
<p class="plaintext">Comment: 37th Conference on Neural Information Processing Systems (NeurIPS 2023)</p>
					</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_AR6JW9ZK">Preprint PDF					</li>
					<li id="item_4NJKCJGL">Snapshot					</li>
				</ul>
			</li>


			<li id="item_2X4SPHMS" class="item preprint">
			<h2>Using Large Language Models for Qualitative Analysis can Introduce Serious Bias</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Preprint</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Julian Ashwin</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Aditya Chhabra</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Vijayendra Rao</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Large Language Models (LLMs) are quickly becoming ubiquitous, 
but the implications for social science research are not yet well 
understood. This paper asks whether LLMs can help us analyse large-N 
qualitative data from open-ended interviews, with an application to 
transcripts of interviews with Rohingya refugees in Cox’s Bazaar, 
Bangladesh. We find that a great deal of caution is needed in using LLMs
 to annotate text as there is a risk of introducing biases that can lead
 to misleading inferences. We here mean bias in the technical sense, 
that the errors that LLMs make in annotating interview transcripts are 
not random with respect to the characteristics of the interview 
subjects. Training simpler supervised models on high-quality human 
annotations with flexible coding leads to less measurement error and 
bias than LLM annotations. Therefore, given that some high quality 
annotations are necessary in order to asses whether an LLM introduces 
bias, we argue that it is probably preferable to train a bespoke model 
on these annotations than it is to use an LLM for annotation.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2023-10-05</td>
					</tr>
					<tr>
					<th>Language</th>
						<td>en</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>arXiv.org</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://arxiv.org/abs/2309.17147">http://arxiv.org/abs/2309.17147</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>5/2/2024, 8:06:39 PM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>arXiv:2309.17147 [cs, econ, q-fin]</td>
					</tr>
					<tr>
					<th>Repository</th>
						<td>arXiv</td>
					</tr>
					<tr>
					<th>Archive ID</th>
						<td>arXiv:2309.17147</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>5/2/2024, 8:06:39 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>5/2/2024, 8:06:40 PM</td>
					</tr>
				</tbody></table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Computer Science - Artificial Intelligence</li>
					<li>Computer Science - Computation and Language</li>
					<li>Economics - General Economics</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_IB7XDDRL">Ashwin et al. - 2023 - Using Large Language Models for Qualitative Analys.pdf					</li>
				</ul>
			</li>


			<li id="item_V67DJXL8" class="item preprint">
			<h2>Using Large Language Models to Create AI Personas for Replication
 and Prediction of Media Effects: An Empirical Test of 133 Published 
Experimental Research Findings</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Preprint</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Leo Yeykelis</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Kaavya Pichai</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>James J. Cummings</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Byron Reeves</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>This report analyzes the potential for large language models 
(LLMs) to expedite accurate replication of published message effects 
studies. We tested LLM-powered participants (personas) by replicating 
133 experimental findings from 14 papers containing 45 recent studies in
 the Journal of Marketing (January 2023-May 2024). We used a new 
software tool, Viewpoints AI (https://viewpoints.ai/), that takes study 
designs, stimuli, and measures as input, automatically generates prompts
 for LLMs to act as a specified sample of unique personas, and collects 
their responses to produce a final output in the form of a complete 
dataset and statistical analysis. The underlying LLM used was 
Anthropic's Claude Sonnet 3.5. We generated 19,447 AI personas to 
replicate these studies with the exact same sample attributes, study 
designs, stimuli, and measures reported in the original human research. 
Our LLM replications successfully reproduced 76% of the original main 
effects (84 out of 111), demonstrating strong potential for AI-assisted 
replication of studies in which people respond to media stimuli. When 
including interaction effects, the overall replication rate was 68% (90 
out of 133). The use of LLMs to replicate and accelerate marketing 
research on media effects is discussed with respect to the replication 
crisis in social science, potential solutions to generalizability 
problems in sampling subjects and experimental conditions, and the 
ability to rapidly test consumer responses to various media stimuli. We 
also address the limitations of this approach, particularly in 
replicating complex interaction effects in media response studies, and 
suggest areas for future research and improvement in AI-assisted 
experimental replication of media effects.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2024-08-28</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>Using Large Language Models to Create AI Personas for Replication and Prediction of Media Effects</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>arXiv.org</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://arxiv.org/abs/2408.16073">http://arxiv.org/abs/2408.16073</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>1/7/2025, 5:14:38 PM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>arXiv:2408.16073 [cs]</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.48550/arXiv.2408.16073">10.48550/arXiv.2408.16073</a></td>
					</tr>
					<tr>
					<th>Repository</th>
						<td>arXiv</td>
					</tr>
					<tr>
					<th>Archive ID</th>
						<td>arXiv:2408.16073</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>1/7/2025, 5:14:39 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Computer Science - Artificial Intelligence</li>
					<li>Computer Science - Computation and Language</li>
				</ul>
				<h3 class="notes">Notes:</h3>
				<ul class="notes">
					<li id="item_PQAGU7SV">
<p class="plaintext">Comment: 24 pages, 3 figures, 2 tables</p>
					</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_99YQN647">Preprint PDF					</li>
					<li id="item_JKFCLGXF">Snapshot					</li>
				</ul>
			</li>


			<li id="item_BGA4JT7L" class="item conferencePaper">
			<h2>Using Topic Modeling for Code Discovery in Large Scale Text Data</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Conference Paper</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Zhiqiang Cai</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Amanda Siebert-Evenstone</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Brendan Eagan</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>David Williamson Shaffer</td>
					</tr>
					<tr>
						<th class="editor">Editor</th>
						<td>Andrew R. Ruis</td>
					</tr>
					<tr>
						<th class="editor">Editor</th>
						<td>Seung B. Lee</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>When text datasets are very large, manually coding line by 
line becomes impractical. As a result, researchers sometimes try to use 
machine learning algorithms to automatically code text data. One of the 
most popular algorithms is topic modeling. For a given text dataset, a 
topic model provides probability distributions of words for a set of 
“topics” in the data, which researchers then use to interpret meaning of
 the topics. A topic model also gives each document in the dataset a 
score for each topic, which can be used as a non-binary coding for what 
proportion of a topic is in the document. Unfortunately, it is often 
difficult to interpret what the topics mean in a defensible way, or to 
validate document topic proportion scores as meaningful codes. In this 
study, we examine how keywords from codes developed by human experts 
were distributed in topics generated from topic modeling. The results 
show that (1) top keywords of a single topic often contain words from 
multiple human-generated codes; and conversely, (2) words from 
human-generated codes appear as high-probability keywords in multiple 
topic. These results explain why directly using topics from topic models
 as codes is problematic. However, they also imply that topic modeling 
makes it possible for researchers to discover codes from short word 
lists.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2021</td>
					</tr>
					<tr>
					<th>Language</th>
						<td>en</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>Springer Link</td>
					</tr>
					<tr>
					<th>Place</th>
						<td>Cham</td>
					</tr>
					<tr>
					<th>Publisher</th>
						<td>Springer International Publishing</td>
					</tr>
					<tr>
					<th>ISBN</th>
						<td>978-3-030-67788-6</td>
					</tr>
					<tr>
					<th>Pages</th>
						<td>18-31</td>
					</tr>
					<tr>
					<th>Proceedings Title</th>
						<td>Advances in Quantitative Ethnography</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.1007/978-3-030-67788-6_2">10.1007/978-3-030-67788-6_2</a></td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>4/30/2024, 11:59:56 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>4/30/2024, 11:59:56 PM</td>
					</tr>
				</tbody></table>
			</li>


			<li id="item_GQ9RZZTJ" class="item journalArticle">
			<h2>Using word embedding models to capture changing media discourses:
 a study on the role of legitimacy, gender and genre in 
24,000&nbsp;music reviews, 1999–2021</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Stijn Daenekindt</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Julian Schaap</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Studies suggest that popular music genres are increasingly 
discussed by cultural intermediaries in ‘legitimate’ or ‘highbrow’ 
terms, rather than merely ‘lowbrow’ commercial entertainment. In 
addition, popular music discourse as produced by such intermediaries has
 historically been decidedly masculine—a trait which tends to increase 
on par with legitimation. However, seeing that women are gradually 
gaining symbolic and numerical representation in popular music 
production, this may have been changing over the last decade(s). In this
 article, we assess how popular music discourse within a key music media
 outlet (Pitchfork) changed between 1999 and 2021. We use word embedding
 models—a novel technique in computational social science—to assess 
legitimacy and gender in the discourses used in 23,992 reviews, and how 
this varies between genres. We find four notable patterns. First, 
reviews increasingly use a discourse that legitimates popular music, 
while, second, also increasingly using more feminine terms. This does 
not, third, occur simultaneously; however, discourse is either 
legitimate or feminine. Finally, these patterns also differ based on 
which popular music genres are discussed. The overall pattern is 
consistently found in pop, electronic and experimental, but not in 
historically masculine genres rap/hip-hop, metal and jazz which seem 
rather resistant to discursive change.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2022-11-01</td>
					</tr>
					<tr>
					<th>Language</th>
						<td>en</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>Using word embedding models to capture changing media discourses</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>Springer Link</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://doi.org/10.1007/s42001-022-00182-8">https://doi.org/10.1007/s42001-022-00182-8</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>1/3/2025, 3:40:42 PM</td>
					</tr>
					<tr>
					<th>Volume</th>
						<td>5</td>
					</tr>
					<tr>
					<th>Pages</th>
						<td>1615-1636</td>
					</tr>
					<tr>
					<th>Publication</th>
						<td>Journal of Computational Social Science</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.1007/s42001-022-00182-8">10.1007/s42001-022-00182-8</a></td>
					</tr>
					<tr>
					<th>Issue</th>
						<td>2</td>
					</tr>
					<tr>
					<th>Journal Abbr</th>
						<td>J Comput Soc Sc</td>
					</tr>
					<tr>
					<th>ISSN</th>
						<td>2432-2725</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>1/3/2025, 3:40:42 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Concept Mover’s Distance</li>
					<li>Cultural intermediaries</li>
					<li>Gender</li>
					<li>Legitimation</li>
					<li>Popular music</li>
					<li>Reviews</li>
					<li>Word embedding models</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_N4M8JSZ4">Full Text PDF					</li>
				</ul>
			</li>


			<li id="item_GFCLWJ6Z" class="item conferencePaper">
			<h2>Using Word Embedding to Reveal Monetary Policy Explanation Changes</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Conference Paper</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Akira Matsui</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Xiang Ren</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Emilio Ferrara</td>
					</tr>
					<tr>
						<th class="editor">Editor</th>
						<td>Udo Hahn</td>
					</tr>
					<tr>
						<th class="editor">Editor</th>
						<td>Veronique Hoste</td>
					</tr>
					<tr>
						<th class="editor">Editor</th>
						<td>Amanda Stent</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Documents have been an essential tool of communication for 
governments to announce their policy operations. Most policy 
announcements have taken the form of text to inform their new policies 
or changes to the public. To understand such policymakers' 
communication, many researchers exploit published policy documents. 
However, the methods well-used in other research domains such as 
sentiment analysis or topic modeling are not suitable for studying 
policy communications. Their training corpora and methods are not for 
policy documents where technical terminologies are used, and sentiment 
expressions are refrained. We leverage word embedding techniques to 
extract semantic changes in the monetary policy documents. Our empirical
 study shows that the policymaker uses different semantics according to 
the type of documents when they change their policy.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2021-11</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>ACLWeb</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://aclanthology.org/2021.econlp-1.8/">https://aclanthology.org/2021.econlp-1.8/</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>1/8/2025, 2:17:39 PM</td>
					</tr>
					<tr>
					<th>Place</th>
						<td>Punta Cana, Dominican Republic</td>
					</tr>
					<tr>
					<th>Publisher</th>
						<td>Association for Computational Linguistics</td>
					</tr>
					<tr>
					<th>Pages</th>
						<td>56–61</td>
					</tr>
					<tr>
					<th>Proceedings Title</th>
						<td>Proceedings of the Third Workshop on Economics and Natural Language Processing</td>
					</tr>
					<tr>
					<th>Conference Name</th>
						<td>ECONLP 2021</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.18653/v1/2021.econlp-1.8">10.18653/v1/2021.econlp-1.8</a></td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>1/8/2025, 2:17:39 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_Z6WTICXP">Full Text PDF					</li>
				</ul>
			</li>


			<li id="item_WD69L2KB" class="item journalArticle">
			<h2>UTDRM: unsupervised method for training debunked-narrative retrieval models</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Iknoor Singh</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Carolina Scarton</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Kalina Bontcheva</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>A key task in the fact-checking workflow is to establish 
whether the claim under investigation has already been debunked or 
fact-checked before. This is essentially a retrieval task where a 
misinformation claim is used as a query to retrieve from a corpus of 
debunks. Prior debunk retrieval methods have typically been trained on 
annotated pairs of misinformation claims and debunks. The novelty of 
this paper is an Unsupervised Method for Training Debunked-Narrative 
Retrieval Models (UTDRM) in a zero-shot setting, eliminating the need 
for human-annotated pairs. This approach leverages fact-checking 
articles for the generation of synthetic claims and employs a neural 
retrieval model for training. Our experiments show that UTDRM tends to 
match or exceed the performance of state-of-the-art methods on seven 
datasets, which demonstrates its effectiveness and broad applicability. 
The paper also analyses the impact of various factors on UTDRM’s 
performance, such as the quantity of fact-checking articles utilised, 
the number of synthetically generated claims employed, the proposed 
entity inoculation method, and the usage of large language models for 
retrieval.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2023/12</td>
					</tr>
					<tr>
					<th>Language</th>
						<td>en</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>UTDRM</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>epjdatascience.springeropen.com</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://epjdatascience.springeropen.com/articles/10.1140/epjds/s13688-023-00437-y">https://epjdatascience.springeropen.com/articles/10.1140/epjds/s13688-023-00437-y</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>1/3/2025, 12:32:26 PM</td>
					</tr>
					<tr>
					<th>Rights</th>
						<td>2023 The Author(s)</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>Number: 1
Publisher: SpringerOpen</td>
					</tr>
					<tr>
					<th>Volume</th>
						<td>12</td>
					</tr>
					<tr>
					<th>Pages</th>
						<td>1-25</td>
					</tr>
					<tr>
					<th>Publication</th>
						<td>EPJ Data Science</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.1140/epjds/s13688-023-00437-y">10.1140/epjds/s13688-023-00437-y</a></td>
					</tr>
					<tr>
					<th>Issue</th>
						<td>1</td>
					</tr>
					<tr>
					<th>Journal Abbr</th>
						<td>EPJ Data Sci.</td>
					</tr>
					<tr>
					<th>ISSN</th>
						<td>2193-1127</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>1/3/2025, 12:32:26 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_IQFPPUD4">Full Text PDF					</li>
				</ul>
			</li>


			<li id="item_G8QEM3XM" class="item preprint">
			<h2>Vox Populi, Vox AI? Using Language Models to Estimate German Public Opinion</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Preprint</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Leah von der Heyde</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Anna-Carolina Haensch</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Alexander Wenz</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>The recent development of large language models (LLMs) has 
spurred discussions about whether LLM-generated "synthetic samples" 
could complement or replace traditional surveys, considering their 
training data potentially reflects attitudes and behaviors prevalent in 
the population. A number of mostly US-based studies have prompted LLMs 
to mimic survey respondents, with some of them finding that the 
responses closely match the survey data. However, several contextual 
factors related to the relationship between the respective target 
population and LLM training data might affect the generalizability of 
such findings. In this study, we investigate the extent to which LLMs 
can estimate public opinion in Germany, using the example of vote 
choice. We generate a synthetic sample of personas matching the 
individual characteristics of the 2017 German Longitudinal Election 
Study respondents. We ask the LLM GPT-3.5 to predict each respondent's 
vote choice and compare these predictions to the survey-based estimates 
on the aggregate and subgroup levels. We find that GPT-3.5 does not 
predict citizens' vote choice accurately, exhibiting a bias towards the 
Green and Left parties. While the LLM captures the tendencies of 
"typical" voter subgroups, such as partisans, it misses the multifaceted
 factors swaying individual voter choices. By examining the LLM-based 
prediction of voting behavior in a new context, our study contributes to
 the growing body of research about the conditions under which LLMs can 
be leveraged for studying public opinion. The findings point to 
disparities in opinion representation in LLMs and underscore the 
limitations in applying them for public opinion estimation.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2024-07-11</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>Vox Populi, Vox AI?</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>arXiv.org</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://arxiv.org/abs/2407.08563">http://arxiv.org/abs/2407.08563</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>1/3/2025, 4:26:10 PM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>arXiv:2407.08563 [cs]</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.48550/arXiv.2407.08563">10.48550/arXiv.2407.08563</a></td>
					</tr>
					<tr>
					<th>Repository</th>
						<td>arXiv</td>
					</tr>
					<tr>
					<th>Archive ID</th>
						<td>arXiv:2407.08563</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>1/3/2025, 4:26:10 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Computer Science - Artificial Intelligence</li>
					<li>Computer Science - Computers and Society</li>
					<li>Statistics - Applications</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_2SZL72MX">Preprint PDF					</li>
					<li id="item_Z2C2ABGY">Snapshot					</li>
				</ul>
			</li>


			<li id="item_TK495GK4" class="item journalArticle">
			<h2>We are What We Consume: Predicting Independent Voters’ Voting Preference From Their Media Diet Color</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Chingching Chang</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Yu-Chuan Hung</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Morris Hsieh</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Party identification is an important predictor of voting 
preference, but because a growing percentage of voters do not express 
any party identification, alternative ways to anticipate voting 
preferences are required. Partisan slants in voters’ media consumption 
might offer a relevant proxy. With method triangulation, the current 
study explores whether media consumption prior to elections can predict 
voting preferences among independents. Depending on the media outlets 
adopted by voters and their partisan skew, as detected by Bert machine 
learning models, the authors calculate an overall partisan slant for 
each voter’s political information consumption. Data from a nationwide 
panel survey conducted in Taiwan affirm that their media diet “color” in
 2019 can predict independent voters' choices in 2020.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2024-06-01</td>
					</tr>
					<tr>
					<th>Language</th>
						<td>en</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>We are What We Consume</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>SAGE Journals</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://doi.org/10.1177/08944393231214027">https://doi.org/10.1177/08944393231214027</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>12/24/2024, 6:48:18 PM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>Publisher: SAGE Publications Inc</td>
					</tr>
					<tr>
					<th>Volume</th>
						<td>42</td>
					</tr>
					<tr>
					<th>Pages</th>
						<td>661-680</td>
					</tr>
					<tr>
					<th>Publication</th>
						<td>Social Science Computer Review</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.1177/08944393231214027">10.1177/08944393231214027</a></td>
					</tr>
					<tr>
					<th>Issue</th>
						<td>3</td>
					</tr>
					<tr>
					<th>ISSN</th>
						<td>0894-4393</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>12/24/2024, 6:48:18 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_KKFUQKRV">SAGE PDF Full Text					</li>
				</ul>
			</li>


			<li id="item_APCTZC36" class="item preprint">
			<h2>What do LLMs remember? A linguistic-sociological framework for studying and using language technology</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Preprint</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Haley Lepp</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>A. J. Alvero</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Language technologies, especially those related to AI and 
machine learning, have grown increasingly popular as objects and methods
 of study for sociologists. We contend that to understand these objects 
and their place in our social world, we must attend to a linguistic 
sociology. In the context of large language models, for example, 
language -text, specifically- acts as both the input and output 
interface between person and machine. Yet text is itself a language 
technology, and carries with it affordances and constraints that 
remember long histories of social relations. Quilting together 
literature from sociology, computational linguistics, science and 
technology studies, and linguistic anthropology, we ask: what do 
language technologies remember, and how can sociologists take these 
memories into account? To answer this question, we offer a three-part 
analytical framework to examine how the memories of language 
technologies are both constructed by and constructing our social world. 
Drawing on two examples of use of language technologies in college 
admissions and in scientific peer review, we apply our framework to 
demonstrate how language technologies can mediate social actions, 
processes, and relations.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2025-01-01</td>
					</tr>
					<tr>
					<th>Language</th>
						<td>en-us</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>What do LLMs remember?</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>OSF Preprints</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://osf.io/phuqt">https://osf.io/phuqt</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>1/3/2025, 4:21:59 PM</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.31235/osf.io/phuqt">10.31235/osf.io/phuqt</a></td>
					</tr>
					<tr>
					<th>Repository</th>
						<td>OSF</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>1/3/2025, 4:21:59 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_FZ4R62VU">OSF Preprint					</li>
				</ul>
			</li>


			<li id="item_P59KPTM7" class="item preprint">
			<h2>What Do Platform Workers Think?: Analysis of Truck Drivers’ Perspectives on Digital Platforms Using GPT-4</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Preprint</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Qi Song</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Kai Jia</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>This study examines platform workers’ experiences and 
perspectives about digital platforms in the Chinese freight 
transportation industry based on a large dataset of self-employed truck 
drivers’ online discussions. Utilizing the state-of-the-art large 
language model (LLM), GPT-4 by OpenAI, this article creates a novel 
topic generation framework, Sequential Abstraction, to analyze the 
topics of concern among truck drivers. Additionally, this study employs 
GPT-4 to perform sentiment analysis and topic classification tasks, 
which achieve accuracy rates of 78.13% and 79.17%, respectively, with 
manually labeled results as benchmarks. The research results highlight 
the significant economic insecurities Chinese truck drivers face when 
finding gigs on digital platforms. This study contributes to existing 
platform work literature by examining workers’ real-time, spontaneous 
voices surrounding digital platforms. This study advocates for further 
uncovering the potential of LLMs in facilitating descriptive and 
explanatory social science research.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2024-06-07</td>
					</tr>
					<tr>
					<th>Language</th>
						<td>en-us</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>What Do Platform Workers Think?</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>OSF Preprints</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://osf.io/k9ye4">https://osf.io/k9ye4</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>1/3/2025, 4:01:43 PM</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.31235/osf.io/k9ye4">10.31235/osf.io/k9ye4</a></td>
					</tr>
					<tr>
					<th>Repository</th>
						<td>OSF</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>1/3/2025, 4:01:43 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>GPT-4</li>
					<li>large language models</li>
					<li>Platform work</li>
					<li>sentiment analysis</li>
					<li>topic generation</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_DCHTNYAY">OSF Preprint					</li>
				</ul>
			</li>


			<li id="item_WSD5NZ6C" class="item journalArticle">
			<h2>What Good is Qualitative Literacy Without Data Transparency?</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Colin Jerolmack</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Ethnographic and interview research have made signiﬁcant 
contributions to cumulative social science and inﬂuenced the public 
conversation around important social issues. However, debates rage over 
whether the standards of positivistic social science can or should be 
used to judge the rigor of interpretive methods. I begin this essay by 
brieﬂy delineating the problem of developing evaluative criteria for 
qualitative research. I then explore the extent to which Small and 
Calarco’s Qualitative Literacy helps advance a set of standards attuned 
to the distinct epistemology of interview and ethnographic methods. I 
argue that “qualitative literacy” is necessary but not sufﬁcient to help
 readers decide whether a particular study is high quality. The reader 
also needs access to enough information about the researcher’s data, 
ﬁeld site, or subjects that she can independently reanalyze the 
researcher’s interpretations and consider alternative explanations. I 
also touch on some important differences between ethnography and 
interviewing that matter for how we evaluate them.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>05/2023</td>
					</tr>
					<tr>
					<th>Language</th>
						<td>en</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>DOI.org (Crossref)</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://journals.sagepub.com/doi/10.1177/00491241221140429">https://journals.sagepub.com/doi/10.1177/00491241221140429</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>1/6/2025, 12:29:24 PM</td>
					</tr>
					<tr>
					<th>Volume</th>
						<td>52</td>
					</tr>
					<tr>
					<th>Pages</th>
						<td>1059-1072</td>
					</tr>
					<tr>
					<th>Publication</th>
						<td>Sociological Methods &amp; Research</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.1177/00491241221140429">10.1177/00491241221140429</a></td>
					</tr>
					<tr>
					<th>Issue</th>
						<td>2</td>
					</tr>
					<tr>
					<th>Journal Abbr</th>
						<td>Sociological Methods &amp; Research</td>
					</tr>
					<tr>
					<th>ISSN</th>
						<td>0049-1241, 1552-8294</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>1/6/2025, 12:29:24 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_YSTT569S">Jerolmack - 2023 - What Good is Qualitative Literacy Without Data Tra.pdf					</li>
				</ul>
			</li>


			<li id="item_LS3VU3HG" class="item preprint">
			<h2>When LLM Meets Hypergraph: A Sociological Analysis on Personality via Online Social Networks</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Preprint</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Zhiyao Shu</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Xiangguo Sun</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Hong Cheng</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Individual personalities significantly influence our 
perceptions, decisions, and social interactions, which is particularly 
crucial for gaining insights into human behavior patterns in online 
social network analysis. Many psychological studies have observed that 
personalities are strongly reflected in their social behaviors and 
social environments. In light of these problems, this paper proposes a 
sociological analysis framework for one's personality in an 
environment-based view instead of individual-level data mining. 
Specifically, to comprehensively understand an individual's behavior 
from low-quality records, we leverage the powerful associative ability 
of LLMs by designing an effective prompt. In this way, LLMs can 
integrate various scattered information with their external knowledge to
 generate higher-quality profiles, which can significantly improve the 
personality analysis performance. To explore the interactive mechanism 
behind the users and their online environments, we design an effective 
hypergraph neural network where the hypergraph nodes are users and the 
hyperedges in the hypergraph are social environments. We offer a useful 
dataset with user profile data, personality traits, and several detected
 environments from the real-world social platform. To the best of our 
knowledge, this is the first network-based dataset containing both 
hypergraph structure and social information, which could push forward 
future research in this area further. By employing the framework on this
 dataset, we can effectively capture the nuances of individual 
personalities and their online behaviors, leading to a deeper 
understanding of human interactions in the digital world.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2024-07-04</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>When LLM Meets Hypergraph</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>arXiv.org</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://arxiv.org/abs/2407.03568">http://arxiv.org/abs/2407.03568</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>1/7/2025, 4:47:21 PM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>arXiv:2407.03568 [cs]</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.48550/arXiv.2407.03568">10.48550/arXiv.2407.03568</a></td>
					</tr>
					<tr>
					<th>Repository</th>
						<td>arXiv</td>
					</tr>
					<tr>
					<th>Archive ID</th>
						<td>arXiv:2407.03568</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>1/7/2025, 4:47:21 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Computer Science - Information Retrieval</li>
					<li>Computer Science - Social and Information Networks</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_R5BKCKBC">Preprint PDF					</li>
					<li id="item_CKTTHLUP">Snapshot					</li>
				</ul>
			</li>


			<li id="item_NBRNY6WV" class="item journalArticle">
			<h2>Word embedding for social sciences: An interdisciplinary survey</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Akira Matsui</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Emilio Ferrara</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Machine learning models learn low-dimensional representations 
from complex high-dimensional data. Not only computer science but also 
social science has benefited from the advancement of these powerful 
tools. Within such tools, word embedding is one of the most popular 
methods in the literature. However, we have no particular documentation 
of this emerging trend because this trend overlaps different social 
science fields. To well compile this fragmented knowledge, we survey 
recent studies that apply word embedding models to human behavior 
mining. Our taxonomy built on the surveyed article provides a concise 
but comprehensive overview of this emerging trend of intersection 
between computer science and social science and guides scholars who are 
going to navigate the use of word embedding algorithms in their voyage 
of social science research.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2024</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>Word embedding for social sciences</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>Google Scholar</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://peerj.com/articles/cs-2562/">https://peerj.com/articles/cs-2562/</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>1/8/2025, 1:48:13 PM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>Publisher: PeerJ Inc.</td>
					</tr>
					<tr>
					<th>Volume</th>
						<td>10</td>
					</tr>
					<tr>
					<th>Pages</th>
						<td>e2562</td>
					</tr>
					<tr>
					<th>Publication</th>
						<td>PeerJ Computer Science</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>1/8/2025, 1:48:13 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 8:13:51 PM</td>
					</tr>
				</tbody></table>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_2ZYCYJZP">Available Version (via Google Scholar)					</li>
				</ul>
			</li>


			<li id="item_3GDMGDUJ" class="item preprint">
			<h2>You don't need a personality test to know these models are 
unreliable: Assessing the Reliability of Large Language Models on 
Psychometric Instruments</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Preprint</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Bangzhao Shu</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Lechen Zhang</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Minje Choi</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Lavinia Dunagan</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Lajanugen Logeswaran</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Moontae Lee</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Dallas Card</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>David Jurgens</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>The versatility of Large Language Models (LLMs) on natural 
language understanding tasks has made them popular for research in 
social sciences. To properly understand the properties and innate 
personas of LLMs, researchers have performed studies that involve using 
prompts in the form of questions that ask LLMs about particular 
opinions. In this study, we take a cautionary step back and examine 
whether the current format of prompting LLMs elicits responses in a 
consistent and robust manner. We first construct a dataset that contains
 693 questions encompassing 39 different instruments of persona 
measurement on 115 persona axes. Additionally, we design a set of 
prompts containing minor variations and examine LLMs' capabilities to 
generate answers, as well as prompt variations to examine their 
consistency with respect to content-level variations such as switching 
the order of response options or negating the statement. Our experiments
 on 17 different LLMs reveal that even simple perturbations 
significantly downgrade a model's question-answering ability, and that 
most LLMs have low negation consistency. Our results suggest that the 
currently widespread practice of prompting is insufficient to accurately
 and reliably capture model perceptions, and we therefore discuss 
potential alternatives to improve these issues.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2024-04-01</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>You don't need a personality test to know these models are unreliable</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>arXiv.org</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://arxiv.org/abs/2311.09718">http://arxiv.org/abs/2311.09718</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>1/7/2025, 5:43:11 PM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>arXiv:2311.09718 [cs]</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.48550/arXiv.2311.09718">10.48550/arXiv.2311.09718</a></td>
					</tr>
					<tr>
					<th>Repository</th>
						<td>arXiv</td>
					</tr>
					<tr>
					<th>Archive ID</th>
						<td>arXiv:2311.09718</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>1/7/2025, 5:43:11 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Computer Science - Artificial Intelligence</li>
					<li>Computer Science - Computation and Language</li>
				</ul>
				<h3 class="notes">Notes:</h3>
				<ul class="notes">
					<li id="item_3T2G942N">
<p class="plaintext">Comment: Camera-ready version for NAACL 2024. First two authors contributed equally</p>
					</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_CLWGN593">Preprint PDF					</li>
					<li id="item_Y3YGF4RD">Snapshot					</li>
				</ul>
			</li>


			<li id="item_SWM4WWFM" class="item preprint">
			<h2>"You Gotta be a Doctor, Lin": An Investigation of Name-Based Bias of Large Language Models in Employment Recommendations</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Preprint</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Huy Nghiem</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>John Prindle</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Jieyu Zhao</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Hal Daumé III</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Social science research has shown that candidates with names 
indicative of certain races or genders often face discrimination in 
employment practices. Similarly, Large Language Models (LLMs) have 
demonstrated racial and gender biases in various applications. In this 
study, we utilize GPT-3.5-Turbo and Llama 3-70B-Instruct to simulate 
hiring decisions and salary recommendations for candidates with 320 
first names that strongly signal their race and gender, across over 
750,000 prompts. Our empirical results indicate a preference among these
 models for hiring candidates with White female-sounding names over 
other demographic groups across 40 occupations. Additionally, even among
 candidates with identical qualifications, salary recommendations vary 
by as much as 5% between different subgroups. A comparison with 
real-world labor data reveals inconsistent alignment with U.S. labor 
market characteristics, underscoring the necessity of risk investigation
 of LLM-powered systems.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2024-10-05</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>"You Gotta be a Doctor, Lin"</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>arXiv.org</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://arxiv.org/abs/2406.12232">http://arxiv.org/abs/2406.12232</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>1/7/2025, 5:28:18 PM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>arXiv:2406.12232 [cs]</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.48550/arXiv.2406.12232">10.48550/arXiv.2406.12232</a></td>
					</tr>
					<tr>
					<th>Repository</th>
						<td>arXiv</td>
					</tr>
					<tr>
					<th>Archive ID</th>
						<td>arXiv:2406.12232</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>1/7/2025, 5:28:18 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Computer Science - Artificial Intelligence</li>
					<li>Computer Science - Computation and Language</li>
				</ul>
				<h3 class="notes">Notes:</h3>
				<ul class="notes">
					<li id="item_RVIK35KB">
<p class="plaintext">Comment: EMNLP 2024, 20 pages</p>
					</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_EX8DQS4Q">Preprint PDF					</li>
					<li id="item_5MKM8QV9">Snapshot					</li>
				</ul>
			</li>


			<li id="item_HAZU9J3L" class="item conferencePaper">
			<h2>Zero-Shot Meta-Learning for Small-Scale Data From Human Subjects</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Conference Paper</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Julie Jiang</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Kristina Lerman</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Emilio Ferrara</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>While developments in machine learning led to impressive 
performance gains on big data, many human subjects data are, in 
actuality, small and sparsely labeled. Existing methods applied to such 
data often do not easily generalize to out-of-sample subjects. Instead, 
models must make predictions on test data that may be drawn from a 
different distribution, a problem known as zero-shot learning. To 
address this challenge, we develop an end-to-end framework using a 
meta-learning approach, which enables the model to rapidly adapt to a 
new prediction task with limited training data for out-of-sample test 
data. We use three real-world small-scale human subjects datasets (two 
randomized control studies and one observational study), for which we 
predict treatment outcomes for held-out treatment groups. Our model 
learns the latent treatment effects of each intervention and, by design,
 can naturally handle multitask predictions. We show that our model 
performs the best holistically for each held-out group and especially 
when the test group is distinctly different from the training group. Our
 model has implications for improved generalization of small-size human 
studies to the wider population.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2023-06</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>IEEE Xplore</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://ieeexplore.ieee.org/abstract/document/10337328">https://ieeexplore.ieee.org/abstract/document/10337328</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>1/8/2025, 2:16:54 PM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>ISSN: 2575-2634</td>
					</tr>
					<tr>
					<th>Pages</th>
						<td>311-320</td>
					</tr>
					<tr>
					<th>Proceedings Title</th>
						<td>2023 IEEE 11th International Conference on Healthcare Informatics (ICHI)</td>
					</tr>
					<tr>
					<th>Conference Name</th>
						<td>2023 IEEE 11th International Conference on Healthcare Informatics (ICHI)</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.1109/ICHI57859.2023.00049">10.1109/ICHI57859.2023.00049</a></td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>1/8/2025, 2:16:54 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Adaptation models</li>
					<li>human subjects</li>
					<li>Machine learning</li>
					<li>meta-learning</li>
					<li>Metalearning</li>
					<li>Predictive models</li>
					<li>small-scale data</li>
					<li>Training</li>
					<li>Training data</li>
					<li>zero-shot learning</li>
					<li>Zero-shot learning</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_EFDZKKV4">IEEE Xplore Abstract Record					</li>
					<li id="item_6SRLHHXI">Submitted Version					</li>
				</ul>
			</li>


			<li id="item_45K8M5NF" class="item preprint">
			<h2>Zero-Shot Topic Classification of Column Headers: Leveraging LLMs for Metadata Enrichment</h2>
				<table>
					<tbody><tr>
						<th>Item Type</th>
						<td>Preprint</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Margherita Martorana</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Tobias Kuhn</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Lise Stork</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Jacco van Ossenbruggen</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Traditional dataset retrieval systems rely on metadata for 
indexing, rather than on the underlying data values. However, 
high-quality metadata creation and enrichment often require manual 
annotations, which is a labour-intensive and challenging process to 
automate. In this study, we propose a method to support metadata 
enrichment using topic annotations generated by three Large Language 
Models (LLMs): ChatGPT-3.5, GoogleBard, and GoogleGemini. Our analysis 
focuses on classifying column headers based on domain-specific topics 
from the Consortium of European Social Science Data Archives (CESSDA), a
 Linked Data controlled vocabulary. Our approach operates in a zero-shot
 setting, integrating the controlled topic vocabulary directly within 
the input prompt. This integration serves as a Large Context Windows 
approach, with the aim of improving the results of the topic 
classification task. We evaluated the performance of the LLMs in terms 
of internal consistency, inter-machine alignment, and agreement with 
human classification. Additionally, we investigate the impact of 
contextual information (i.e., dataset description) on the classification
 outcomes. Our findings suggest that ChatGPT and GoogleGemini outperform
 GoogleBard in terms of internal consistency as well as 
LLM-human-agreement. Interestingly, we found that contextual information
 had no significant impact on LLM performance. This work proposes a 
novel approach that leverages LLMs for topic classification of column 
headers using a controlled vocabulary, presenting a practical 
application of LLMs and Large Context Windows within the Semantic Web 
domain. This approach has the potential to facilitate automated metadata
 enrichment, thereby enhancing dataset retrieval and the Findability, 
Accessibility, Interoperability, and Reusability (FAIR) of research data
 on the Web.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2024-09-06</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>Zero-Shot Topic Classification of Column Headers</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>arXiv.org</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://arxiv.org/abs/2403.00884">http://arxiv.org/abs/2403.00884</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>1/7/2025, 5:30:13 PM</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>arXiv:2403.00884 [cs]</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.48550/arXiv.2403.00884">10.48550/arXiv.2403.00884</a></td>
					</tr>
					<tr>
					<th>Repository</th>
						<td>arXiv</td>
					</tr>
					<tr>
					<th>Archive ID</th>
						<td>arXiv:2403.00884</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>1/7/2025, 5:30:13 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>1/13/2025, 7:51:47 PM</td>
					</tr>
				</tbody></table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Computer Science - Artificial Intelligence</li>
					<li>Computer Science - Databases</li>
					<li>Computer Science - Information Retrieval</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_NRNY2K5V">Preprint PDF					</li>
					<li id="item_AC9VSCRU">Snapshot					</li>
				</ul>
			</li>

		</ul>
	
</body></html>